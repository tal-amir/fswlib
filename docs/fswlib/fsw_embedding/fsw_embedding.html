<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.3"/>
    <title>fswlib.fsw_embedding.fsw_embeddingfswlib documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>
            <img src="logo.png" class="logo" alt="project logo"/>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



        <h2>fswlib documentation</h2>
            <ul class="memberlist">

            <li>
                    <a class="class" href="#FSWEmbedding">FSWEmbedding</a>
                            <ul class="memberlist">

                        <li>
                                <a class="function" href="#FSWEmbedding.__init__">FSWEmbedding</a>
                        </li>
                        <li>
                                <a class="function" href="#FSWEmbedding.from_config">from_config</a>
                        </li>
                        <li>
                                <a class="function" href="#FSWEmbedding.reset_parameters">reset_parameters</a>
                        </li>
                        <li>
                                <a class="function" href="#FSWEmbedding.to">to</a>
                        </li>
                        <li>
                                <a class="function" href="#FSWEmbedding.forward">forward</a>
                        </li>


                        <li>
                            <a class="method" href="#FSWEmbedding.slice_vectors">slice_vectors</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.frequencies">frequencies</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.bias">bias</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.num_slices">num_slices</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.num_frequencies">num_frequencies</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.cartesian_mode">cartesian_mode</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.flatten_cartesian_axes">flatten_cartesian_axes</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.learnable_slices">learnable_slices</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.learnable_frequencies">learnable_frequencies</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.enable_bias">enable_bias</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.encode_total_mass">encode_total_mass</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.total_mass_encoding_transformation">total_mass_encoding_transformation</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.total_mass_encoding_method">total_mass_encoding_method</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.total_mass_encoding_scale">total_mass_encoding_scale</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.total_mass_padding_thresh">total_mass_padding_thresh</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.d_in">d_in</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.d_out">d_out</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.device">device</a>
                        </li>
                        <li>
                            <a class="method" href="#FSWEmbedding.dtype">dtype</a>
                        </li>

                </ul>

            </li>
            <li>
                    <a class="class" href="#EnumWithResolve">EnumWithResolve</a>
                            <ul class="memberlist">

                        <li>
                                <a class="function" href="#EnumWithResolve.resolve">resolve</a>
                        </li>



                </ul>

            </li>
            <li>
                    <a class="class" href="#TotalMassEncodingTransformation">TotalMassEncodingTransformation</a>
                            <ul class="memberlist">



                        <li>
                            <a class="method" href="#TotalMassEncodingTransformation.IDENTITY">IDENTITY</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingTransformation.SQRT">SQRT</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingTransformation.LOG">LOG</a>
                        </li>

                </ul>

            </li>
            <li>
                    <a class="class" href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a>
                            <ul class="memberlist">



                        <li>
                            <a class="method" href="#TotalMassEncodingMethod.DECOUPLED">DECOUPLED</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingMethod.SCALED">SCALED</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingMethod.HOMOGENEOUS">HOMOGENEOUS</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingMethod.HOMOGENEOUS_SCALED">HOMOGENEOUS_SCALED</a>
                        </li>
                        <li>
                            <a class="method" href="#TotalMassEncodingMethod.HOMOGENEOUS_LEGACY">HOMOGENEOUS_LEGACY</a>
                        </li>

                </ul>

            </li>
            <li>
                    <a class="class" href="#FrequencyInitMethod">FrequencyInitMethod</a>
                            <ul class="memberlist">



                        <li>
                            <a class="method" href="#FrequencyInitMethod.RANDOM">RANDOM</a>
                        </li>
                        <li>
                            <a class="method" href="#FrequencyInitMethod.EVEN">EVEN</a>
                        </li>

                </ul>

            </li>



    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../fswlib.html">fswlib</a><wbr>.fsw_embedding<wbr>.fsw_embedding    </h1>

                        <div class="docstring"><p>fsw_embedding.py</p>

<p>Main module for computing the Fourier Sliced-Wasserstein (FSW) embedding.
Part of the <code><a href="../../fswlib.html">fswlib</a></code> package: <a href="https://pypi.org/project/fswlib/">https://pypi.org/project/fswlib/</a></p>

<p>Authors:
    Tal Amir, Nadav Dym
    Technion – Israel Institute of Technology</p>

<p>This code is based on the paper:
    "Fourier Sliced-Wasserstein Embedding for Multisets and Measures"
    Tal Amir, Nadav Dym
    International Conference on Learning Representations (ICLR), 2025</p>

<p>Paper URL:
    <a href="https://iclr.cc/virtual/2025/poster/30562">https://iclr.cc/virtual/2025/poster/30562</a></p>

<p>Project repository:
    <a href="https://github.com/tal-amir/fswlib">https://github.com/tal-amir/fswlib</a></p>
</div>

                        <input id="mod-fsw_embedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-fsw_embedding-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">   1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">   2</span></a><span class="sd">fsw_embedding.py</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">   3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">   4</span></a><span class="sd">Main module for computing the Fourier Sliced-Wasserstein (FSW) embedding.</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">   5</span></a><span class="sd">Part of the `fswlib` package: https://pypi.org/project/fswlib/</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">   6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">   7</span></a><span class="sd">Authors:</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">   8</span></a><span class="sd">    Tal Amir, Nadav Dym</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">   9</span></a><span class="sd">    Technion – Israel Institute of Technology</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">  10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos">  11</span></a><span class="sd">This code is based on the paper:</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">  12</span></a><span class="sd">    &quot;Fourier Sliced-Wasserstein Embedding for Multisets and Measures&quot;</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">  13</span></a><span class="sd">    Tal Amir, Nadav Dym</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">  14</span></a><span class="sd">    International Conference on Learning Representations (ICLR), 2025</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">  15</span></a>
</span><span id="L-16"><a href="#L-16"><span class="linenos">  16</span></a><span class="sd">Paper URL:</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">  17</span></a><span class="sd">    https://iclr.cc/virtual/2025/poster/30562</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">  18</span></a>
</span><span id="L-19"><a href="#L-19"><span class="linenos">  19</span></a><span class="sd">Project repository:</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">  20</span></a><span class="sd">    https://github.com/tal-amir/fswlib</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">  21</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">  22</span></a>
</span><span id="L-23"><a href="#L-23"><span class="linenos">  23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos">  24</span></a><span class="n">version</span> <span class="o">=</span> <span class="s1">&#39;2.3&#39;</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">  25</span></a><span class="n">version_date</span> <span class="o">=</span> <span class="s1">&#39;2025-06-07&#39;</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">  26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos">  27</span></a><span class="c1"># Edge features:</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">  28</span></a><span class="c1"># - Do not split self.slice_vectors. Do self.slice_vectors.shape[1] == d_in + d_edge.</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">  29</span></a><span class="c1">#</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">  30</span></a><span class="c1"># Conditions:</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">  31</span></a><span class="c1"># - Edge features are used iff d_edge &gt; 0</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">  32</span></a><span class="c1"># - Edge features are only allowed in graph mode</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">  33</span></a><span class="c1"># - An input X_edge is expected in forward(). Its default is None, and a tensor with numel()==0 is also treated as None.</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">  34</span></a><span class="c1"># - X_edge.shape == (&lt;W.shape&gt;, d_edge)</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos">  35</span></a><span class="c1">#   X_edge is sparse iff W is sparse. If sparse, its dense dimension must equal 1, and its nonzero pattern must match that of W. It must be coalesced (as is W).</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">  36</span></a>
</span><span id="L-37"><a href="#L-37"><span class="linenos">  37</span></a><span class="c1"># TODO:</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">  38</span></a><span class="c1"># 0. Support d_in=0</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">  39</span></a><span class="c1"># 1. If d_in+d_edge=1, all slice vecs should equal 1.</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">  40</span></a><span class="c1"># 2. Add support for zero-sized output along any possible dimension (flat input corresponding to empty multisets, d_out=0)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">  41</span></a><span class="c1"># 3. Accelerate by explicitly computing W, W_sum and W_cumsum when W=&#39;uniform&#39; or &#39;unit&#39;</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">  42</span></a><span class="c1"># 3. Rename &#39;cartesian_mode&#39; to &#39;cartesian_freqs&#39;</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">  43</span></a><span class="c1"># 3. Add support for edge features and cartesian mode together, or at least add warning that it doesn&#39;t work</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">  44</span></a><span class="c1"># 4. Replace torch&#39;s sparse tensors by custom handling of indices, values</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">  45</span></a><span class="c1"># 5. For safety, in all functions under the class sp that return sparse tensors, make sure gradient of input is off</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">  46</span></a><span class="c1"># 6. Make sure that the state_dict saves all the embedding parameters, not just the pytorch tensors (e.g. encode_total_mass).</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">  47</span></a><span class="c1"># 7. Implement custom .state_dict() and .load_state_dict() methods, which should save and load accompanyting non-tensor parameters,</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">  48</span></a><span class="c1">#    upon loading update required_grads of tensors according to .learnable_slices, .learnable_frequencies, and allow initializing a model</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">  49</span></a><span class="c1">#    directly from a saved state_dict.</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">  50</span></a>
</span><span id="L-51"><a href="#L-51"><span class="linenos">  51</span></a><span class="c1"># Changelog:</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">  52</span></a><span class="c1"># 2.12    added support for total_mass_encoding_transformation</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">  53</span></a><span class="c1"># 2.11    fixed sparse padding bug</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">  54</span></a><span class="c1"># 2.1     added full edge-feature support</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">  55</span></a><span class="c1"># 2.09a   edge feature support is working (beta)</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">  56</span></a><span class="c1"># 2.03a   can handle failed loading of custom CUDA extension</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos">  57</span></a><span class="c1"># 2.02a   more efficient gradient handling in permute_sparse; renamed dimension variable names d, m to d_in, d_out</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">  58</span></a><span class="c1"># 2.01a   added safety mechanisms to detect uncoanesced tensors</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">  59</span></a><span class="c1"># 2.0a    low total-mass padding; total-mass encoding</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">  60</span></a><span class="c1"># 1.6     finished optimizing code</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos">  61</span></a><span class="c1"># 1.54    speed up: 11 sec. / epoch; removed outdated code</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">  62</span></a><span class="c1"># 1.53    speed up: 12 sec. / epoch; before removing outdated code pieces</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">  63</span></a><span class="c1"># 1.52    speed up: 13 sec. / epoch</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos">  64</span></a><span class="c1"># 1.51    Speed up: 17 sec. / epoch</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos">  65</span></a><span class="c1"># 1.5     Speed up by sum_sparse, incorporated segcumsum</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos">  66</span></a><span class="c1"># 1.44    Added end-to-end support for int64 indexing</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos">  67</span></a><span class="c1"># 1.43a   Testing the computation time of sum(A,dim=0) when A is sparse</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos">  68</span></a><span class="c1"># 1.42    More memory-efficient sparse_cumsum backward()</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos">  69</span></a><span class="c1"># 1.41    Added reverse option to sparse_cumsum</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos">  70</span></a><span class="c1"># 1.4     Incorporated segcumsum</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos">  71</span></a><span class="c1"># 1.31b   Reverted to correct &amp; slow sparse cumsum due to bug in the new segcumsum</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos">  72</span></a><span class="c1"># 1.31a   Testing hierarchical segcumsum</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos">  73</span></a><span class="c1"># 1.30    Added CUDA-implemented segcumsum, still slow</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos">  74</span></a><span class="c1"># 1.29    Added CUDA-implemented segcumsum!</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos">  75</span></a><span class="c1"># 1.28t3  Added cumsum_segments_consecutive</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos">  76</span></a><span class="c1"># 1.28t2  Testing cumsum_segments using jit</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos">  77</span></a><span class="c1"># 1.28t1  Testing sparse_cumsum_alt1</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos">  78</span></a><span class="c1"># 1.27    Made some slow assersions run only when fsw_embedding_debug_mode or fsw_embedding_basic_safety_checks are True</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos">  79</span></a><span class="c1"># 1.26    Removed the sp_old class. Got rid of more coalesce()</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos">  80</span></a><span class="c1"># 1.25c   Got rid of some more coalesce()</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos">  81</span></a><span class="c1"># 1.25b   Got rid of some more coalesce()</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos">  82</span></a><span class="c1"># 1.25    Removed most of coalesce() calls that follow calls to torch.sparse_coo_tensor()</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos">  83</span></a><span class="c1"># 1.24    Removed attributes &#39;device&#39; and &#39;dtype&#39; from FSW_embedding due to lack of safety. Use get_device(), get_dtype() instead.</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos">  84</span></a><span class="c1"># 1.23    Added project_W()</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos">  85</span></a><span class="c1"># 1.22    Added support for biases</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos">  86</span></a><span class="c1">#         learnable_frequencies=True now initializes frequencies to zero</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos">  87</span></a><span class="c1"># 1.21    Added reset_parameters()</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos">  88</span></a><span class="c1">#         Added type hinting and enforcement</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos">  89</span></a>
</span><span id="L-90"><a href="#L-90"><span class="linenos">  90</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos">  91</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos">  92</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd.function</span><span class="w"> </span><span class="kn">import</span> <span class="n">once_differentiable</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos">  93</span></a>
</span><span id="L-94"><a href="#L-94"><span class="linenos">  94</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos">  95</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">overload</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos">  96</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos">  97</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos">  98</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos">  99</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">numbers</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos"> 100</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos"> 101</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos"> 102</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">ctypes</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos"> 103</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">platform</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos"> 104</span></a>
</span><span id="L-105"><a href="#L-105"><span class="linenos"> 105</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FSWEmbedding&quot;</span><span class="p">,</span> <span class="s2">&quot;EnumWithResolve&quot;</span><span class="p">,</span> <span class="s2">&quot;TotalMassEncodingTransformation&quot;</span><span class="p">,</span> <span class="s2">&quot;TotalMassEncodingMethod&quot;</span><span class="p">,</span> <span class="s2">&quot;FrequencyInitMethod&quot;</span><span class="p">]</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos"> 106</span></a>
</span><span id="L-107"><a href="#L-107"><span class="linenos"> 107</span></a><span class="c1"># Name of custom CUDA extension binary</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos"> 108</span></a><span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;Windows&quot;</span><span class="p">:</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos"> 109</span></a>    <span class="n">_lib_name</span> <span class="o">=</span> <span class="s2">&quot;fsw_embedding.dll&quot;</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos"> 110</span></a><span class="k">elif</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;Darwin&quot;</span><span class="p">:</span>  <span class="c1"># macOS</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos"> 111</span></a>    <span class="n">_lib_name</span> <span class="o">=</span> <span class="s2">&quot;libfsw_embedding.dylib&quot;</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos"> 112</span></a><span class="k">else</span><span class="p">:</span>  <span class="c1"># Linux and others</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos"> 113</span></a>    <span class="n">_lib_name</span> <span class="o">=</span> <span class="s2">&quot;libfsw_embedding.so&quot;</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos"> 114</span></a>
</span><span id="L-115"><a href="#L-115"><span class="linenos"> 115</span></a><span class="c1"># Path to the compiled library (.so/.dll).</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos"> 116</span></a><span class="c1"># Should be at the same directory as this script file.</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos"> 117</span></a><span class="n">_lib_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="n">_lib_name</span><span class="p">)</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos"> 118</span></a>
</span><span id="L-119"><a href="#L-119"><span class="linenos"> 119</span></a><span class="c1"># Internal state for custom CUDA extension</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos"> 120</span></a><span class="c1"># The library will be loaded on the first time it is needed</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos"> 121</span></a><span class="n">_tried_to_load_lib</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos"> 122</span></a><span class="n">_lib_handle</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos"> 123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos"> 124</span></a><span class="c1"># Turn this on to run some verifications and sanity checks during runtime.</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos"> 125</span></a><span class="c1"># If an error is encountered, a runtime error is raised</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos"> 126</span></a><span class="n">fsw_embedding_debug_mode</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos"> 127</span></a>
</span><span id="L-128"><a href="#L-128"><span class="linenos"> 128</span></a><span class="c1"># Conduct basic safety checks, mainly on the user input.</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos"> 129</span></a><span class="c1"># Recommended to leave True, unless running time is of utmost importance, and the input is known to be consistent.</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos"> 130</span></a><span class="c1"># Setting this to False does not significantly reduce running time.</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos"> 131</span></a><span class="n">fsw_embedding_basic_safety_checks</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos"> 132</span></a>
</span><span id="L-133"><a href="#L-133"><span class="linenos"> 133</span></a><span class="c1"># Tells whether to use float64 in numerically-challenging parts of the code even if the data is in float32 format.</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos"> 134</span></a><span class="c1"># This was not observed to increase accuracy, and it incurs a significantly narrower memory bottleneck and a slightly higher running time.</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos"> 135</span></a><span class="n">fsw_embedding_high_precision</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos"> 136</span></a>
</span><span id="L-137"><a href="#L-137"><span class="linenos"> 137</span></a><span class="c1"># These are used for measuring running time</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos"> 138</span></a><span class="n">tal_global_timer</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos"> 139</span></a><span class="n">tal_global_timer_start</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos"> 140</span></a>
</span><span id="L-141"><a href="#L-141"><span class="linenos"> 141</span></a>
</span><span id="L-142"><a href="#L-142"><span class="linenos"> 142</span></a>
</span><span id="L-143"><a href="#L-143"><span class="linenos"> 143</span></a><span class="n">_E</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;_E&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="s2">&quot;EnumWithResolve&quot;</span><span class="p">)</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos"> 144</span></a>
</span><span id="L-145"><a href="#L-145"><span class="linenos"> 145</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EnumWithResolve</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos"> 146</span></a>    <span class="nd">@classmethod</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos"> 147</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">resolve</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">_E</span><span class="p">],</span> <span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_E</span><span class="p">:</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos"> 148</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos"> 149</span></a>            <span class="k">return</span> <span class="n">obj</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos"> 150</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos"> 151</span></a>            <span class="k">try</span><span class="p">:</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos"> 152</span></a>                <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos"> 153</span></a>            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos"> 154</span></a>                <span class="n">valid</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">cls</span><span class="p">]</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos"> 155</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos"> 156</span></a>                    <span class="sa">f</span><span class="s2">&quot;Invalid string &#39;</span><span class="si">{</span><span class="n">obj</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">. Valid options: </span><span class="si">{</span><span class="n">valid</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos"> 157</span></a>                <span class="p">)</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos"> 158</span></a>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos"> 159</span></a>            <span class="sa">f</span><span class="s2">&quot;Expected a string or </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instance, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos"> 160</span></a>        <span class="p">)</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos"> 161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos"> 162</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos"> 163</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the string value of the enum member.&quot;&quot;&quot;</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos"> 164</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos"> 165</span></a>
</span><span id="L-166"><a href="#L-166"><span class="linenos"> 166</span></a>
</span><span id="L-167"><a href="#L-167"><span class="linenos"> 167</span></a><span class="k">class</span><span class="w"> </span><span class="nc">TotalMassEncodingTransformation</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos"> 168</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformation applied to the total mass before incorporating into the embedding.</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos"> 169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos"> 170</span></a><span class="sd">    Each option defines a different transformation applied to the total mass of an input measure/multiset</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos"> 171</span></a><span class="sd">    before it is incorporated into the embedding vector.</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos"> 172</span></a>
</span><span id="L-173"><a href="#L-173"><span class="linenos"> 173</span></a><span class="sd">    Attributes</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos"> 174</span></a><span class="sd">    ----------</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos"> 175</span></a><span class="sd">    IDENTITY : str</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos"> 176</span></a><span class="sd">        $f(x) = x$; no transformation.</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos"> 177</span></a><span class="sd">    SQRT : str</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos"> 178</span></a><span class="sd">        $f(x) = \\sqrt{1 + x} - 1$; mild nonlinearity.</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos"> 179</span></a><span class="sd">    LOG : str</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos"> 180</span></a><span class="sd">        $f(x) = \\log(1 + x)$; stronger compression of large values.</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos"> 181</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos"> 182</span></a>    <span class="n">IDENTITY</span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos"> 183</span></a>    <span class="n">SQRT</span> <span class="o">=</span> <span class="s1">&#39;sqrt&#39;</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos"> 184</span></a>    <span class="n">LOG</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos"> 185</span></a>
</span><span id="L-186"><a href="#L-186"><span class="linenos"> 186</span></a>
</span><span id="L-187"><a href="#L-187"><span class="linenos"> 187</span></a>
</span><span id="L-188"><a href="#L-188"><span class="linenos"> 188</span></a><span class="k">class</span><span class="w"> </span><span class="nc">TotalMassEncodingMethod</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos"> 189</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos"> 190</span></a><span class="sd">    Strategies for incorporating total mass into the embedding.</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos"> 191</span></a>
</span><span id="L-192"><a href="#L-192"><span class="linenos"> 192</span></a><span class="sd">    Each method defines a different way of incorporating the total mass $\\mu\\left(\\Omega\\right) = \\sum_{i=1}^n w_i$ of an input measure</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos"> 193</span></a><span class="sd">    $\\mu = \\sum_{i=1}^n w_i \\delta_{\\mathbf{x}^{(i)}}$ (i.e. the multiset size if $\\mu$ is a multiset) with the FSW embedding of the normalized input $\\mu_{\\rho}$ into a single output vector.</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos"> 194</span></a><span class="sd">    For further discussion, see Appendix A.1 of the reference below.</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos"> 195</span></a>
</span><span id="L-196"><a href="#L-196"><span class="linenos"> 196</span></a><span class="sd">    Attributes</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos"> 197</span></a><span class="sd">    ----------</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos"> 198</span></a><span class="sd">    DECOUPLED : str</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos"> 199</span></a><span class="sd">        The total mass is appended as a separate component to the embedding vector,</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos"> 200</span></a><span class="sd">        which is computed from the normalized input measure, as in Equation (18)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos"> 201</span></a><span class="sd">        in our paper:</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos"> 202</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos"> 203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos"> 204</span></a><span class="sd">    SCALED : str</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos"> 205</span></a><span class="sd">        Similar to `DECOUPLED`, but the embedding of the normalized input is scaled</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos"> 206</span></a><span class="sd">        by the total mass. Using the notation of Equation (18), this yields:</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos"> 207</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos"> 208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos"> 209</span></a><span class="sd">    HOMOGENEOUS : str</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos"> 210</span></a><span class="sd">        A method that encodes the total mass while preserving homogeneity</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos"> 211</span></a><span class="sd">        with respect to the elements of the input multiset. See Equation (19).</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos"> 212</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert \\cdot \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos"> 213</span></a>
</span><span id="L-214"><a href="#L-214"><span class="linenos"> 214</span></a><span class="sd">    HOMOGENEOUS_SCALED : str</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos"> 215</span></a><span class="sd">        Similar to `SCALED`, but preserves homogeneity.</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos"> 216</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert, \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos"> 217</span></a>
</span><span id="L-218"><a href="#L-218"><span class="linenos"> 218</span></a><span class="sd">    HOMOGENEOUS_LEGACY : str</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos"> 219</span></a><span class="sd">        An alternative, legacy version of the homogeneous method, retained</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos"> 220</span></a><span class="sd">        for reference and compatibility.</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos"> 221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos"> 222</span></a><span class="sd">    Notes</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos"> 223</span></a><span class="sd">    -----</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos"> 224</span></a><span class="sd">    In practice, $\\mu\\left(\\Omega\\right)$ in the above expressions is replaced by $\\alpha \\cdot f \\left( \\mu\\left(\\Omega\\right) \\right)$,</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos"> 225</span></a><span class="sd">    where $f$ is the function defined by `TotalMassEncodingTransformation` and $\\alpha$ is a scale factor given in `total_mass_encoding_scale`.</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos"> 226</span></a><span class="sd">    Additionally, $\\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert$ is multiplied by a normalizing factor $\\sqrt{m-1}^{-1}$.</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos"> 227</span></a>
</span><span id="L-228"><a href="#L-228"><span class="linenos"> 228</span></a><span class="sd">    Reference</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos"> 229</span></a><span class="sd">    ---------</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos"> 230</span></a><span class="sd">    Tal Amir, Nadav Dym.</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos"> 231</span></a><span class="sd">    &quot;Fourier Sliced-Wasserstein Embedding for Multisets and Measures.&quot;</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos"> 232</span></a><span class="sd">    International Conference on Learning Representations (ICLR), 2025.</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos"> 233</span></a><span class="sd">    https://iclr.cc/virtual/2025/poster/30562</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos"> 234</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos"> 235</span></a>    <span class="n">DECOUPLED</span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos"> 236</span></a>    <span class="n">SCALED</span> <span class="o">=</span> <span class="s1">&#39;scaled&#39;</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos"> 237</span></a>    <span class="n">HOMOGENEOUS</span> <span class="o">=</span> <span class="s1">&#39;homogeneous&#39;</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos"> 238</span></a>    <span class="n">HOMOGENEOUS_SCALED</span> <span class="o">=</span> <span class="s1">&#39;homogeneous_scaled&#39;</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos"> 239</span></a>    <span class="n">HOMOGENEOUS_LEGACY</span> <span class="o">=</span> <span class="s1">&#39;homogeneous_legacy&#39;</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos"> 240</span></a>
</span><span id="L-241"><a href="#L-241"><span class="linenos"> 241</span></a>
</span><span id="L-242"><a href="#L-242"><span class="linenos"> 242</span></a>
</span><span id="L-243"><a href="#L-243"><span class="linenos"> 243</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FrequencyInitMethod</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos"> 244</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos"> 245</span></a><span class="sd">    Method for initializing frequencies in the FSW embedding.</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos"> 246</span></a>
</span><span id="L-247"><a href="#L-247"><span class="linenos"> 247</span></a><span class="sd">    This enumeration specifies how the frequencies in the FSW embedding are</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos"> 248</span></a><span class="sd">    initialized.</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos"> 249</span></a>
</span><span id="L-250"><a href="#L-250"><span class="linenos"> 250</span></a><span class="sd">    Attributes</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos"> 251</span></a><span class="sd">    ----------</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos"> 252</span></a><span class="sd">    RANDOM : str</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos"> 253</span></a><span class="sd">        Frequencies are sampled independently at random from the distribution</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos"> 254</span></a><span class="sd">        D_ξ, as defined in Section 3 of our paper.</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos"> 255</span></a><span class="sd">    EVEN : str</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos"> 256</span></a><span class="sd">        Frequencies are spaced deterministically for efficient coverage of the frequency domain,</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos"> 257</span></a><span class="sd">        with spacing inversely proportional to the density function f_ξ.</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos"> 258</span></a>
</span><span id="L-259"><a href="#L-259"><span class="linenos"> 259</span></a><span class="sd">    See Also</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos"> 260</span></a><span class="sd">    --------</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos"> 261</span></a><span class="sd">    FSWEmbedding.__init__ : Where this method is selected and used.</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos"> 262</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos"> 263</span></a>
</span><span id="L-264"><a href="#L-264"><span class="linenos"> 264</span></a>    <span class="n">RANDOM</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos"> 265</span></a>    <span class="n">EVEN</span> <span class="o">=</span> <span class="s2">&quot;even&quot;</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos"> 266</span></a>
</span><span id="L-267"><a href="#L-267"><span class="linenos"> 267</span></a>
</span><span id="L-268"><a href="#L-268"><span class="linenos"> 268</span></a>
</span><span id="L-269"><a href="#L-269"><span class="linenos"> 269</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FSWEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos"> 270</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos"> 271</span></a><span class="sd">    Fourier Sliced-Wasserstein (FSW) embedding module.</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos"> 272</span></a>
</span><span id="L-273"><a href="#L-273"><span class="linenos"> 273</span></a><span class="sd">    Maps input multisets (or, more generally, discrete measures) in</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos"> 274</span></a><span class="sd">    $\mathbb{R}^{d_\text{in}}$ to fixed-length vectors in</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos"> 275</span></a><span class="sd">    $\mathbb{R}^{d_\text{out}}$ via the Fourier Sliced-Wasserstein</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos"> 276</span></a><span class="sd">    embedding as described in [Amir &amp; Dym, ICLR 2025].</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos"> 277</span></a>
</span><span id="L-278"><a href="#L-278"><span class="linenos"> 278</span></a><span class="sd">    Features</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos"> 279</span></a><span class="sd">    --------</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos"> 280</span></a><span class="sd">    • **Batched inputs**: eupports arbitrary number of batch dimensions.</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos"> 281</span></a><span class="sd">    • **Graph mode**: efficient message-aggregation, including sparse adjacency support.</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos"> 282</span></a><span class="sd">    • **Differentiability**: Full autograd/gradient support.</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos"> 283</span></a>
</span><span id="L-284"><a href="#L-284"><span class="linenos"> 284</span></a><span class="sd">    See Also</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos"> 285</span></a><span class="sd">    --------</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos"> 286</span></a><span class="sd">    `FSWEmbedding.__init__` : Constructor parameters.</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos"> 287</span></a><span class="sd">    `FSWEmbedding.forward` : Input/output tensor shapes and options.</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos"> 288</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos"> 289</span></a>
</span><span id="L-290"><a href="#L-290"><span class="linenos"> 290</span></a>
</span><span id="L-291"><a href="#L-291"><span class="linenos"> 291</span></a>
</span><span id="L-292"><a href="#L-292"><span class="linenos"> 292</span></a>
</span><span id="L-293"><a href="#L-293"><span class="linenos"> 293</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos"> 294</span></a>                 <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos"> 295</span></a>                 <span class="n">d_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos"> 296</span></a>                 <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos"> 297</span></a>                 <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos"> 298</span></a>                 <span class="n">flatten_cartesian_axes</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos"> 299</span></a>                 <span class="n">d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos"> 300</span></a>                 <span class="n">encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos"> 301</span></a>                 <span class="n">total_mass_encoding_transformation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingTransformation</span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span><span class="p">,</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos"> 302</span></a>                 <span class="n">total_mass_encoding_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingMethod</span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span><span class="p">,</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos"> 303</span></a>                 <span class="n">total_mass_encoding_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos"> 304</span></a>                 <span class="n">total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos"> 305</span></a>                 <span class="n">learnable_slices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos"> 306</span></a>                 <span class="n">learnable_frequencies</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos"> 307</span></a>                 <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span><span class="p">,</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos"> 308</span></a>                 <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos"> 309</span></a>                 <span class="n">enable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos"> 310</span></a>                 <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos"> 311</span></a>                 <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos"> 312</span></a>                 <span class="n">use_custom_cuda_extension_if_available</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos"> 313</span></a>                 <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos"> 314</span></a>                 <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos"> 315</span></a>                 <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos"> 316</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos"> 317</span></a><span class="sd">        Initialize an FSWEmbedding module.</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos"> 318</span></a>
</span><span id="L-319"><a href="#L-319"><span class="linenos"> 319</span></a><span class="sd">        Parameters</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos"> 320</span></a><span class="sd">        ----------</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos"> 321</span></a><span class="sd">        d_in : int</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos"> 322</span></a><span class="sd">            The dimension of input multiset elements or, more generally, measure support points.  </span>
</span><span id="L-323"><a href="#L-323"><span class="linenos"> 323</span></a><span class="sd">            Coresponds to $d$ in $\mathcal{S}_{\leq N}\left(\mathbb{R}^d\right)$, $\mathcal{P}_{\leq N}\left(\mathbb{R}^d\right)$, or $\mathcal{M}_{\leq N}\left(\mathbb{R}^d\right)$ in our paper. </span>
</span><span id="L-324"><a href="#L-324"><span class="linenos"> 324</span></a><span class="sd">        d_out : int; optional</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos"> 325</span></a><span class="sd">            Desired embedding dimension.  </span>
</span><span id="L-326"><a href="#L-326"><span class="linenos"> 326</span></a><span class="sd">            If not set, both `num_slices` and `num_frequencies` must be explicitly provided.</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos"> 327</span></a><span class="sd">        num_slices : int; optional</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos"> 328</span></a><span class="sd">            Number of slices.  </span>
</span><span id="L-329"><a href="#L-329"><span class="linenos"> 329</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="L-330"><a href="#L-330"><span class="linenos"> 330</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos"> 331</span></a><span class="sd">        num_frequencies : int; optional</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos"> 332</span></a><span class="sd">            Number of frequencies per slice.  </span>
</span><span id="L-333"><a href="#L-333"><span class="linenos"> 333</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="L-334"><a href="#L-334"><span class="linenos"> 334</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos"> 335</span></a><span class="sd">        flatten_cartesian_axes : bool; default=False</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos"> 336</span></a><span class="sd">            If True, flattens the slice and frequency dimensions into a single output axis.  </span>
</span><span id="L-337"><a href="#L-337"><span class="linenos"> 337</span></a><span class="sd">            Only relevant if `num_slices` and `num_frequencies` are provided.</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos"> 338</span></a><span class="sd">        d_edge : int; default=0</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos"> 339</span></a><span class="sd">            Dimension of edge feature vectors. Used only for graph inputs.  </span>
</span><span id="L-340"><a href="#L-340"><span class="linenos"> 340</span></a><span class="sd">            See the `graph_mode` argument of `FSWEmbedding.forward` for details.</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos"> 341</span></a><span class="sd">        encode_total_mass : bool; default=False</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos"> 342</span></a><span class="sd">            Whether to incorporate the input multiset size (or, more generally, the *total mass* of the input measure)</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos"> 343</span></a><span class="sd">            into the embedding output.</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos"> 344</span></a><span class="sd">        total_mass_encoding_transformation : {&#39;identity&#39;, &#39;sqrt&#39;, &#39;log&#39;} or TotalMassEncodingFunction; default=&#39;identity&#39;</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos"> 345</span></a><span class="sd">            Transformation applied to the total mass *before* embedding.  </span>
</span><span id="L-346"><a href="#L-346"><span class="linenos"> 346</span></a><span class="sd">            See also: `TotalMassEncodingFunction`</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos"> 347</span></a><span class="sd">        total_mass_encoding_method : {&#39;decoupled&#39;, &#39;scaled&#39;, &#39;homogeneous&#39;, &#39;homogeneous_scaled&#39;, &#39;homogeneous_legacy&#39;} or TotalMassEncodingMethod; default=&#39;decoupled&#39;</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos"> 348</span></a><span class="sd">            Strategy for combining the total mass with the core embedding.  </span>
</span><span id="L-349"><a href="#L-349"><span class="linenos"> 349</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos"> 350</span></a><span class="sd">        total_mass_encoding_scale : float; default=1.0</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos"> 351</span></a><span class="sd">            The encoded total mass is multiplied by this scaling factor.  </span>
</span><span id="L-352"><a href="#L-352"><span class="linenos"> 352</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos"> 353</span></a><span class="sd">        total_mass_padding_thresh : float or int; default=1.0</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos"> 354</span></a><span class="sd">            Inputs with total mass below this threshold are padded with the zero vector to reach it; see</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos"> 355</span></a><span class="sd">            in [Amir and Dym, ICLR 2025], Appendix A.1.  </span>
</span><span id="L-356"><a href="#L-356"><span class="linenos"> 356</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos"> 357</span></a><span class="sd">        learnable_slices : bool; default=False</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos"> 358</span></a><span class="sd">            If True, slice vectors are learnable parameters.  </span>
</span><span id="L-359"><a href="#L-359"><span class="linenos"> 359</span></a><span class="sd">        learnable_frequencies : bool; default=False</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos"> 360</span></a><span class="sd">            If True, frequency values are learnable parameters.</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos"> 361</span></a><span class="sd">        frequency_init : float, str, tuple of float, or FrequencyInitMethod; default=&#39;random&#39;</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos"> 362</span></a><span class="sd">            Initialization scheme for frequencies:</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos"> 363</span></a><span class="sd">              - A float: sets all frequencies to the same value.</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos"> 364</span></a><span class="sd">              - A tuple `(low, high)` of floats: sets evenly spaced values in that interval.</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos"> 365</span></a><span class="sd">              - &#39;random&#39;: frequencies are drawn independently from the distribution $\mathcal{D_{\xi}}$, defined in</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos"> 366</span></a><span class="sd">                          [Amir and Dym, ICLR 2025], Section 3.</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos"> 367</span></a><span class="sd">              - &#39;even&#39;: frequencies are spaced evenly according to their distribution $\mathcal{D_{\xi}}$, with spaces</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos"> 368</span></a><span class="sd">                        inversely proportional to the density.  </span>
</span><span id="L-369"><a href="#L-369"><span class="linenos"> 369</span></a><span class="sd">            See also: `FrequencyInitMethod`</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos"> 370</span></a><span class="sd">        minimize_slice_coherence : bool; default=False</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos"> 371</span></a><span class="sd">            If True, minimizes the *mutual coherence* between slices for a more uniform spread on the unit sphere.  </span>
</span><span id="L-372"><a href="#L-372"><span class="linenos"> 372</span></a><span class="sd">            If False, slice vectors are drawn uniformly at random from the unit sphere.</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos"> 373</span></a><span class="sd">        enable_bias : bool; default=True</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos"> 374</span></a><span class="sd">            If True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos"> 375</span></a><span class="sd">            to zero.  </span>
</span><span id="L-376"><a href="#L-376"><span class="linenos"> 376</span></a><span class="sd">        device : torch.device, int, str, or None, optional</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos"> 377</span></a><span class="sd">            The torch device on which to allocate tensors (e.g., &#39;cpu&#39;, &#39;cuda&#39;, or an index).  </span>
</span><span id="L-378"><a href="#L-378"><span class="linenos"> 378</span></a><span class="sd">            If not provided, the default device defined in Torch is used.</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos"> 379</span></a><span class="sd">        dtype : torch.dtype, optional</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos"> 380</span></a><span class="sd">            Data type of input and output tensors (e.g., torch.float32).</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos"> 381</span></a><span class="sd">            If not provided, the default dtype defined in Torch is used.</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos"> 382</span></a><span class="sd">        use_custom_cuda_extension_if_available : bool or None, optional</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos"> 383</span></a><span class="sd">            Whether to use the custom CUDA kernel if present.</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos"> 384</span></a><span class="sd">            Default: Linux: True, all other systems: False</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos"> 385</span></a><span class="sd">        fail_if_cuda_extension_load_fails : bool; default=False</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos"> 386</span></a><span class="sd">            Whether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos"> 387</span></a><span class="sd">        report : bool; default=False</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos"> 388</span></a><span class="sd">            If True, prints a report with diagnostic information during initialization and forward computation.</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos"> 389</span></a><span class="sd">        report_on_coherence_minimization : bool; default=False</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos"> 390</span></a><span class="sd">            If True, prints special diagnostics during slice coherence minimization.</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos"> 391</span></a>
</span><span id="L-392"><a href="#L-392"><span class="linenos"> 392</span></a><span class="sd">        Notes</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos"> 393</span></a><span class="sd">        -----</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos"> 394</span></a><span class="sd">        If Cartesian mode is activated and `encode_total_mass` is True, `flatten_cartesian_axes` must be True.</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos"> 395</span></a>
</span><span id="L-396"><a href="#L-396"><span class="linenos"> 396</span></a><span class="sd">        See Also</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos"> 397</span></a><span class="sd">        --------</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos"> 398</span></a><span class="sd">        FrequencyInitMethod :</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos"> 399</span></a><span class="sd">            Enum for selecting frequency initialization strategies.</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos"> 400</span></a><span class="sd">        TotalMassEncodingTransformation :</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos"> 401</span></a><span class="sd">            Enum for total mass transformations.</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos"> 402</span></a><span class="sd">        TotalMassEncodingMethod :</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos"> 403</span></a><span class="sd">            Enum for strategies to incorporate total mass into the embedding.</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos"> 404</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos"> 405</span></a>
</span><span id="L-406"><a href="#L-406"><span class="linenos"> 406</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos"> 407</span></a>
</span><span id="L-408"><a href="#L-408"><span class="linenos"> 408</span></a>        <span class="c1"># Process sizes</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos"> 409</span></a>        <span class="k">assert</span> <span class="n">d_in</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_in must be nonnegative&#39;</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos"> 410</span></a>        <span class="k">assert</span> <span class="n">d_edge</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_edge must be nonnegative&#39;</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos"> 411</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;d_out must be nonnegative or None&#39;</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos"> 412</span></a>
</span><span id="L-413"><a href="#L-413"><span class="linenos"> 413</span></a>        <span class="k">if</span> <span class="n">d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos"> 414</span></a>            <span class="c1"># If the output should be empty, we force encode_total_mass to be False</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos"> 415</span></a>            <span class="n">encode_total_mass</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos"> 416</span></a>
</span><span id="L-417"><a href="#L-417"><span class="linenos"> 417</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_in</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos"> 418</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_edge</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos"> 419</span></a>
</span><span id="L-420"><a href="#L-420"><span class="linenos"> 420</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">encode_total_mass</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos"> 421</span></a>
</span><span id="L-422"><a href="#L-422"><span class="linenos"> 422</span></a>        <span class="n">total_mass_padding_thresh</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos"> 423</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be inf&#39;</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos"> 424</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be NaN&#39;</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos"> 425</span></a>        <span class="k">assert</span> <span class="n">total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos"> 426</span></a>
</span><span id="L-427"><a href="#L-427"><span class="linenos"> 427</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos"> 428</span></a>        <span class="k">del</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos"> 429</span></a>
</span><span id="L-430"><a href="#L-430"><span class="linenos"> 430</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="o">=</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_method</span><span class="p">)</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos"> 431</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_method</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos"> 432</span></a>
</span><span id="L-433"><a href="#L-433"><span class="linenos"> 433</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span> <span class="o">=</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos"> 434</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos"> 435</span></a>
</span><span id="L-436"><a href="#L-436"><span class="linenos"> 436</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span> <span class="o">=</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_transformation</span><span class="p">)</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos"> 437</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_transformation</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos"> 438</span></a>
</span><span id="L-439"><a href="#L-439"><span class="linenos"> 439</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos"> 440</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos"> 441</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos"> 442</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^(</span><span class="si">%d</span><span class="s1">+</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">)</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos"> 443</span></a>
</span><span id="L-444"><a href="#L-444"><span class="linenos"> 444</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos"> 445</span></a>
</span><span id="L-446"><a href="#L-446"><span class="linenos"> 446</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos"> 447</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos"> 448</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos"> 449</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">d_out</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos"> 450</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos"> 451</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos"> 452</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos"> 453</span></a>
</span><span id="L-454"><a href="#L-454"><span class="linenos"> 454</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos"> 455</span></a>            <span class="k">assert</span> <span class="n">flatten_cartesian_axes</span>  <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">encode_total_mass</span><span class="p">),</span> <span class="s1">&#39;Cartesian mode with flatten_cartesian_axes =False is not supported when encode_total_mass=True&#39;</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos"> 456</span></a>
</span><span id="L-457"><a href="#L-457"><span class="linenos"> 457</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos"> 458</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="n">flatten_cartesian_axes</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos"> 459</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">num_slices</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos"> 460</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">num_frequencies</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos"> 461</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">num_slices</span> <span class="o">*</span> <span class="n">num_frequencies</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos"> 462</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="k">else</span> <span class="p">(</span><span class="s1">&#39;R^(</span><span class="si">%d</span><span class="se">\u00d7</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos"> 463</span></a>
</span><span id="L-464"><a href="#L-464"><span class="linenos"> 464</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos"> 465</span></a>            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Expected exactly one of (d_out != None) or (num_slices != None and num_frequencies != None)&quot;</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos"> 466</span></a>
</span><span id="L-467"><a href="#L-467"><span class="linenos"> 467</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_out must be nonnegative&#39;</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos"> 468</span></a>
</span><span id="L-469"><a href="#L-469"><span class="linenos"> 469</span></a>        <span class="c1">#d_out = self.d_out</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos"> 470</span></a>        <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos"> 471</span></a>        <span class="n">num_frequencies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos"> 472</span></a>
</span><span id="L-473"><a href="#L-473"><span class="linenos"> 473</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">minimize_slice_coherence</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos"> 474</span></a>
</span><span id="L-475"><a href="#L-475"><span class="linenos"> 475</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="o">=</span> <span class="n">learnable_slices</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos"> 476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span> <span class="o">=</span> <span class="n">learnable_frequencies</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos"> 477</span></a>
</span><span id="L-478"><a href="#L-478"><span class="linenos"> 478</span></a>        <span class="c1"># Note: frequency_init is checked for correctness downstream at generate_embedding_parameters()</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos"> 479</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">frequency_init</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos"> 480</span></a>
</span><span id="L-481"><a href="#L-481"><span class="linenos"> 481</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span> <span class="o">=</span> <span class="n">enable_bias</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos"> 482</span></a>
</span><span id="L-483"><a href="#L-483"><span class="linenos"> 483</span></a>        <span class="c1"># _device_new and _dtype_new are only defined here on __init__ and passed on to reset_parameters(), which then deletes them</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos"> 484</span></a>        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos"> 485</span></a>            <span class="c1"># Use get_default_device if available (PyTorch 2.3+)</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos"> 486</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_device&quot;</span><span class="p">):</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos"> 487</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_device</span><span class="p">()</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos"> 488</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos"> 489</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos"> 490</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos"> 491</span></a>
</span><span id="L-492"><a href="#L-492"><span class="linenos"> 492</span></a>        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-493"><a href="#L-493"><span class="linenos"> 493</span></a>            <span class="c1"># Use get_default_dtype if available</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos"> 494</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_dtype&quot;</span><span class="p">):</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos"> 495</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos"> 496</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-497"><a href="#L-497"><span class="linenos"> 497</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos"> 498</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos"> 499</span></a>
</span><span id="L-500"><a href="#L-500"><span class="linenos"> 500</span></a>        <span class="k">assert</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">),</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">dtype</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos"> 501</span></a>
</span><span id="L-502"><a href="#L-502"><span class="linenos"> 502</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span> <span class="o">=</span> <span class="n">device</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos"> 503</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span> <span class="o">=</span> <span class="n">dtype</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos"> 504</span></a>
</span><span id="L-505"><a href="#L-505"><span class="linenos"> 505</span></a>        <span class="k">if</span> <span class="n">use_custom_cuda_extension_if_available</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos"> 506</span></a>            <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;Windows&#39;</span><span class="p">,</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">}:</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos"> 507</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos"> 508</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos"> 509</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos"> 510</span></a>
</span><span id="L-511"><a href="#L-511"><span class="linenos"> 511</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos"> 512</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos"> 513</span></a>
</span><span id="L-514"><a href="#L-514"><span class="linenos"> 514</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">report</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos"> 515</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos"> 516</span></a>
</span><span id="L-517"><a href="#L-517"><span class="linenos"> 517</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos"> 518</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Fourier Sliced-Wasserstein Embedding&#39;</span><span class="p">)</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos"> 519</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;version </span><span class="si">%s</span><span class="s1">, </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="n">version_date</span><span class="p">))</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos"> 520</span></a>
</span><span id="L-521"><a href="#L-521"><span class="linenos"> 521</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos"> 522</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Based on our paper titled &quot;Fourier Sliced-Wasserstrin Embedding for Multisets and Measures&quot;, ICLR 2025&#39;</span><span class="p">)</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos"> 523</span></a>
</span><span id="L-524"><a href="#L-524"><span class="linenos"> 524</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos"> 525</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Constructing embedding for sets in </span><span class="si">%s</span><span class="s1"> into </span><span class="si">%s</span><span class="s1">  &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_space_name</span><span class="p">,</span> <span class="n">output_space_name</span><span class="p">))</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos"> 526</span></a>
</span><span id="L-527"><a href="#L-527"><span class="linenos"> 527</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos"> 528</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%d</span><span class="s1"> frequencies, collapsed to one </span><span class="si">%d</span><span class="s1"> dimensional axis; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">num_slices</span><span class="o">*</span><span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos"> 529</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">:</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos"> 530</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> frequencies; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos"> 531</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos"> 532</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> (slice, frequency) pairs; &#39;</span> <span class="o">%</span> <span class="n">num_slices</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos"> 533</span></a>
</span><span id="L-534"><a href="#L-534"><span class="linenos"> 534</span></a>        <span class="n">qprint</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">slice_freq_str</span><span class="p">)</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos"> 535</span></a>
</span><span id="L-536"><a href="#L-536"><span class="linenos"> 536</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos"> 537</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos"> 538</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, frequences and biases&#39;</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos"> 539</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos"> 540</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and frequences, no bias&#39;</span>
</span><span id="L-541"><a href="#L-541"><span class="linenos"> 541</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos"> 542</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos"> 543</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and biases, fixed frequencies&#39;</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos"> 544</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos"> 545</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, fixed frequences, no bias&#39;</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos"> 546</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos"> 547</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos"> 548</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos"> 549</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos"> 550</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, no biases&#39;</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos"> 551</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos"> 552</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos"> 553</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos"> 554</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos"> 555</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, no bias&#39;</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos"> 556</span></a>
</span><span id="L-557"><a href="#L-557"><span class="linenos"> 557</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">learnable_str</span><span class="p">)</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos"> 558</span></a>
</span><span id="L-559"><a href="#L-559"><span class="linenos"> 559</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;device: </span><span class="si">%s</span><span class="s1">    dtype: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span><span class="p">))</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos"> 560</span></a>
</span><span id="L-561"><a href="#L-561"><span class="linenos"> 561</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos"> 562</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos"> 563</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos"> 564</span></a>
</span><span id="L-565"><a href="#L-565"><span class="linenos"> 565</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos"> 566</span></a>
</span><span id="L-567"><a href="#L-567"><span class="linenos"> 567</span></a>    <span class="nd">@classmethod</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos"> 568</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;FSWEmbedding&quot;</span><span class="p">:</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos"> 569</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos"> 570</span></a><span class="sd">        Construct an FSWEmbedding instance from a configuration dictionary.</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos"> 571</span></a>
</span><span id="L-572"><a href="#L-572"><span class="linenos"> 572</span></a><span class="sd">        Parameters</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos"> 573</span></a><span class="sd">        ----------</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos"> 574</span></a><span class="sd">        config : dict</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos"> 575</span></a><span class="sd">            Dictionary of keyword arguments matching the `__init__` parameters.</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos"> 576</span></a>
</span><span id="L-577"><a href="#L-577"><span class="linenos"> 577</span></a><span class="sd">        Returns</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos"> 578</span></a><span class="sd">        -------</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos"> 579</span></a><span class="sd">        FSWEmbedding</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos"> 580</span></a><span class="sd">            A new instance initialized with the provided configuration.</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos"> 581</span></a>
</span><span id="L-582"><a href="#L-582"><span class="linenos"> 582</span></a><span class="sd">        Raises</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos"> 583</span></a><span class="sd">        ------</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos"> 584</span></a><span class="sd">        TypeError</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos"> 585</span></a><span class="sd">            If any keys in the dictionary are not valid constructor arguments.</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos"> 586</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos"> 587</span></a>        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos"> 588</span></a>        <span class="n">valid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s1">&#39;self&#39;</span><span class="p">}</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos"> 589</span></a>        <span class="n">invalid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="o">-</span> <span class="n">valid_keys</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos"> 590</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos"> 591</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config key: &#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos"> 592</span></a>        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos"> 593</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config keys: </span><span class="si">{</span><span class="n">invalid_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos"> 594</span></a>        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos"> 595</span></a>
</span><span id="L-596"><a href="#L-596"><span class="linenos"> 596</span></a>    <span class="c1"># Resets the model parameters (slice vectors and frequencies) and updates the model settings.</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos"> 597</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos"> 598</span></a>                         <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos"> 599</span></a>                         <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos"> 600</span></a>                         <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos"> 601</span></a>                         <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos"> 602</span></a>
</span><span id="L-603"><a href="#L-603"><span class="linenos"> 603</span></a>        <span class="c1"># Apply user updates for these parameters</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos"> 604</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">)</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos"> 605</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">)</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos"> 606</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos"> 607</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report_on_coherence_minimization</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos"> 608</span></a>
</span><span id="L-609"><a href="#L-609"><span class="linenos"> 609</span></a>        <span class="c1"># To make sure we don&#39;t use these values inside the function; if any of then is None, we must use its self. counterpart.</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos"> 610</span></a>        <span class="k">del</span> <span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="n">frequency_init</span><span class="p">,</span> <span class="n">report</span><span class="p">,</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos"> 611</span></a>
</span><span id="L-612"><a href="#L-612"><span class="linenos"> 612</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos"> 613</span></a>
</span><span id="L-614"><a href="#L-614"><span class="linenos"> 614</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos"> 615</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Generating embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos"> 616</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos"> 617</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Resetting embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos"> 618</span></a>
</span><span id="L-619"><a href="#L-619"><span class="linenos"> 619</span></a>
</span><span id="L-620"><a href="#L-620"><span class="linenos"> 620</span></a>        <span class="c1"># If we&#39;re running for the first time, get the device and dtype that were set in the __init__ method;</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos"> 621</span></a>        <span class="c1"># otherwise use the current device and dtype.</span>
</span><span id="L-622"><a href="#L-622"><span class="linenos"> 622</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos"> 623</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos"> 624</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">)</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos"> 625</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos"> 626</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos"> 627</span></a>
</span><span id="L-628"><a href="#L-628"><span class="linenos"> 628</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">):</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos"> 629</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos"> 630</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">)</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos"> 631</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-632"><a href="#L-632"><span class="linenos"> 632</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos"> 633</span></a>
</span><span id="L-634"><a href="#L-634"><span class="linenos"> 634</span></a>
</span><span id="L-635"><a href="#L-635"><span class="linenos"> 635</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos"> 636</span></a>
</span><span id="L-637"><a href="#L-637"><span class="linenos"> 637</span></a>        <span class="c1"># Generate slice vectors and frequencies</span>
</span><span id="L-638"><a href="#L-638"><span class="linenos"> 638</span></a>        <span class="c1"># We always generate (and optimize) them in float64 and then convert to the desired dtype.</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos"> 639</span></a>        <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_generate_embedding_parameters</span><span class="p">(</span><span class="n">d_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos"> 640</span></a>                                                                                       <span class="n">num_slices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">,</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos"> 641</span></a>                                                                                       <span class="n">cartesian_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos"> 642</span></a>                                                                                       <span class="n">flatten_cartesian_axes</span> <span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span><span class="p">,</span>
</span><span id="L-643"><a href="#L-643"><span class="linenos"> 643</span></a>                                                                                       <span class="n">total_mass_encoding_dim</span><span class="o">=</span><span class="n">total_mass_encoding_dim</span><span class="p">,</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos"> 644</span></a>                                                                                       <span class="n">frequency_init</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">,</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos"> 645</span></a>                                                                                       <span class="n">minimize_slice_coherence</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">,</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos"> 646</span></a>                                                                                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos"> 647</span></a>                                                                                       <span class="n">report</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos"> 648</span></a>                                                                                       <span class="n">report_on_coherence_minimization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos"> 649</span></a>
</span><span id="L-650"><a href="#L-650"><span class="linenos"> 650</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos"> 651</span></a>        <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos"> 652</span></a>
</span><span id="L-653"><a href="#L-653"><span class="linenos"> 653</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="L-654"><a href="#L-654"><span class="linenos"> 654</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">)</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos"> 655</span></a>
</span><span id="L-656"><a href="#L-656"><span class="linenos"> 656</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos"> 657</span></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-658"><a href="#L-658"><span class="linenos"> 658</span></a>
</span><span id="L-659"><a href="#L-659"><span class="linenos"> 659</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos"> 660</span></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos"> 661</span></a>
</span><span id="L-662"><a href="#L-662"><span class="linenos"> 662</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos"> 663</span></a>
</span><span id="L-664"><a href="#L-664"><span class="linenos"> 664</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos"> 665</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos"> 666</span></a>
</span><span id="L-667"><a href="#L-667"><span class="linenos"> 667</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-668"><a href="#L-668"><span class="linenos"> 668</span></a>
</span><span id="L-669"><a href="#L-669"><span class="linenos"> 669</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos"> 670</span></a>
</span><span id="L-671"><a href="#L-671"><span class="linenos"> 671</span></a>
</span><span id="L-672"><a href="#L-672"><span class="linenos"> 672</span></a>
</span><span id="L-673"><a href="#L-673"><span class="linenos"> 673</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos"> 674</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the module to the specified device or dtype.</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos"> 675</span></a>
</span><span id="L-676"><a href="#L-676"><span class="linenos"> 676</span></a><span class="sd">        Example:</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos"> 677</span></a>
</span><span id="L-678"><a href="#L-678"><span class="linenos"> 678</span></a><span class="sd">            module.to(torch.float32)</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos"> 679</span></a><span class="sd">            module.to(device=&#39;cuda&#39;)</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos"> 680</span></a>
</span><span id="L-681"><a href="#L-681"><span class="linenos"> 681</span></a><span class="sd">        See also: torch.nn.Module.to()</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos"> 682</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-683"><a href="#L-683"><span class="linenos"> 683</span></a>        <span class="k">if</span> <span class="s1">&#39;dtype&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos"> 684</span></a>            <span class="n">arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos"> 685</span></a>
</span><span id="L-686"><a href="#L-686"><span class="linenos"> 686</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="s1">&#39;invalid input type </span><span class="si">%s</span><span class="s1"> at argument &#39;&#39;dtype&#39;&#39;&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos"> 687</span></a>            <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos"> 688</span></a>
</span><span id="L-689"><a href="#L-689"><span class="linenos"> 689</span></a>        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos"> 690</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos"> 691</span></a>                <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos"> 692</span></a>
</span><span id="L-693"><a href="#L-693"><span class="linenos"> 693</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos"> 694</span></a>
</span><span id="L-695"><a href="#L-695"><span class="linenos"> 695</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos"> 696</span></a>
</span><span id="L-697"><a href="#L-697"><span class="linenos"> 697</span></a>
</span><span id="L-698"><a href="#L-698"><span class="linenos"> 698</span></a>    <span class="nd">@property</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos"> 699</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos"> 700</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of slices used in the embedding.&quot;&quot;&quot;</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos"> 701</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos"> 702</span></a>
</span><span id="L-703"><a href="#L-703"><span class="linenos"> 703</span></a>    <span class="nd">@property</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos"> 704</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-705"><a href="#L-705"><span class="linenos"> 705</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.&quot;&quot;&quot;</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos"> 706</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos"> 707</span></a>
</span><span id="L-708"><a href="#L-708"><span class="linenos"> 708</span></a>    <span class="nd">@property</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos"> 709</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">cartesian_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos"> 710</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos"> 711</span></a><span class="sd">        and frequencies.</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos"> 712</span></a><span class="sd">        In Cartesian mode, the embeding dimension is `d_out` = `num_slices × num_frequencies`.</span>
</span><span id="L-713"><a href="#L-713"><span class="linenos"> 713</span></a><span class="sd">        Cartesian mode is activated by providing `num_slices` and `num_frequencies` to `FSWEmbedding.__init__`bool</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos"> 714</span></a><span class="sd">        See also: `flatten_cartesian_axes`&quot;&quot;&quot;</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos"> 715</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos"> 716</span></a>
</span><span id="L-717"><a href="#L-717"><span class="linenos"> 717</span></a>    <span class="nd">@property</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos"> 718</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">flatten_cartesian_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos"> 719</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos"> 720</span></a><span class="sd">        If True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (`num_slices`, `num_frequencies`).</span>
</span><span id="L-721"><a href="#L-721"><span class="linenos"> 721</span></a><span class="sd">        If False, the otput is shaped `num_slices` × `num_frequencies`.</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos"> 722</span></a><span class="sd">        This setting is only relevant if `cartesian_mode` is True.&quot;&quot;&quot;</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos"> 723</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos"> 724</span></a>
</span><span id="L-725"><a href="#L-725"><span class="linenos"> 725</span></a>    <span class="nd">@property</span>
</span><span id="L-726"><a href="#L-726"><span class="linenos"> 726</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos"> 727</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether slice directions are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos"> 728</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span>
</span><span id="L-729"><a href="#L-729"><span class="linenos"> 729</span></a>
</span><span id="L-730"><a href="#L-730"><span class="linenos"> 730</span></a>    <span class="nd">@property</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos"> 731</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos"> 732</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether frequency values are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos"> 733</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos"> 734</span></a>
</span><span id="L-735"><a href="#L-735"><span class="linenos"> 735</span></a>    <span class="nd">@property</span>
</span><span id="L-736"><a href="#L-736"><span class="linenos"> 736</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">enable_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-737"><a href="#L-737"><span class="linenos"> 737</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether a learnable bias vector is added to the output embedding.&quot;&quot;&quot;</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos"> 738</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos"> 739</span></a>
</span><span id="L-740"><a href="#L-740"><span class="linenos"> 740</span></a>    <span class="nd">@property</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos"> 741</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">encode_total_mass</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos"> 742</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the total mass of the input measure is encoded into the embedding.&quot;&quot;&quot;</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos"> 743</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span>
</span><span id="L-744"><a href="#L-744"><span class="linenos"> 744</span></a>
</span><span id="L-745"><a href="#L-745"><span class="linenos"> 745</span></a>    <span class="nd">@property</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos"> 746</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_transformation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingTransformation</span><span class="p">:</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos"> 747</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Function applied to the total mass before it is stored.&quot;&quot;&quot;</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos"> 748</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos"> 749</span></a>
</span><span id="L-750"><a href="#L-750"><span class="linenos"> 750</span></a>    <span class="nd">@property</span>
</span><span id="L-751"><a href="#L-751"><span class="linenos"> 751</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingMethod</span><span class="p">:</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos"> 752</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Strategy used to incorporate total mass into the final embedding vector.&quot;&quot;&quot;</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos"> 753</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos"> 754</span></a>
</span><span id="L-755"><a href="#L-755"><span class="linenos"> 755</span></a>    <span class="nd">@property</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos"> 756</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos"> 757</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The encoded total mass is scaled by this factor.&quot;&quot;&quot;</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos"> 758</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos"> 759</span></a>
</span><span id="L-760"><a href="#L-760"><span class="linenos"> 760</span></a>    <span class="nd">@property</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos"> 761</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_padding_thresh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos"> 762</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Minimum total mass threshold; inputs below this value are padded to reach it.&quot;&quot;&quot;</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos"> 763</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos"> 764</span></a>
</span><span id="L-765"><a href="#L-765"><span class="linenos"> 765</span></a>
</span><span id="L-766"><a href="#L-766"><span class="linenos"> 766</span></a>    <span class="nd">@property</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos"> 767</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_in</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos"> 768</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Ambient dimension of the input elements.</span>
</span><span id="L-769"><a href="#L-769"><span class="linenos"> 769</span></a>
</span><span id="L-770"><a href="#L-770"><span class="linenos"> 770</span></a><span class="sd">        Returns</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos"> 771</span></a><span class="sd">        -------</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos"> 772</span></a><span class="sd">        int</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos"> 773</span></a><span class="sd">            The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos"> 774</span></a>
</span><span id="L-775"><a href="#L-775"><span class="linenos"> 775</span></a><span class="sd">        Notes</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos"> 776</span></a><span class="sd">        -----</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos"> 777</span></a><span class="sd">        This value is set at initialization and determines the expected feature dimension of input points.</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos"> 778</span></a>
</span><span id="L-779"><a href="#L-779"><span class="linenos"> 779</span></a><span class="sd">        See Also</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos"> 780</span></a><span class="sd">        --------</span>
</span><span id="L-781"><a href="#L-781"><span class="linenos"> 781</span></a><span class="sd">        __init__ : The `d_in` argument specifies this value at initialization.</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos"> 782</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos"> 783</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos"> 784</span></a>
</span><span id="L-785"><a href="#L-785"><span class="linenos"> 785</span></a>
</span><span id="L-786"><a href="#L-786"><span class="linenos"> 786</span></a>    <span class="nd">@property</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos"> 787</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_out</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="L-788"><a href="#L-788"><span class="linenos"> 788</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Dimensionality of the embedding output.</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos"> 789</span></a>
</span><span id="L-790"><a href="#L-790"><span class="linenos"> 790</span></a><span class="sd">        Returns</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos"> 791</span></a><span class="sd">        -------</span>
</span><span id="L-792"><a href="#L-792"><span class="linenos"> 792</span></a><span class="sd">        int</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos"> 793</span></a><span class="sd">            The dimension of the vector produced by the embedding for each multiset or distribution.</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos"> 794</span></a>
</span><span id="L-795"><a href="#L-795"><span class="linenos"> 795</span></a><span class="sd">        Notes</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos"> 796</span></a><span class="sd">        -----</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos"> 797</span></a><span class="sd">        This value is set at initialization and governs the size of the embedding output.</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos"> 798</span></a>
</span><span id="L-799"><a href="#L-799"><span class="linenos"> 799</span></a><span class="sd">        See Also</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos"> 800</span></a><span class="sd">        --------</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos"> 801</span></a><span class="sd">        __init__ : The `d_out` argument specifies this value at initialization.</span>
</span><span id="L-802"><a href="#L-802"><span class="linenos"> 802</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos"> 803</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos"> 804</span></a>
</span><span id="L-805"><a href="#L-805"><span class="linenos"> 805</span></a>    <span class="nd">@property</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos"> 806</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos"> 807</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.device: The device on which the module&#39;s parameters and buffers are stored.</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos"> 808</span></a>
</span><span id="L-809"><a href="#L-809"><span class="linenos"> 809</span></a><span class="sd">        Returns</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos"> 810</span></a><span class="sd">        -------</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos"> 811</span></a><span class="sd">        torch.device</span>
</span><span id="L-812"><a href="#L-812"><span class="linenos"> 812</span></a><span class="sd">            The PyTorch device (`&#39;cpu&#39;`, `&#39;cuda&#39;`, etc.) where the embedding computations will take place.</span>
</span><span id="L-813"><a href="#L-813"><span class="linenos"> 813</span></a>
</span><span id="L-814"><a href="#L-814"><span class="linenos"> 814</span></a><span class="sd">        Notes</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos"> 815</span></a><span class="sd">        -----</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos"> 816</span></a><span class="sd">        This behaves like the `device` property in standard PyTorch modules.</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos"> 817</span></a>
</span><span id="L-818"><a href="#L-818"><span class="linenos"> 818</span></a><span class="sd">        See Also</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos"> 819</span></a><span class="sd">        --------</span>
</span><span id="L-820"><a href="#L-820"><span class="linenos"> 820</span></a><span class="sd">        __init__ : The `device` can be specified at initialization.</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos"> 821</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-822"><a href="#L-822"><span class="linenos"> 822</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos"> 823</span></a>
</span><span id="L-824"><a href="#L-824"><span class="linenos"> 824</span></a>
</span><span id="L-825"><a href="#L-825"><span class="linenos"> 825</span></a>    <span class="nd">@property</span>
</span><span id="L-826"><a href="#L-826"><span class="linenos"> 826</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos"> 827</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.dtype: The default data type used by the module.</span>
</span><span id="L-828"><a href="#L-828"><span class="linenos"> 828</span></a>
</span><span id="L-829"><a href="#L-829"><span class="linenos"> 829</span></a><span class="sd">        Returns</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos"> 830</span></a><span class="sd">        -------</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos"> 831</span></a><span class="sd">        torch.dtype</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos"> 832</span></a><span class="sd">            The data type (`torch.float32`, `torch.float64`, etc.) of the module’s parameters and buffers.</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos"> 833</span></a>
</span><span id="L-834"><a href="#L-834"><span class="linenos"> 834</span></a><span class="sd">        Notes</span>
</span><span id="L-835"><a href="#L-835"><span class="linenos"> 835</span></a><span class="sd">        -----</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos"> 836</span></a><span class="sd">        This behaves like the `dtype` property in standard PyTorch modules.</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos"> 837</span></a>
</span><span id="L-838"><a href="#L-838"><span class="linenos"> 838</span></a><span class="sd">        See Also</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos"> 839</span></a><span class="sd">        --------</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos"> 840</span></a><span class="sd">        __init__ : The `dtype` can be specified at initialization.</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos"> 841</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-842"><a href="#L-842"><span class="linenos"> 842</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos"> 843</span></a>
</span><span id="L-844"><a href="#L-844"><span class="linenos"> 844</span></a>
</span><span id="L-845"><a href="#L-845"><span class="linenos"> 845</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos"> 846</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_embedding_parameters</span><span class="p">(</span><span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos"> 847</span></a>                                       <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos"> 848</span></a>                                       <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos"> 849</span></a>                                       <span class="n">cartesian_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos"> 850</span></a>                                       <span class="n">flatten_cartesian_axes</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-851"><a href="#L-851"><span class="linenos"> 851</span></a>                                       <span class="n">total_mass_encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos"> 852</span></a>                                       <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span><span class="p">,</span>
</span><span id="L-853"><a href="#L-853"><span class="linenos"> 853</span></a>                                       <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos"> 854</span></a>                                       <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-855"><a href="#L-855"><span class="linenos"> 855</span></a>                                       <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos"> 856</span></a>                                       <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos"> 857</span></a>        <span class="n">dtype_init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos"> 858</span></a>
</span><span id="L-859"><a href="#L-859"><span class="linenos"> 859</span></a>        <span class="c1"># Axis number for the ambient space R^d_in</span>
</span><span id="L-860"><a href="#L-860"><span class="linenos"> 860</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-861"><a href="#L-861"><span class="linenos"> 861</span></a>
</span><span id="L-862"><a href="#L-862"><span class="linenos"> 862</span></a>        <span class="c1">### A. Generate slice vectors</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos"> 863</span></a>
</span><span id="L-864"><a href="#L-864"><span class="linenos"> 864</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-865"><a href="#L-865"><span class="linenos"> 865</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">ambspace_axis</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos"> 866</span></a>
</span><span id="L-867"><a href="#L-867"><span class="linenos"> 867</span></a>        <span class="k">if</span> <span class="n">minimize_slice_coherence</span><span class="p">:</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos"> 868</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">num_slices</span> <span class="o">&gt;</span> <span class="n">d_in</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos"> 869</span></a>                <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">minimize_mutual_coherence</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="n">report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos"> 870</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> slice vectors in R^</span><span class="si">%d</span><span class="s1"> with minimized mutual coherence&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">))</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos"> 871</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos"> 872</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;num_slices: &#39;</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">,</span> <span class="s1">&#39;d_in: &#39;</span><span class="p">,</span> <span class="n">d_in</span><span class="p">)</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos"> 873</span></a>                <span class="c1"># Here we need to compute a set of num_slices orthogonal vectors in R^d_in.</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos"> 874</span></a>                <span class="c1"># Below are two methods to do so: SVD and QR decomposition</span>
</span><span id="L-875"><a href="#L-875"><span class="linenos"> 875</span></a>                <span class="c1"># In some cases with little available memory, SVD seems more resilient, whereas QR sometimes crashes.</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos"> 876</span></a>                <span class="n">use_svd</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos"> 877</span></a>
</span><span id="L-878"><a href="#L-878"><span class="linenos"> 878</span></a>                <span class="k">if</span> <span class="n">use_svd</span><span class="p">:</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos"> 879</span></a>                    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-880"><a href="#L-880"><span class="linenos"> 880</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">Vh</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos"> 881</span></a>                    <span class="k">del</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span>
</span><span id="L-882"><a href="#L-882"><span class="linenos"> 882</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos"> 883</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-884"><a href="#L-884"><span class="linenos"> 884</span></a>                    <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;reduced&#39;</span><span class="p">)</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos"> 885</span></a>                    <span class="k">del</span> <span class="n">R</span>
</span><span id="L-886"><a href="#L-886"><span class="linenos"> 886</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-887"><a href="#L-887"><span class="linenos"> 887</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> perpendicular slice vectors in R^</span><span class="si">%d</span><span class="s1"> using QR decomposition&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">))</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos"> 888</span></a>
</span><span id="L-889"><a href="#L-889"><span class="linenos"> 889</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-890"><a href="#L-890"><span class="linenos"> 890</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> random slice vectors&#39;</span> <span class="o">%</span> <span class="n">num_slices</span><span class="p">)</span>
</span><span id="L-891"><a href="#L-891"><span class="linenos"> 891</span></a>
</span><span id="L-892"><a href="#L-892"><span class="linenos"> 892</span></a>        <span class="c1"># Detect nans, infs and zero vectors in slice_vectors</span>
</span><span id="L-893"><a href="#L-893"><span class="linenos"> 893</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found infs in slice_vectors&quot;</span>
</span><span id="L-894"><a href="#L-894"><span class="linenos"> 894</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found nans in slice_vectors&quot;</span>
</span><span id="L-895"><a href="#L-895"><span class="linenos"> 895</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">slice_vectors</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Found zero vectors in slice_vectors&#39;</span>
</span><span id="L-896"><a href="#L-896"><span class="linenos"> 896</span></a>
</span><span id="L-897"><a href="#L-897"><span class="linenos"> 897</span></a>
</span><span id="L-898"><a href="#L-898"><span class="linenos"> 898</span></a>
</span><span id="L-899"><a href="#L-899"><span class="linenos"> 899</span></a>        <span class="c1">### B. Generate frequencies</span>
</span><span id="L-900"><a href="#L-900"><span class="linenos"> 900</span></a>        <span class="n">freqs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,)</span> <span class="c1"># Note: Changing this to (self.num_frequencies, 1) yields incorrect results in self.forward()</span>
</span><span id="L-901"><a href="#L-901"><span class="linenos"> 901</span></a>
</span><span id="L-902"><a href="#L-902"><span class="linenos"> 902</span></a>        <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-903"><a href="#L-903"><span class="linenos"> 903</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-904"><a href="#L-904"><span class="linenos"> 904</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized 0 frequencies&#39;</span><span class="p">)</span>
</span><span id="L-905"><a href="#L-905"><span class="linenos"> 905</span></a>
</span><span id="L-906"><a href="#L-906"><span class="linenos"> 906</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
</span><span id="L-907"><a href="#L-907"><span class="linenos"> 907</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">),</span> <span class="s1">&#39;frequency_init cannot be infinite&#39;</span>
</span><span id="L-908"><a href="#L-908"><span class="linenos"> 908</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">),</span> <span class="s1">&#39;frequency_init cannot be NaN&#39;</span>
</span><span id="L-909"><a href="#L-909"><span class="linenos"> 909</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequency_init</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-910"><a href="#L-910"><span class="linenos"> 910</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> frequencies to </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">frequency_init</span><span class="p">))</span>
</span><span id="L-911"><a href="#L-911"><span class="linenos"> 911</span></a>
</span><span id="L-912"><a href="#L-912"><span class="linenos"> 912</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span><span id="L-913"><a href="#L-913"><span class="linenos"> 913</span></a>            <span class="c1"># Here frequency_init should have been type-enforced to be a tuple of two real numbers.</span>
</span><span id="L-914"><a href="#L-914"><span class="linenos"> 914</span></a>            <span class="c1"># However, it does not prevent the tuple from containing more numbers.</span>
</span><span id="L-915"><a href="#L-915"><span class="linenos"> 915</span></a>            <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;When frequency_init is a tuple, it must be of length 2&#39;</span>
</span><span id="L-916"><a href="#L-916"><span class="linenos"> 916</span></a>
</span><span id="L-917"><a href="#L-917"><span class="linenos"> 917</span></a>            <span class="n">a</span> <span class="o">=</span> <span class="n">frequency_init</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-918"><a href="#L-918"><span class="linenos"> 918</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">frequency_init</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-919"><a href="#L-919"><span class="linenos"> 919</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="s1">&#39;Received infinite value in frequency_init tuple&#39;</span>
</span><span id="L-920"><a href="#L-920"><span class="linenos"> 920</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="s1">&#39;Received NaN value in frequency_init tuple&#39;</span>
</span><span id="L-921"><a href="#L-921"><span class="linenos"> 921</span></a>            <span class="k">assert</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;When frequency_init is a tuple, it is required to satisfy frequency_init[0] &lt;= frequency_init[1]&#39;</span>
</span><span id="L-922"><a href="#L-922"><span class="linenos"> 922</span></a>
</span><span id="L-923"><a href="#L-923"><span class="linenos"> 923</span></a>            <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-924"><a href="#L-924"><span class="linenos"> 924</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-925"><a href="#L-925"><span class="linenos"> 925</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-926"><a href="#L-926"><span class="linenos"> 926</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-927"><a href="#L-927"><span class="linenos"> 927</span></a>
</span><span id="L-928"><a href="#L-928"><span class="linenos"> 928</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> equispaced frequencies in the interval [</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>
</span><span id="L-929"><a href="#L-929"><span class="linenos"> 929</span></a>
</span><span id="L-930"><a href="#L-930"><span class="linenos"> 930</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-931"><a href="#L-931"><span class="linenos"> 931</span></a>            <span class="n">frequency_init</span> <span class="o">=</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">)</span>
</span><span id="L-932"><a href="#L-932"><span class="linenos"> 932</span></a>
</span><span id="L-933"><a href="#L-933"><span class="linenos"> 933</span></a>            <span class="k">if</span> <span class="n">frequency_init</span> <span class="o">==</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">RANDOM</span><span class="p">:</span>
</span><span id="L-934"><a href="#L-934"><span class="linenos"> 934</span></a>                <span class="n">frequencies</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-935"><a href="#L-935"><span class="linenos"> 935</span></a>                <span class="n">frequencies</span><span class="p">,</span> <span class="n">junk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-936"><a href="#L-936"><span class="linenos"> 936</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">frequencies</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;Unexpected behavior of torch.rand(): Returned a value of 1, whereas values are supposed to be in [0,1)&quot;</span>
</span><span id="L-937"><a href="#L-937"><span class="linenos"> 937</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">frequencies</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;Unexpected behavior of torch.rand(): Returned a value &gt; 1, whereas values are supposed to be in [0,1)&quot;</span>
</span><span id="L-938"><a href="#L-938"><span class="linenos"> 938</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="L-939"><a href="#L-939"><span class="linenos"> 939</span></a>
</span><span id="L-940"><a href="#L-940"><span class="linenos"> 940</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> random frequencies i.i.d. with density f(x) = 1/(1+x)^2, x</span><span class="se">\u2265</span><span class="s1">0&#39;</span> <span class="o">%</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="L-941"><a href="#L-941"><span class="linenos"> 941</span></a>
</span><span id="L-942"><a href="#L-942"><span class="linenos"> 942</span></a>            <span class="k">elif</span> <span class="n">frequency_init</span> <span class="o">==</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">EVEN</span><span class="p">:</span>
</span><span id="L-943"><a href="#L-943"><span class="linenos"> 943</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="p">(</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">freqs_shape</span><span class="p">)</span> <span class="p">)</span> <span class="o">/</span> <span class="n">num_frequencies</span>
</span><span id="L-944"><a href="#L-944"><span class="linenos"> 944</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="L-945"><a href="#L-945"><span class="linenos"> 945</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> frequencies spread evenly in [</span><span class="si">%g</span><span class="s1">, </span><span class="si">%g</span><span class="s1">] according to probability density&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">frequencies</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</span><span id="L-946"><a href="#L-946"><span class="linenos"> 946</span></a>
</span><span id="L-947"><a href="#L-947"><span class="linenos"> 947</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-948"><a href="#L-948"><span class="linenos"> 948</span></a>                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Invalid value for argument frequency_init; expected number, tuple (a,b) of numbers denoting an interval, </span><span class="se">\&#39;</span><span class="s1">random</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">spread</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="L-949"><a href="#L-949"><span class="linenos"> 949</span></a>
</span><span id="L-950"><a href="#L-950"><span class="linenos"> 950</span></a>        <span class="c1"># Detect nan and inf entries in frequencies</span>
</span><span id="L-951"><a href="#L-951"><span class="linenos"> 951</span></a>        <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-952"><a href="#L-952"><span class="linenos"> 952</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found infs in frequencies&quot;</span>
</span><span id="L-953"><a href="#L-953"><span class="linenos"> 953</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found nans in frequencies&quot;</span>
</span><span id="L-954"><a href="#L-954"><span class="linenos"> 954</span></a>
</span><span id="L-955"><a href="#L-955"><span class="linenos"> 955</span></a>        <span class="c1"># C. Generate bias vector. Always initialized to zero.</span>
</span><span id="L-956"><a href="#L-956"><span class="linenos"> 956</span></a>        <span class="k">if</span> <span class="n">cartesian_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="L-957"><a href="#L-957"><span class="linenos"> 957</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="L-958"><a href="#L-958"><span class="linenos"> 958</span></a>        <span class="k">elif</span> <span class="n">cartesian_mode</span> <span class="ow">and</span> <span class="n">flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="L-959"><a href="#L-959"><span class="linenos"> 959</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span><span class="o">*</span><span class="n">num_frequencies</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span><span class="p">,)</span>
</span><span id="L-960"><a href="#L-960"><span class="linenos"> 960</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-961"><a href="#L-961"><span class="linenos"> 961</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span><span class="p">,)</span>
</span><span id="L-962"><a href="#L-962"><span class="linenos"> 962</span></a>
</span><span id="L-963"><a href="#L-963"><span class="linenos"> 963</span></a>        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-964"><a href="#L-964"><span class="linenos"> 964</span></a>
</span><span id="L-965"><a href="#L-965"><span class="linenos"> 965</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-966"><a href="#L-966"><span class="linenos"> 966</span></a>
</span><span id="L-967"><a href="#L-967"><span class="linenos"> 967</span></a>        <span class="k">return</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">bias</span>
</span><span id="L-968"><a href="#L-968"><span class="linenos"> 968</span></a>
</span><span id="L-969"><a href="#L-969"><span class="linenos"> 969</span></a>
</span><span id="L-970"><a href="#L-970"><span class="linenos"> 970</span></a>
</span><span id="L-971"><a href="#L-971"><span class="linenos"> 971</span></a>    <span class="c1"># Spreads the frequencies on an interval centered at &#39;center&#39; with the given radius, in an equispaced manner.</span>
</span><span id="L-972"><a href="#L-972"><span class="linenos"> 972</span></a>    <span class="c1"># This might be useful when using the embedding for graph message passing with learnable_slices=True, as the magnitude of the</span>
</span><span id="L-973"><a href="#L-973"><span class="linenos"> 973</span></a>    <span class="c1"># slice vectors already determines the effective frequency, and having a very high max-frequency-to-low-frequency ratio</span>
</span><span id="L-974"><a href="#L-974"><span class="linenos"> 974</span></a>    <span class="c1"># may impede the optimization due to ill conditioning.</span>
</span><span id="L-975"><a href="#L-975"><span class="linenos"> 975</span></a>
</span><span id="L-976"><a href="#L-976"><span class="linenos"> 976</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_spread_freqs_at_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">center</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="L-977"><a href="#L-977"><span class="linenos"> 977</span></a>        <span class="k">assert</span> <span class="n">radius</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</span><span id="L-978"><a href="#L-978"><span class="linenos"> 978</span></a>
</span><span id="L-979"><a href="#L-979"><span class="linenos"> 979</span></a>        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">radius</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="L-980"><a href="#L-980"><span class="linenos"> 980</span></a>            <span class="n">freqs_new</span> <span class="o">=</span> <span class="n">center</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="L-981"><a href="#L-981"><span class="linenos"> 981</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-982"><a href="#L-982"><span class="linenos"> 982</span></a>            <span class="n">spread</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-983"><a href="#L-983"><span class="linenos"> 983</span></a>            <span class="n">spread</span> <span class="o">=</span> <span class="n">spread</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">)</span>
</span><span id="L-984"><a href="#L-984"><span class="linenos"> 984</span></a>            <span class="n">freqs_new</span> <span class="o">=</span> <span class="n">center</span> <span class="o">+</span> <span class="n">radius</span> <span class="o">*</span> <span class="n">spread</span>
</span><span id="L-985"><a href="#L-985"><span class="linenos"> 985</span></a>
</span><span id="L-986"><a href="#L-986"><span class="linenos"> 986</span></a>        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</span><span id="L-987"><a href="#L-987"><span class="linenos"> 987</span></a>        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;frequencies&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">freqs_new</span>
</span><span id="L-988"><a href="#L-988"><span class="linenos"> 988</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</span><span id="L-989"><a href="#L-989"><span class="linenos"> 989</span></a>
</span><span id="L-990"><a href="#L-990"><span class="linenos"> 990</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="L-991"><a href="#L-991"><span class="linenos"> 991</span></a>
</span><span id="L-992"><a href="#L-992"><span class="linenos"> 992</span></a>
</span><span id="L-993"><a href="#L-993"><span class="linenos"> 993</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="L-994"><a href="#L-994"><span class="linenos"> 994</span></a>                <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-995"><a href="#L-995"><span class="linenos"> 995</span></a>                <span class="n">W</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="s1">&#39;unit&#39;</span><span class="p">,</span>
</span><span id="L-996"><a href="#L-996"><span class="linenos"> 996</span></a>                <span class="n">X_edge</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-997"><a href="#L-997"><span class="linenos"> 997</span></a>                <span class="n">graph_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-998"><a href="#L-998"><span class="linenos"> 998</span></a>                <span class="n">max_parallel_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-999"><a href="#L-999"><span class="linenos"> 999</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-1000"><a href="#L-1000"><span class="linenos">1000</span></a><span class="sd">        Compute the FSW embedding of an input multiset, measure, or graph.</span>
</span><span id="L-1001"><a href="#L-1001"><span class="linenos">1001</span></a>
</span><span id="L-1002"><a href="#L-1002"><span class="linenos">1002</span></a><span class="sd">        This method maps input sets of vectors (optionally weighted) to vectors in ℝ^{d_out}</span>
</span><span id="L-1003"><a href="#L-1003"><span class="linenos">1003</span></a><span class="sd">        using the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and</span>
</span><span id="L-1004"><a href="#L-1004"><span class="linenos">1004</span></a><span class="sd">        graph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</span>
</span><span id="L-1005"><a href="#L-1005"><span class="linenos">1005</span></a>
</span><span id="L-1006"><a href="#L-1006"><span class="linenos">1006</span></a><span class="sd">        Parameters</span>
</span><span id="L-1007"><a href="#L-1007"><span class="linenos">1007</span></a><span class="sd">        ----------</span>
</span><span id="L-1008"><a href="#L-1008"><span class="linenos">1008</span></a><span class="sd">        X : torch.Tensor</span>
</span><span id="L-1009"><a href="#L-1009"><span class="linenos">1009</span></a><span class="sd">            Input tensor of shape `(n, d_in)` or `(..., n, d_in)` for batched input.</span>
</span><span id="L-1010"><a href="#L-1010"><span class="linenos">1010</span></a><span class="sd">        W : torch.Tensor or {&#39;unit&#39;, &#39;uniform&#39;}, default=&#39;unit&#39;</span>
</span><span id="L-1011"><a href="#L-1011"><span class="linenos">1011</span></a><span class="sd">            Weights tensor of shape `(n,)` or `(..., n)` corresponding to point importance.</span>
</span><span id="L-1012"><a href="#L-1012"><span class="linenos">1012</span></a><span class="sd">            If set to `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights of `1/n` are assumed.</span>
</span><span id="L-1013"><a href="#L-1013"><span class="linenos">1013</span></a><span class="sd">        X_edge : torch.Tensor, optional</span>
</span><span id="L-1014"><a href="#L-1014"><span class="linenos">1014</span></a><span class="sd">            Optional edge feature tensor. Required if `d_edge &gt; 0` was set at initialization.</span>
</span><span id="L-1015"><a href="#L-1015"><span class="linenos">1015</span></a><span class="sd">        graph_mode : bool, default=False</span>
</span><span id="L-1016"><a href="#L-1016"><span class="linenos">1016</span></a><span class="sd">            If True, interprets `W` as an adjacency matrix and computes a neighbor-aggregated</span>
</span><span id="L-1017"><a href="#L-1017"><span class="linenos">1017</span></a><span class="sd">            embedding.</span>
</span><span id="L-1018"><a href="#L-1018"><span class="linenos">1018</span></a><span class="sd">        max_parallel_slices : int, optional</span>
</span><span id="L-1019"><a href="#L-1019"><span class="linenos">1019</span></a><span class="sd">            Limits the number of slices processed in parallel. Reduces memory usage by computing</span>
</span><span id="L-1020"><a href="#L-1020"><span class="linenos">1020</span></a><span class="sd">            the embedding in smaller blocks without changing the result.</span>
</span><span id="L-1021"><a href="#L-1021"><span class="linenos">1021</span></a>
</span><span id="L-1022"><a href="#L-1022"><span class="linenos">1022</span></a><span class="sd">        Returns</span>
</span><span id="L-1023"><a href="#L-1023"><span class="linenos">1023</span></a><span class="sd">        -------</span>
</span><span id="L-1024"><a href="#L-1024"><span class="linenos">1024</span></a><span class="sd">        torch.Tensor</span>
</span><span id="L-1025"><a href="#L-1025"><span class="linenos">1025</span></a><span class="sd">            The embedding tensor. Shape depends on the mode:</span>
</span><span id="L-1026"><a href="#L-1026"><span class="linenos">1026</span></a><span class="sd">            - `(d_out,)` or `(..., d_out)` in standard mode.</span>
</span><span id="L-1027"><a href="#L-1027"><span class="linenos">1027</span></a><span class="sd">            - `(..., num_slices, num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=False`.</span>
</span><span id="L-1028"><a href="#L-1028"><span class="linenos">1028</span></a><span class="sd">            - `(..., num_slices * num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=True`.</span>
</span><span id="L-1029"><a href="#L-1029"><span class="linenos">1029</span></a>
</span><span id="L-1030"><a href="#L-1030"><span class="linenos">1030</span></a><span class="sd">        Notes</span>
</span><span id="L-1031"><a href="#L-1031"><span class="linenos">1031</span></a><span class="sd">        -----</span>
</span><span id="L-1032"><a href="#L-1032"><span class="linenos">1032</span></a><span class="sd">        Multisets and distributions:</span>
</span><span id="L-1033"><a href="#L-1033"><span class="linenos">1033</span></a><span class="sd">            If `X` is `(n, d_in)` and `W` is `(n,)`, the pair represents a weighted point cloud.</span>
</span><span id="L-1034"><a href="#L-1034"><span class="linenos">1034</span></a><span class="sd">            Weights must be non-negative with positive total mass.</span>
</span><span id="L-1035"><a href="#L-1035"><span class="linenos">1035</span></a><span class="sd">            If `W` is `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights are used internally.</span>
</span><span id="L-1036"><a href="#L-1036"><span class="linenos">1036</span></a>
</span><span id="L-1037"><a href="#L-1037"><span class="linenos">1037</span></a><span class="sd">        Batching:</span>
</span><span id="L-1038"><a href="#L-1038"><span class="linenos">1038</span></a><span class="sd">            Input tensors may include leading batch dimensions. For `X` of shape `(..., n, d_in)`</span>
</span><span id="L-1039"><a href="#L-1039"><span class="linenos">1039</span></a><span class="sd">            and `W` of shape `(..., n)`, the output shape is `(..., d_out)`.</span>
</span><span id="L-1040"><a href="#L-1040"><span class="linenos">1040</span></a>
</span><span id="L-1041"><a href="#L-1041"><span class="linenos">1041</span></a><span class="sd">        Graph mode:</span>
</span><span id="L-1042"><a href="#L-1042"><span class="linenos">1042</span></a><span class="sd">            When `graph_mode=True`, `W` must be of shape `(..., n_recipients, n)` and `X` of</span>
</span><span id="L-1043"><a href="#L-1043"><span class="linenos">1043</span></a><span class="sd">            shape `(..., n, d_in)` or broadcastable to that. The output will be</span>
</span><span id="L-1044"><a href="#L-1044"><span class="linenos">1044</span></a><span class="sd">            `(..., n_recipients, d_out)`, where each vector represents a weighted embedding of</span>
</span><span id="L-1045"><a href="#L-1045"><span class="linenos">1045</span></a><span class="sd">            neighboring nodes. This avoids expanding `X` across `n_recipients` explicitly.</span>
</span><span id="L-1046"><a href="#L-1046"><span class="linenos">1046</span></a>
</span><span id="L-1047"><a href="#L-1047"><span class="linenos">1047</span></a><span class="sd">        Cartesian mode:</span>
</span><span id="L-1048"><a href="#L-1048"><span class="linenos">1048</span></a><span class="sd">            If `d_out` is not specified but `num_slices` and `num_frequencies` are, the embedding</span>
</span><span id="L-1049"><a href="#L-1049"><span class="linenos">1049</span></a><span class="sd">            is computed over a Cartesian product. The output shape is:</span>
</span><span id="L-1050"><a href="#L-1050"><span class="linenos">1050</span></a><span class="sd">                - `(..., num_slices, num_frequencies)` if `flatten_cartesian_axes=False`</span>
</span><span id="L-1051"><a href="#L-1051"><span class="linenos">1051</span></a><span class="sd">                - `(..., num_slices * num_frequencies)` if `flatten_cartesian_axes=True`</span>
</span><span id="L-1052"><a href="#L-1052"><span class="linenos">1052</span></a>
</span><span id="L-1053"><a href="#L-1053"><span class="linenos">1053</span></a><span class="sd">        Slice serialization:</span>
</span><span id="L-1054"><a href="#L-1054"><span class="linenos">1054</span></a><span class="sd">            If `max_parallel_slices=t` is set, the computation is performed in blocks of size `t`,</span>
</span><span id="L-1055"><a href="#L-1055"><span class="linenos">1055</span></a><span class="sd">            reducing memory complexity by a factor of `num_slices / t`. The output remains unchanged.</span>
</span><span id="L-1056"><a href="#L-1056"><span class="linenos">1056</span></a>
</span><span id="L-1057"><a href="#L-1057"><span class="linenos">1057</span></a><span class="sd">        See Also</span>
</span><span id="L-1058"><a href="#L-1058"><span class="linenos">1058</span></a><span class="sd">        --------</span>
</span><span id="L-1059"><a href="#L-1059"><span class="linenos">1059</span></a><span class="sd">        FSWEmbedding.__init__ : Constructor for model configuration options.</span>
</span><span id="L-1060"><a href="#L-1060"><span class="linenos">1060</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-1061"><a href="#L-1061"><span class="linenos">1061</span></a>
</span><span id="L-1062"><a href="#L-1062"><span class="linenos">1062</span></a>
</span><span id="L-1063"><a href="#L-1063"><span class="linenos">1063</span></a>        <span class="c1"># Verify slices and frequencies at each forward pass if they are learnable</span>
</span><span id="L-1064"><a href="#L-1064"><span class="linenos">1064</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="L-1065"><a href="#L-1065"><span class="linenos">1065</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain NaNs&#39;</span>
</span><span id="L-1066"><a href="#L-1066"><span class="linenos">1066</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain infs&#39;</span>
</span><span id="L-1067"><a href="#L-1067"><span class="linenos">1067</span></a>            <span class="c1"># Note: We allow them to contain zero vectors when they are learnable, in case i.e. when sparsity is desired</span>
</span><span id="L-1068"><a href="#L-1068"><span class="linenos">1068</span></a>            <span class="c1"># assert not (self.slice_vectors == 0).all(dim=1).any(), &#39;Slice vectors contain a zero vector&#39;</span>
</span><span id="L-1069"><a href="#L-1069"><span class="linenos">1069</span></a>
</span><span id="L-1070"><a href="#L-1070"><span class="linenos">1070</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="L-1071"><a href="#L-1071"><span class="linenos">1071</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain NaNs&#39;</span>
</span><span id="L-1072"><a href="#L-1072"><span class="linenos">1072</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain infs&#39;</span>
</span><span id="L-1073"><a href="#L-1073"><span class="linenos">1073</span></a>
</span><span id="L-1074"><a href="#L-1074"><span class="linenos">1074</span></a>        <span class="c1">### A. Verify input types and content</span>
</span><span id="L-1075"><a href="#L-1075"><span class="linenos">1075</span></a>
</span><span id="L-1076"><a href="#L-1076"><span class="linenos">1076</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="L-1077"><a href="#L-1077"><span class="linenos">1077</span></a>
</span><span id="L-1078"><a href="#L-1078"><span class="linenos">1078</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1079"><a href="#L-1079"><span class="linenos">1079</span></a>            <span class="k">assert</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="s1">&#39;d_edge &gt; 0 (given at initialization) necessitates graph_mode=True on forward call&#39;</span>
</span><span id="L-1080"><a href="#L-1080"><span class="linenos">1080</span></a>            <span class="k">assert</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;X_edge must be provided since d_edge &gt; 0&#39;</span>
</span><span id="L-1081"><a href="#L-1081"><span class="linenos">1081</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1082"><a href="#L-1082"><span class="linenos">1082</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;X_edge should be None or empty since d_edge == 0&#39;</span>
</span><span id="L-1083"><a href="#L-1083"><span class="linenos">1083</span></a>            <span class="n">X_edge</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-1084"><a href="#L-1084"><span class="linenos">1084</span></a>
</span><span id="L-1085"><a href="#L-1085"><span class="linenos">1085</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;X must be a pytorch tensor. Instead got type </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span><span id="L-1086"><a href="#L-1086"><span class="linenos">1086</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="ow">or</span> <span class="n">W</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">},</span> <span class="s1">&#39;W must be a pytorch tensor, </span><span class="se">\&#39;</span><span class="s1">unit</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">uniform</span><span class="se">\&#39;</span><span class="s1">&#39;</span>
</span><span id="L-1087"><a href="#L-1087"><span class="linenos">1087</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1088"><a href="#L-1088"><span class="linenos">1088</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1089"><a href="#L-1089"><span class="linenos">1089</span></a>
</span><span id="L-1090"><a href="#L-1090"><span class="linenos">1090</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1091"><a href="#L-1091"><span class="linenos">1091</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;The entries of X cannot contain NaNs&quot;</span>
</span><span id="L-1092"><a href="#L-1092"><span class="linenos">1092</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X must be finite&quot;</span>
</span><span id="L-1093"><a href="#L-1093"><span class="linenos">1093</span></a>
</span><span id="L-1094"><a href="#L-1094"><span class="linenos">1094</span></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="L-1095"><a href="#L-1095"><span class="linenos">1095</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1096"><a href="#L-1096"><span class="linenos">1096</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1097"><a href="#L-1097"><span class="linenos">1097</span></a>
</span><span id="L-1098"><a href="#L-1098"><span class="linenos">1098</span></a>            <span class="c1"># Check if W is sparse. If so, ensure that W is of the correct layout.</span>
</span><span id="L-1099"><a href="#L-1099"><span class="linenos">1099</span></a>            <span class="c1"># Note: Strangely enough, sparse tensors of layouts other than COO (e.g. CSR) may have is_sparse=False.</span>
</span><span id="L-1100"><a href="#L-1100"><span class="linenos">1100</span></a>            <span class="c1">#       This may lead us to mistakenly treat a, e.g. W that is sparse CSR as dense.</span>
</span><span id="L-1101"><a href="#L-1101"><span class="linenos">1101</span></a>            <span class="c1">#       Currently there is no is_dense() function in torch, so reading the layout string directly is the second best.</span>
</span><span id="L-1102"><a href="#L-1102"><span class="linenos">1102</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="L-1103"><a href="#L-1103"><span class="linenos">1103</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse W has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="L-1104"><a href="#L-1104"><span class="linenos">1104</span></a>
</span><span id="L-1105"><a href="#L-1105"><span class="linenos">1105</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse W must be coalesced&#39;</span>
</span><span id="L-1106"><a href="#L-1106"><span class="linenos">1106</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;W.dense_dim() must be zero&#39;</span>
</span><span id="L-1107"><a href="#L-1107"><span class="linenos">1107</span></a>
</span><span id="L-1108"><a href="#L-1108"><span class="linenos">1108</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1109"><a href="#L-1109"><span class="linenos">1109</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1110"><a href="#L-1110"><span class="linenos">1110</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1111"><a href="#L-1111"><span class="linenos">1111</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1112"><a href="#L-1112"><span class="linenos">1112</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span>
</span><span id="L-1113"><a href="#L-1113"><span class="linenos">1113</span></a>
</span><span id="L-1114"><a href="#L-1114"><span class="linenos">1114</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1115"><a href="#L-1115"><span class="linenos">1115</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W_vals</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="L-1116"><a href="#L-1116"><span class="linenos">1116</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;W cannot contain NaNs&quot;</span>
</span><span id="L-1117"><a href="#L-1117"><span class="linenos">1117</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be finite&quot;</span>
</span><span id="L-1118"><a href="#L-1118"><span class="linenos">1118</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">W_vals</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be nonnegative&quot;</span>
</span><span id="L-1119"><a href="#L-1119"><span class="linenos">1119</span></a>                <span class="k">del</span> <span class="n">W_vals</span>
</span><span id="L-1120"><a href="#L-1120"><span class="linenos">1120</span></a>
</span><span id="L-1121"><a href="#L-1121"><span class="linenos">1121</span></a>        <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1122"><a href="#L-1122"><span class="linenos">1122</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;When X_edge is provided, W must be provided explicitly&#39;</span>
</span><span id="L-1123"><a href="#L-1123"><span class="linenos">1123</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1124"><a href="#L-1124"><span class="linenos">1124</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1125"><a href="#L-1125"><span class="linenos">1125</span></a>
</span><span id="L-1126"><a href="#L-1126"><span class="linenos">1126</span></a>            <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="L-1127"><a href="#L-1127"><span class="linenos">1127</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse X_edge has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="L-1128"><a href="#L-1128"><span class="linenos">1128</span></a>
</span><span id="L-1129"><a href="#L-1129"><span class="linenos">1129</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must be coalesced&#39;</span>
</span><span id="L-1130"><a href="#L-1130"><span class="linenos">1130</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 or 0&#39;</span>
</span><span id="L-1131"><a href="#L-1131"><span class="linenos">1131</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 since d_edge &gt; 1&#39;</span>
</span><span id="L-1132"><a href="#L-1132"><span class="linenos">1132</span></a>
</span><span id="L-1133"><a href="#L-1133"><span class="linenos">1133</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1134"><a href="#L-1134"><span class="linenos">1134</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1135"><a href="#L-1135"><span class="linenos">1135</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1136"><a href="#L-1136"><span class="linenos">1136</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1137"><a href="#L-1137"><span class="linenos">1137</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span>
</span><span id="L-1138"><a href="#L-1138"><span class="linenos">1138</span></a>
</span><span id="L-1139"><a href="#L-1139"><span class="linenos">1139</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1140"><a href="#L-1140"><span class="linenos">1140</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;X_edge_vals cannot contain NaNs&quot;</span>
</span><span id="L-1141"><a href="#L-1141"><span class="linenos">1141</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X_edge_vals must be finite&quot;</span>
</span><span id="L-1142"><a href="#L-1142"><span class="linenos">1142</span></a>                <span class="k">del</span> <span class="n">X_edge_vals</span>
</span><span id="L-1143"><a href="#L-1143"><span class="linenos">1143</span></a>
</span><span id="L-1144"><a href="#L-1144"><span class="linenos">1144</span></a>            <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;X_edge and W must either both or neither be sparse&#39;</span>
</span><span id="L-1145"><a href="#L-1145"><span class="linenos">1145</span></a>
</span><span id="L-1146"><a href="#L-1146"><span class="linenos">1146</span></a>
</span><span id="L-1147"><a href="#L-1147"><span class="linenos">1147</span></a>        <span class="c1">### B. Verify input sizes</span>
</span><span id="L-1148"><a href="#L-1148"><span class="linenos">1148</span></a>
</span><span id="L-1149"><a href="#L-1149"><span class="linenos">1149</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;X must be a tensor of order at least 2&quot;</span>
</span><span id="L-1150"><a href="#L-1150"><span class="linenos">1150</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="s2">&quot;The last dimension of X must equal d_in=</span><span class="si">%d</span><span class="s2">. Instead got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="L-1151"><a href="#L-1151"><span class="linenos">1151</span></a>
</span><span id="L-1152"><a href="#L-1152"><span class="linenos">1152</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="L-1153"><a href="#L-1153"><span class="linenos">1153</span></a>            <span class="c1"># batch_dims contains everything that precedes (n,d_in) in X.shape</span>
</span><span id="L-1154"><a href="#L-1154"><span class="linenos">1154</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="L-1155"><a href="#L-1155"><span class="linenos">1155</span></a>            <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)]</span>
</span><span id="L-1156"><a href="#L-1156"><span class="linenos">1156</span></a>
</span><span id="L-1157"><a href="#L-1157"><span class="linenos">1157</span></a>            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="L-1158"><a href="#L-1158"><span class="linenos">1158</span></a>                <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
</span><span id="L-1159"><a href="#L-1159"><span class="linenos">1159</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (Perhaps missing argument graph_mode=True?)&quot;</span>
</span><span id="L-1160"><a href="#L-1160"><span class="linenos">1160</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-1161"><a href="#L-1161"><span class="linenos">1161</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (unless graph_mode=True)&quot;</span>
</span><span id="L-1162"><a href="#L-1162"><span class="linenos">1162</span></a>
</span><span id="L-1163"><a href="#L-1163"><span class="linenos">1163</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">err_str</span>
</span><span id="L-1164"><a href="#L-1164"><span class="linenos">1164</span></a>
</span><span id="L-1165"><a href="#L-1165"><span class="linenos">1165</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;unit&#39;</span><span class="p">:</span>
</span><span id="L-1166"><a href="#L-1166"><span class="linenos">1166</span></a>                <span class="c1"># Initialize with unit weights</span>
</span><span id="L-1167"><a href="#L-1167"><span class="linenos">1167</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1168"><a href="#L-1168"><span class="linenos">1168</span></a>
</span><span id="L-1169"><a href="#L-1169"><span class="linenos">1169</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
</span><span id="L-1170"><a href="#L-1170"><span class="linenos">1170</span></a>                <span class="c1"># Initialize with uniform weights</span>
</span><span id="L-1171"><a href="#L-1171"><span class="linenos">1171</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1172"><a href="#L-1172"><span class="linenos">1172</span></a>
</span><span id="L-1173"><a href="#L-1173"><span class="linenos">1173</span></a>        <span class="k">elif</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="L-1174"><a href="#L-1174"><span class="linenos">1174</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;W must be explicitly provided when graph_mode=True&#39;</span>
</span><span id="L-1175"><a href="#L-1175"><span class="linenos">1175</span></a>
</span><span id="L-1176"><a href="#L-1176"><span class="linenos">1176</span></a>            <span class="c1"># batch_dims contains everything that precedes (nRecipients, n) in W.shape</span>
</span><span id="L-1177"><a href="#L-1177"><span class="linenos">1177</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="L-1178"><a href="#L-1178"><span class="linenos">1178</span></a>            <span class="n">nRecipients</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span><span id="L-1179"><a href="#L-1179"><span class="linenos">1179</span></a>            <span class="c1">#n = W.shape[-1]</span>
</span><span id="L-1180"><a href="#L-1180"><span class="linenos">1180</span></a>
</span><span id="L-1181"><a href="#L-1181"><span class="linenos">1181</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span> <span class="s2">&quot;Shape mismatch between X and W: When graph_mode=True, if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,n,d_in)&quot;</span>
</span><span id="L-1182"><a href="#L-1182"><span class="linenos">1182</span></a>
</span><span id="L-1183"><a href="#L-1183"><span class="linenos">1183</span></a>            <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1184"><a href="#L-1184"><span class="linenos">1184</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># For PyCharm to know</span>
</span><span id="L-1185"><a href="#L-1185"><span class="linenos">1185</span></a>
</span><span id="L-1186"><a href="#L-1186"><span class="linenos">1186</span></a>                <span class="c1"># Verify that X_edge has the right shape and is compatible with W</span>
</span><span id="L-1187"><a href="#L-1187"><span class="linenos">1187</span></a>                <span class="k">assert</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">or</span>
</span><span id="L-1188"><a href="#L-1188"><span class="linenos">1188</span></a>                        <span class="p">((</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">))),</span> <span class="p">(</span>
</span><span id="L-1189"><a href="#L-1189"><span class="linenos">1189</span></a>                    <span class="s2">&quot;Shape mismatch between X_edge and W: if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,nRecipients,n,d_edge) (with the possible exception (b1,b2,...,bk,nRecipients,n) when d_edge=1&quot;</span> <span class="p">)</span>
</span><span id="L-1190"><a href="#L-1190"><span class="linenos">1190</span></a>
</span><span id="L-1191"><a href="#L-1191"><span class="linenos">1191</span></a>                <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="L-1192"><a href="#L-1192"><span class="linenos">1192</span></a>                    <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;Sparse X_edge must have the same number of values() as W&#39;</span>
</span><span id="L-1193"><a href="#L-1193"><span class="linenos">1193</span></a>                    <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="L-1194"><a href="#L-1194"><span class="linenos">1194</span></a>                        <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must have the same nonzero pattern as W&#39;</span>
</span><span id="L-1195"><a href="#L-1195"><span class="linenos">1195</span></a>
</span><span id="L-1196"><a href="#L-1196"><span class="linenos">1196</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
</span><span id="L-1197"><a href="#L-1197"><span class="linenos">1197</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_dense_dim</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">)</span>
</span><span id="L-1198"><a href="#L-1198"><span class="linenos">1198</span></a>
</span><span id="L-1199"><a href="#L-1199"><span class="linenos">1199</span></a>
</span><span id="L-1200"><a href="#L-1200"><span class="linenos">1200</span></a>        <span class="c1">### C. Precalculate axis indices and output shape</span>
</span><span id="L-1201"><a href="#L-1201"><span class="linenos">1201</span></a>
</span><span id="L-1202"><a href="#L-1202"><span class="linenos">1202</span></a>        <span class="c1"># These are the different axes we use to store data for processing. These definitions are repeated in forward_helper()</span>
</span><span id="L-1203"><a href="#L-1203"><span class="linenos">1203</span></a>        <span class="c1"># element_axis corresponds to the index of the multiset elements</span>
</span><span id="L-1204"><a href="#L-1204"><span class="linenos">1204</span></a>        <span class="c1"># ambspace_axis corresponds to the elements&#39; coordinate index in the ambient space R^d_in</span>
</span><span id="L-1205"><a href="#L-1205"><span class="linenos">1205</span></a>        <span class="c1"># After projection, the ambient space coordinates are replaced by the slice number; thus slice_axis=ambspace_axis</span>
</span><span id="L-1206"><a href="#L-1206"><span class="linenos">1206</span></a>        <span class="c1"># If we&#39;re in Cartesian mode, the frequencies have their own axis freq_axis, otherwise it is the same axis as slice_axis.</span>
</span><span id="L-1207"><a href="#L-1207"><span class="linenos">1207</span></a>        <span class="n">recipient_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># Message-recipient vertices</span>
</span><span id="L-1208"><a href="#L-1208"><span class="linenos">1208</span></a>        <span class="n">element_axis</span>  <span class="o">=</span> <span class="n">recipient_axis</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="c1"># In graph mode this axis denotes the message-sender vertices</span>
</span><span id="L-1209"><a href="#L-1209"><span class="linenos">1209</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-1210"><a href="#L-1210"><span class="linenos">1210</span></a>        <span class="n">slice_axis</span>     <span class="o">=</span> <span class="n">ambspace_axis</span>
</span><span id="L-1211"><a href="#L-1211"><span class="linenos">1211</span></a>        <span class="c1"># noinspection PyUnusedLocal</span>
</span><span id="L-1212"><a href="#L-1212"><span class="linenos">1212</span></a>        <span class="n">freq_axis</span>     <span class="o">=</span> <span class="n">slice_axis</span> <span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="n">slice_axis</span>
</span><span id="L-1213"><a href="#L-1213"><span class="linenos">1213</span></a>        <span class="n">output_slice_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="c1"># In the output, the element axis is replaced by the slice axis</span>
</span><span id="L-1214"><a href="#L-1214"><span class="linenos">1214</span></a>
</span><span id="L-1215"><a href="#L-1215"><span class="linenos">1215</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">=</span>  <span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">nRecipients</span><span class="p">,)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="n">batch_dims</span>
</span><span id="L-1216"><a href="#L-1216"><span class="linenos">1216</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,)</span>
</span><span id="L-1217"><a href="#L-1217"><span class="linenos">1217</span></a>
</span><span id="L-1218"><a href="#L-1218"><span class="linenos">1218</span></a>        <span class="c1">### D. Input is ok. Start working.</span>
</span><span id="L-1219"><a href="#L-1219"><span class="linenos">1219</span></a>
</span><span id="L-1220"><a href="#L-1220"><span class="linenos">1220</span></a>        <span class="c1"># Calculate W_sum, which contains the total mass of the input measures</span>
</span><span id="L-1221"><a href="#L-1221"><span class="linenos">1221</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="L-1222"><a href="#L-1222"><span class="linenos">1222</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="L-1223"><a href="#L-1223"><span class="linenos">1223</span></a>            <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-1224"><a href="#L-1224"><span class="linenos">1224</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1225"><a href="#L-1225"><span class="linenos">1225</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sum_sparseToDense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1226"><a href="#L-1226"><span class="linenos">1226</span></a>
</span><span id="L-1227"><a href="#L-1227"><span class="linenos">1227</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1228"><a href="#L-1228"><span class="linenos">1228</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="L-1229"><a href="#L-1229"><span class="linenos">1229</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1230"><a href="#L-1230"><span class="linenos">1230</span></a>
</span><span id="L-1231"><a href="#L-1231"><span class="linenos">1231</span></a>        <span class="c1"># Total-mass deficit to be compensated for by padding</span>
</span><span id="L-1232"><a href="#L-1232"><span class="linenos">1232</span></a>        <span class="n">W_pad</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">-</span> <span class="n">W_sum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="L-1233"><a href="#L-1233"><span class="linenos">1233</span></a>
</span><span id="L-1234"><a href="#L-1234"><span class="linenos">1234</span></a>        <span class="c1"># Detect weight deficit and augment W and X accordingly</span>
</span><span id="L-1235"><a href="#L-1235"><span class="linenos">1235</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">W_pad</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
</span><span id="L-1236"><a href="#L-1236"><span class="linenos">1236</span></a>            <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1237"><a href="#L-1237"><span class="linenos">1237</span></a>            <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-1238"><a href="#L-1238"><span class="linenos">1238</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="L-1239"><a href="#L-1239"><span class="linenos">1239</span></a>
</span><span id="L-1240"><a href="#L-1240"><span class="linenos">1240</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="L-1241"><a href="#L-1241"><span class="linenos">1241</span></a>                <span class="c1"># Make sure this works</span>
</span><span id="L-1242"><a href="#L-1242"><span class="linenos">1242</span></a>                <span class="n">W_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">to_sparse_full</span><span class="p">(</span><span class="n">W_pad</span><span class="p">)</span>
</span><span id="L-1243"><a href="#L-1243"><span class="linenos">1243</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">)</span>
</span><span id="L-1244"><a href="#L-1244"><span class="linenos">1244</span></a>                <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-1245"><a href="#L-1245"><span class="linenos">1245</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1246"><a href="#L-1246"><span class="linenos">1246</span></a>
</span><span id="L-1247"><a href="#L-1247"><span class="linenos">1247</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1248"><a href="#L-1248"><span class="linenos">1248</span></a>                    <span class="n">X_edge_pad_inds</span> <span class="o">=</span> <span class="n">W_pad</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-1249"><a href="#L-1249"><span class="linenos">1249</span></a>                    <span class="n">X_edge_pad_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nRecipients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-1250"><a href="#L-1250"><span class="linenos">1250</span></a>                    <span class="n">X_edge_pad_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1251"><a href="#L-1251"><span class="linenos">1251</span></a>
</span><span id="L-1252"><a href="#L-1252"><span class="linenos">1252</span></a>                    <span class="n">X_edge_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">X_edge_pad_inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">X_edge_pad_vals</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">X_edge_pad_shape</span><span class="p">)</span>
</span><span id="L-1253"><a href="#L-1253"><span class="linenos">1253</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">X_edge_pad</span><span class="p">)</span>
</span><span id="L-1254"><a href="#L-1254"><span class="linenos">1254</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1255"><a href="#L-1255"><span class="linenos">1255</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="L-1256"><a href="#L-1256"><span class="linenos">1256</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
</span><span id="L-1257"><a href="#L-1257"><span class="linenos">1257</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-1258"><a href="#L-1258"><span class="linenos">1258</span></a>                    <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span> <span class="p">]</span>
</span><span id="L-1259"><a href="#L-1259"><span class="linenos">1259</span></a>                    <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-1260"><a href="#L-1260"><span class="linenos">1260</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
</span><span id="L-1261"><a href="#L-1261"><span class="linenos">1261</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1262"><a href="#L-1262"><span class="linenos">1262</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="L-1263"><a href="#L-1263"><span class="linenos">1263</span></a>
</span><span id="L-1264"><a href="#L-1264"><span class="linenos">1264</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="L-1265"><a href="#L-1265"><span class="linenos">1265</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1266"><a href="#L-1266"><span class="linenos">1266</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="L-1267"><a href="#L-1267"><span class="linenos">1267</span></a>
</span><span id="L-1268"><a href="#L-1268"><span class="linenos">1268</span></a>        <span class="k">del</span> <span class="n">W_pad</span>
</span><span id="L-1269"><a href="#L-1269"><span class="linenos">1269</span></a>
</span><span id="L-1270"><a href="#L-1270"><span class="linenos">1270</span></a>        <span class="c1"># Normalize W according to W_sum_padded</span>
</span><span id="L-1271"><a href="#L-1271"><span class="linenos">1271</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="L-1272"><a href="#L-1272"><span class="linenos">1272</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">div_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_sum_padded</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span>
</span><span id="L-1273"><a href="#L-1273"><span class="linenos">1273</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1274"><a href="#L-1274"><span class="linenos">1274</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1275"><a href="#L-1275"><span class="linenos">1275</span></a>            <span class="k">del</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="n">W_sum_padded</span>
</span><span id="L-1276"><a href="#L-1276"><span class="linenos">1276</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1277"><a href="#L-1277"><span class="linenos">1277</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">/</span> <span class="n">W_sum_padded</span>
</span><span id="L-1278"><a href="#L-1278"><span class="linenos">1278</span></a>            <span class="k">del</span> <span class="n">W_sum_padded</span>
</span><span id="L-1279"><a href="#L-1279"><span class="linenos">1279</span></a>
</span><span id="L-1280"><a href="#L-1280"><span class="linenos">1280</span></a>        <span class="c1"># For compatibility reasons, we support the case of zero-dimensional output tensor</span>
</span><span id="L-1281"><a href="#L-1281"><span class="linenos">1281</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1282"><a href="#L-1282"><span class="linenos">1282</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1283"><a href="#L-1283"><span class="linenos">1283</span></a>
</span><span id="L-1284"><a href="#L-1284"><span class="linenos">1284</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">):</span>
</span><span id="L-1285"><a href="#L-1285"><span class="linenos">1285</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="L-1286"><a href="#L-1286"><span class="linenos">1286</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1287"><a href="#L-1287"><span class="linenos">1287</span></a>                                                 <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1288"><a href="#L-1288"><span class="linenos">1288</span></a>
</span><span id="L-1289"><a href="#L-1289"><span class="linenos">1289</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1290"><a href="#L-1290"><span class="linenos">1290</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;max_parallel_slices must be None or a positive integer&#39;</span>
</span><span id="L-1291"><a href="#L-1291"><span class="linenos">1291</span></a>
</span><span id="L-1292"><a href="#L-1292"><span class="linenos">1292</span></a>            <span class="n">nIter</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">%</span> <span class="n">max_parallel_slices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span>
</span><span id="L-1293"><a href="#L-1293"><span class="linenos">1293</span></a>
</span><span id="L-1294"><a href="#L-1294"><span class="linenos">1294</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1295"><a href="#L-1295"><span class="linenos">1295</span></a>
</span><span id="L-1296"><a href="#L-1296"><span class="linenos">1296</span></a>            <span class="k">for</span> <span class="n">iIter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nIter</span><span class="p">):</span>
</span><span id="L-1297"><a href="#L-1297"><span class="linenos">1297</span></a>                <span class="n">inds_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">iIter</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="p">(</span><span class="n">iIter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1298"><a href="#L-1298"><span class="linenos">1298</span></a>                <span class="n">slice_vecs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="L-1299"><a href="#L-1299"><span class="linenos">1299</span></a>                <span class="n">freqs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">]</span>
</span><span id="L-1300"><a href="#L-1300"><span class="linenos">1300</span></a>
</span><span id="L-1301"><a href="#L-1301"><span class="linenos">1301</span></a>                <span class="n">out_curr</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">slice_vecs_curr</span><span class="p">,</span> <span class="n">freqs_curr</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="L-1302"><a href="#L-1302"><span class="linenos">1302</span></a>                                                        <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1303"><a href="#L-1303"><span class="linenos">1303</span></a>                                                        <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1304"><a href="#L-1304"><span class="linenos">1304</span></a>
</span><span id="L-1305"><a href="#L-1305"><span class="linenos">1305</span></a>                <span class="n">assign_at</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">out_curr</span><span class="p">,</span> <span class="n">output_slice_axis</span><span class="p">,</span> <span class="n">inds_curr</span><span class="p">)</span>
</span><span id="L-1306"><a href="#L-1306"><span class="linenos">1306</span></a>
</span><span id="L-1307"><a href="#L-1307"><span class="linenos">1307</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="L-1308"><a href="#L-1308"><span class="linenos">1308</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1309"><a href="#L-1309"><span class="linenos">1309</span></a>
</span><span id="L-1310"><a href="#L-1310"><span class="linenos">1310</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span>
</span><span id="L-1311"><a href="#L-1311"><span class="linenos">1311</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="p">:</span>
</span><span id="L-1312"><a href="#L-1312"><span class="linenos">1312</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">IDENTITY</span><span class="p">:</span>
</span><span id="L-1313"><a href="#L-1313"><span class="linenos">1313</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="L-1314"><a href="#L-1314"><span class="linenos">1314</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">SQRT</span><span class="p">:</span>
</span><span id="L-1315"><a href="#L-1315"><span class="linenos">1315</span></a>                    <span class="c1"># x/(sqrt(x+1)+1) is a numerically-safe formulation of sqrt(1+x)-1</span>
</span><span id="L-1316"><a href="#L-1316"><span class="linenos">1316</span></a>                    <span class="c1"># note that we don&#39;t use sqrt(1+x) since we need the function to vanish at x=0,</span>
</span><span id="L-1317"><a href="#L-1317"><span class="linenos">1317</span></a>                    <span class="c1"># and we don&#39;t use sqrt(x) since we need it to have a gradient at x=0.</span>
</span><span id="L-1318"><a href="#L-1318"><span class="linenos">1318</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span> <span class="n">W_sum</span> <span class="o">/</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">W_sum</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1319"><a href="#L-1319"><span class="linenos">1319</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">LOG</span><span class="p">:</span>
</span><span id="L-1320"><a href="#L-1320"><span class="linenos">1320</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">W_sum</span><span class="p">)</span>
</span><span id="L-1321"><a href="#L-1321"><span class="linenos">1321</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="L-1322"><a href="#L-1322"><span class="linenos">1322</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-1323"><a href="#L-1323"><span class="linenos">1323</span></a>
</span><span id="L-1324"><a href="#L-1324"><span class="linenos">1324</span></a>            <span class="n">encoded_total_mass</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span><span id="L-1325"><a href="#L-1325"><span class="linenos">1325</span></a>
</span><span id="L-1326"><a href="#L-1326"><span class="linenos">1326</span></a>            <span class="k">del</span> <span class="n">W_sum</span>
</span><span id="L-1327"><a href="#L-1327"><span class="linenos">1327</span></a>
</span><span id="L-1328"><a href="#L-1328"><span class="linenos">1328</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="L-1329"><a href="#L-1329"><span class="linenos">1329</span></a>
</span><span id="L-1330"><a href="#L-1330"><span class="linenos">1330</span></a>            <span class="n">needs_emb_norm</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="ow">in</span>
</span><span id="L-1331"><a href="#L-1331"><span class="linenos">1331</span></a>                              <span class="p">{</span><span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">,</span>
</span><span id="L-1332"><a href="#L-1332"><span class="linenos">1332</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">,</span>
</span><span id="L-1333"><a href="#L-1333"><span class="linenos">1333</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">})</span>
</span><span id="L-1334"><a href="#L-1334"><span class="linenos">1334</span></a>
</span><span id="L-1335"><a href="#L-1335"><span class="linenos">1335</span></a>            <span class="k">if</span> <span class="n">needs_emb_norm</span><span class="p">:</span>
</span><span id="L-1336"><a href="#L-1336"><span class="linenos">1336</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span><span id="L-1337"><a href="#L-1337"><span class="linenos">1337</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1338"><a href="#L-1338"><span class="linenos">1338</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-1339"><a href="#L-1339"><span class="linenos">1339</span></a>
</span><span id="L-1340"><a href="#L-1340"><span class="linenos">1340</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="p">:</span>
</span><span id="L-1341"><a href="#L-1341"><span class="linenos">1341</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">DECOUPLED</span><span class="p">:</span>
</span><span id="L-1342"><a href="#L-1342"><span class="linenos">1342</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1343"><a href="#L-1343"><span class="linenos">1343</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">SCALED</span><span class="p">:</span>
</span><span id="L-1344"><a href="#L-1344"><span class="linenos">1344</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1345"><a href="#L-1345"><span class="linenos">1345</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">:</span>
</span><span id="L-1346"><a href="#L-1346"><span class="linenos">1346</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1347"><a href="#L-1347"><span class="linenos">1347</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">:</span>
</span><span id="L-1348"><a href="#L-1348"><span class="linenos">1348</span></a>                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="L-1349"><a href="#L-1349"><span class="linenos">1349</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1350"><a href="#L-1350"><span class="linenos">1350</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">:</span>
</span><span id="L-1351"><a href="#L-1351"><span class="linenos">1351</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part1</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span>
</span><span id="L-1352"><a href="#L-1352"><span class="linenos">1352</span></a>                                       <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part2</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1353"><a href="#L-1353"><span class="linenos">1353</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>  <span class="c1"># fallback</span>
</span><span id="L-1354"><a href="#L-1354"><span class="linenos">1354</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding method: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-1355"><a href="#L-1355"><span class="linenos">1355</span></a>
</span><span id="L-1356"><a href="#L-1356"><span class="linenos">1356</span></a>            <span class="k">del</span> <span class="n">X_emb_norm</span>
</span><span id="L-1357"><a href="#L-1357"><span class="linenos">1357</span></a>
</span><span id="L-1358"><a href="#L-1358"><span class="linenos">1358</span></a>        <span class="c1"># Add bias</span>
</span><span id="L-1359"><a href="#L-1359"><span class="linenos">1359</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="L-1360"><a href="#L-1360"><span class="linenos">1360</span></a>            <span class="n">X_emb</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span><span id="L-1361"><a href="#L-1361"><span class="linenos">1361</span></a>
</span><span id="L-1362"><a href="#L-1362"><span class="linenos">1362</span></a>        <span class="k">return</span> <span class="n">X_emb</span>
</span><span id="L-1363"><a href="#L-1363"><span class="linenos">1363</span></a>
</span><span id="L-1364"><a href="#L-1364"><span class="linenos">1364</span></a>
</span><span id="L-1365"><a href="#L-1365"><span class="linenos">1365</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-1366"><a href="#L-1366"><span class="linenos">1366</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="n">cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="L-1367"><a href="#L-1367"><span class="linenos">1367</span></a>                        <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1368"><a href="#L-1368"><span class="linenos">1368</span></a>                        <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-1369"><a href="#L-1369"><span class="linenos">1369</span></a>        <span class="c1"># This function computes the embedding of (X,W) for a subset of the slices and frequencies.</span>
</span><span id="L-1370"><a href="#L-1370"><span class="linenos">1370</span></a>        <span class="c1"># slice_vectors should be of size (num_slices x d_in), and frequencies should be of size num_frequencies (not num_frequencies x 1).</span>
</span><span id="L-1371"><a href="#L-1371"><span class="linenos">1371</span></a>
</span><span id="L-1372"><a href="#L-1372"><span class="linenos">1372</span></a>        <span class="n">d_in</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-1373"><a href="#L-1373"><span class="linenos">1373</span></a>        <span class="c1">#n = W.shape[-1]</span>
</span><span id="L-1374"><a href="#L-1374"><span class="linenos">1374</span></a>        <span class="n">nRecepients</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1375"><a href="#L-1375"><span class="linenos">1375</span></a>        <span class="n">num_slices</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-1376"><a href="#L-1376"><span class="linenos">1376</span></a>        <span class="n">num_frequencies</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="L-1377"><a href="#L-1377"><span class="linenos">1377</span></a>        <span class="n">sparse_mode</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="L-1378"><a href="#L-1378"><span class="linenos">1378</span></a>        <span class="n">d_edge</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="L-1379"><a href="#L-1379"><span class="linenos">1379</span></a>
</span><span id="L-1380"><a href="#L-1380"><span class="linenos">1380</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;This should not happen&quot;</span>
</span><span id="L-1381"><a href="#L-1381"><span class="linenos">1381</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">d_in</span> <span class="o">+</span> <span class="n">d_edge</span><span class="p">),</span> <span class="s2">&quot;This should not happen&quot;</span>
</span><span id="L-1382"><a href="#L-1382"><span class="linenos">1382</span></a>
</span><span id="L-1383"><a href="#L-1383"><span class="linenos">1383</span></a>        <span class="c1"># Calculate the projections of X</span>
</span><span id="L-1384"><a href="#L-1384"><span class="linenos">1384</span></a>        <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1385"><a href="#L-1385"><span class="linenos">1385</span></a>            <span class="n">Xp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="L-1386"><a href="#L-1386"><span class="linenos">1386</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-1387"><a href="#L-1387"><span class="linenos">1387</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-1388"><a href="#L-1388"><span class="linenos">1388</span></a>            <span class="n">Xp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">d_in</span><span class="p">],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="L-1389"><a href="#L-1389"><span class="linenos">1389</span></a>
</span><span id="L-1390"><a href="#L-1390"><span class="linenos">1390</span></a>        <span class="k">del</span> <span class="n">X</span>
</span><span id="L-1391"><a href="#L-1391"><span class="linenos">1391</span></a>
</span><span id="L-1392"><a href="#L-1392"><span class="linenos">1392</span></a>        <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1393"><a href="#L-1393"><span class="linenos">1393</span></a>            <span class="c1"># Sort the projected elements </span>
</span><span id="L-1394"><a href="#L-1394"><span class="linenos">1394</span></a>            <span class="c1"># Note: We sort before the graph-mode expansion because it makes things simpler in the case when W is sparse</span>
</span><span id="L-1395"><a href="#L-1395"><span class="linenos">1395</span></a>
</span><span id="L-1396"><a href="#L-1396"><span class="linenos">1396</span></a>            <span class="c1"># Sort along element/sender axis</span>
</span><span id="L-1397"><a href="#L-1397"><span class="linenos">1397</span></a>            <span class="k">if</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="L-1398"><a href="#L-1398"><span class="linenos">1398</span></a>                <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xp</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="L-1399"><a href="#L-1399"><span class="linenos">1399</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1400"><a href="#L-1400"><span class="linenos">1400</span></a>                <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">Xp</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-1401"><a href="#L-1401"><span class="linenos">1401</span></a>
</span><span id="L-1402"><a href="#L-1402"><span class="linenos">1402</span></a>            <span class="k">del</span> <span class="n">Xp</span>
</span><span id="L-1403"><a href="#L-1403"><span class="linenos">1403</span></a>
</span><span id="L-1404"><a href="#L-1404"><span class="linenos">1404</span></a>            <span class="k">if</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="L-1405"><a href="#L-1405"><span class="linenos">1405</span></a>                <span class="c1"># Create recepient axis before sender axis and slice axis</span>
</span><span id="L-1406"><a href="#L-1406"><span class="linenos">1406</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
</span><span id="L-1407"><a href="#L-1407"><span class="linenos">1407</span></a>                <span class="n">Xpi</span> <span class="o">=</span> <span class="n">Xpi</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
</span><span id="L-1408"><a href="#L-1408"><span class="linenos">1408</span></a>
</span><span id="L-1409"><a href="#L-1409"><span class="linenos">1409</span></a>        <span class="k">elif</span> <span class="n">sparse_mode</span><span class="p">:</span> <span class="c1"># d_edge &gt; 0, sparse_mode=True</span>
</span><span id="L-1410"><a href="#L-1410"><span class="linenos">1410</span></a>            <span class="n">Xe</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1411"><a href="#L-1411"><span class="linenos">1411</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-1412"><a href="#L-1412"><span class="linenos">1412</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">Xe</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="n">d_in</span><span class="p">:],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="L-1413"><a href="#L-1413"><span class="linenos">1413</span></a>            <span class="k">del</span> <span class="n">Xe</span>
</span><span id="L-1414"><a href="#L-1414"><span class="linenos">1414</span></a>
</span><span id="L-1415"><a href="#L-1415"><span class="linenos">1415</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-1416"><a href="#L-1416"><span class="linenos">1416</span></a>
</span><span id="L-1417"><a href="#L-1417"><span class="linenos">1417</span></a>            <span class="c1"># Remove recepient axis from inds</span>
</span><span id="L-1418"><a href="#L-1418"><span class="linenos">1418</span></a>            <span class="n">dims_without_recipient</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)]</span>
</span><span id="L-1419"><a href="#L-1419"><span class="linenos">1419</span></a>
</span><span id="L-1420"><a href="#L-1420"><span class="linenos">1420</span></a>            <span class="c1"># For each edge, get the corresponding sender vertex feature vector after projection</span>
</span><span id="L-1421"><a href="#L-1421"><span class="linenos">1421</span></a>            <span class="n">Xp_temp</span> <span class="o">=</span> <span class="n">Xp</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">inds</span><span class="p">[</span><span class="n">dims_without_recipient</span><span class="p">,:])]</span>
</span><span id="L-1422"><a href="#L-1422"><span class="linenos">1422</span></a>
</span><span id="L-1423"><a href="#L-1423"><span class="linenos">1423</span></a>            <span class="n">Xep</span> <span class="o">+=</span> <span class="n">Xp_temp</span>
</span><span id="L-1424"><a href="#L-1424"><span class="linenos">1424</span></a>            <span class="k">del</span> <span class="n">Xp_temp</span><span class="p">,</span> <span class="n">inds</span>
</span><span id="L-1425"><a href="#L-1425"><span class="linenos">1425</span></a>
</span><span id="L-1426"><a href="#L-1426"><span class="linenos">1426</span></a>            <span class="n">Xep_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Xep</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span id="L-1427"><a href="#L-1427"><span class="linenos">1427</span></a>
</span><span id="L-1428"><a href="#L-1428"><span class="linenos">1428</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">Xep</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Xep_shape</span><span class="p">)</span>
</span><span id="L-1429"><a href="#L-1429"><span class="linenos">1429</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">flatten_dense_dim</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xep</span><span class="p">)</span>
</span><span id="L-1430"><a href="#L-1430"><span class="linenos">1430</span></a>            <span class="n">Xeps</span><span class="p">,</span> <span class="n">Xepi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xep</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="L-1431"><a href="#L-1431"><span class="linenos">1431</span></a>            <span class="k">del</span> <span class="n">Xep</span>
</span><span id="L-1432"><a href="#L-1432"><span class="linenos">1432</span></a>
</span><span id="L-1433"><a href="#L-1433"><span class="linenos">1433</span></a>        <span class="k">else</span><span class="p">:</span> <span class="c1"># d_edge &gt; 0, sparse_mode=False</span>
</span><span id="L-1434"><a href="#L-1434"><span class="linenos">1434</span></a>            <span class="c1"># Create recepient axis before sender axis and slice axis</span>
</span><span id="L-1435"><a href="#L-1435"><span class="linenos">1435</span></a>            <span class="n">Xpx</span> <span class="o">=</span> <span class="n">Xp</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#.expand(tuple(W.shape) + (num_slices,))</span>
</span><span id="L-1436"><a href="#L-1436"><span class="linenos">1436</span></a>            <span class="c1"># Replicate Xpx along recepient axis</span>
</span><span id="L-1437"><a href="#L-1437"><span class="linenos">1437</span></a>            <span class="n">Xpx</span> <span class="o">=</span> <span class="n">Xpx</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">replace_in_tuple</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span><span class="o">*</span><span class="n">Xpx</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="n">nRecepients</span><span class="p">))</span>
</span><span id="L-1438"><a href="#L-1438"><span class="linenos">1438</span></a>            <span class="c1"># Add edge-feature part of inner product to each recepient for each sender and projection</span>
</span><span id="L-1439"><a href="#L-1439"><span class="linenos">1439</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-1440"><a href="#L-1440"><span class="linenos">1440</span></a>            <span class="n">Xpx</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="n">d_in</span><span class="p">:],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="L-1441"><a href="#L-1441"><span class="linenos">1441</span></a>            <span class="c1"># Sort along element/sender axis</span>
</span><span id="L-1442"><a href="#L-1442"><span class="linenos">1442</span></a>            <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xpx</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="L-1443"><a href="#L-1443"><span class="linenos">1443</span></a>            <span class="c1">#Xps, Xpi = torch.sort(Xpx, dim=-2, descending=False, stable=True)</span>
</span><span id="L-1444"><a href="#L-1444"><span class="linenos">1444</span></a>
</span><span id="L-1445"><a href="#L-1445"><span class="linenos">1445</span></a>            <span class="k">del</span> <span class="n">Xpx</span>
</span><span id="L-1446"><a href="#L-1446"><span class="linenos">1446</span></a>
</span><span id="L-1447"><a href="#L-1447"><span class="linenos">1447</span></a>        <span class="c1"># Axis numbers as in the implementation of forward()</span>
</span><span id="L-1448"><a href="#L-1448"><span class="linenos">1448</span></a>        <span class="c1"># Note: These numbers are true only from here</span>
</span><span id="L-1449"><a href="#L-1449"><span class="linenos">1449</span></a>        <span class="n">recipient_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># Message-recipient vertices</span>
</span><span id="L-1450"><a href="#L-1450"><span class="linenos">1450</span></a>        <span class="n">element_axis</span>  <span class="o">=</span> <span class="n">recipient_axis</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="c1"># In graph mode this axis denotes the message-sender vertices</span>
</span><span id="L-1451"><a href="#L-1451"><span class="linenos">1451</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="o">+</span> <span class="mi">1</span>        
</span><span id="L-1452"><a href="#L-1452"><span class="linenos">1452</span></a>        <span class="n">slice_axis</span>     <span class="o">=</span> <span class="n">ambspace_axis</span>
</span><span id="L-1453"><a href="#L-1453"><span class="linenos">1453</span></a>        <span class="n">freq_axis</span>     <span class="o">=</span> <span class="n">slice_axis</span> <span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">cartesian_mode</span> <span class="k">else</span> <span class="n">slice_axis</span>
</span><span id="L-1454"><a href="#L-1454"><span class="linenos">1454</span></a>        <span class="c1"># noinspection PyUnusedLocal</span>
</span><span id="L-1455"><a href="#L-1455"><span class="linenos">1455</span></a>        <span class="n">output_slice_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="c1"># In the output, the element axis is replaced by the slice axis</span>
</span><span id="L-1456"><a href="#L-1456"><span class="linenos">1456</span></a>
</span><span id="L-1457"><a href="#L-1457"><span class="linenos">1457</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
</span><span id="L-1458"><a href="#L-1458"><span class="linenos">1458</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">freq_axis</span><span class="p">):</span>
</span><span id="L-1459"><a href="#L-1459"><span class="linenos">1459</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-1460"><a href="#L-1460"><span class="linenos">1460</span></a>
</span><span id="L-1461"><a href="#L-1461"><span class="linenos">1461</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="L-1462"><a href="#L-1462"><span class="linenos">1462</span></a>            <span class="k">if</span> <span class="n">graph_mode</span> <span class="ow">and</span> <span class="p">(</span><span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="L-1463"><a href="#L-1463"><span class="linenos">1463</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,))</span>
</span><span id="L-1464"><a href="#L-1464"><span class="linenos">1464</span></a>                <span class="n">Xpi</span> <span class="o">=</span> <span class="n">Xpi</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,))</span>
</span><span id="L-1465"><a href="#L-1465"><span class="linenos">1465</span></a>
</span><span id="L-1466"><a href="#L-1466"><span class="linenos">1466</span></a>            <span class="c1"># Sort the weights according to their corresponding projected elements</span>
</span><span id="L-1467"><a href="#L-1467"><span class="linenos">1467</span></a>            <span class="n">W_big</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">Xps</span><span class="p">)</span>
</span><span id="L-1468"><a href="#L-1468"><span class="linenos">1468</span></a>            <span class="n">Wps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">Xpi</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span><span id="L-1469"><a href="#L-1469"><span class="linenos">1469</span></a>
</span><span id="L-1470"><a href="#L-1470"><span class="linenos">1470</span></a>            <span class="k">if</span> <span class="n">cartesian_mode</span><span class="p">:</span>
</span><span id="L-1471"><a href="#L-1471"><span class="linenos">1471</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">Wps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1472"><a href="#L-1472"><span class="linenos">1472</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">Xps</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,))</span>
</span><span id="L-1473"><a href="#L-1473"><span class="linenos">1473</span></a>
</span><span id="L-1474"><a href="#L-1474"><span class="linenos">1474</span></a>            <span class="c1"># Once we have Wps we don&#39;t need W_big and Xpi</span>
</span><span id="L-1475"><a href="#L-1475"><span class="linenos">1475</span></a>            <span class="k">del</span> <span class="n">W_big</span><span class="p">,</span> <span class="n">Xpi</span>
</span><span id="L-1476"><a href="#L-1476"><span class="linenos">1476</span></a>
</span><span id="L-1477"><a href="#L-1477"><span class="linenos">1477</span></a>            <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="L-1478"><a href="#L-1478"><span class="linenos">1478</span></a>
</span><span id="L-1479"><a href="#L-1479"><span class="linenos">1479</span></a>            <span class="c1"># Here we assume sinc(x) = sin(pi*x)/(pi*x)</span>
</span><span id="L-1480"><a href="#L-1480"><span class="linenos">1480</span></a>            <span class="n">sincs</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">Wps_sum</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinc</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">frequencies</span> <span class="o">*</span> <span class="n">Wps_sum</span><span class="p">)</span>
</span><span id="L-1481"><a href="#L-1481"><span class="linenos">1481</span></a>            <span class="n">sinc_diffs</span> <span class="o">=</span> <span class="n">diff_zeropad</span><span class="p">(</span><span class="n">sincs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="L-1482"><a href="#L-1482"><span class="linenos">1482</span></a>            <span class="k">del</span> <span class="n">sincs</span>
</span><span id="L-1483"><a href="#L-1483"><span class="linenos">1483</span></a>
</span><span id="L-1484"><a href="#L-1484"><span class="linenos">1484</span></a>        <span class="k">elif</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="L-1485"><a href="#L-1485"><span class="linenos">1485</span></a>            <span class="c1"># We unsqueeze W to add a slice axis, in order to sort W according to each projection of X</span>
</span><span id="L-1486"><a href="#L-1486"><span class="linenos">1486</span></a>            <span class="c1"># Note: This repmat is unavoidable, because we sort the weights according to different permutations along slice_axis</span>
</span><span id="L-1487"><a href="#L-1487"><span class="linenos">1487</span></a>            <span class="n">W_unsqueeze</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1488"><a href="#L-1488"><span class="linenos">1488</span></a>            <span class="k">del</span> <span class="n">W</span>
</span><span id="L-1489"><a href="#L-1489"><span class="linenos">1489</span></a>
</span><span id="L-1490"><a href="#L-1490"><span class="linenos">1490</span></a>            <span class="c1"># 1.71 seconds</span>
</span><span id="L-1491"><a href="#L-1491"><span class="linenos">1491</span></a>            <span class="n">W_big</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_unsqueeze</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">,</span> <span class="n">slice_axis</span><span class="p">)</span>
</span><span id="L-1492"><a href="#L-1492"><span class="linenos">1492</span></a>            <span class="k">del</span> <span class="n">W_unsqueeze</span>
</span><span id="L-1493"><a href="#L-1493"><span class="linenos">1493</span></a>
</span><span id="L-1494"><a href="#L-1494"><span class="linenos">1494</span></a>            <span class="k">if</span> <span class="n">d_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1495"><a href="#L-1495"><span class="linenos">1495</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse_vals</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">Xepi</span><span class="p">)</span>
</span><span id="L-1496"><a href="#L-1496"><span class="linenos">1496</span></a>                <span class="k">del</span> <span class="n">Xepi</span>
</span><span id="L-1497"><a href="#L-1497"><span class="linenos">1497</span></a>            <span class="k">elif</span> <span class="n">graph_mode</span><span class="p">:</span> 
</span><span id="L-1498"><a href="#L-1498"><span class="linenos">1498</span></a>                <span class="c1"># 1.82 seconds</span>
</span><span id="L-1499"><a href="#L-1499"><span class="linenos">1499</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">Xpi</span><span class="p">,</span> <span class="n">recipient_axis</span><span class="p">)</span>
</span><span id="L-1500"><a href="#L-1500"><span class="linenos">1500</span></a>                <span class="k">del</span> <span class="n">Xpi</span>
</span><span id="L-1501"><a href="#L-1501"><span class="linenos">1501</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1502"><a href="#L-1502"><span class="linenos">1502</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">Xpi</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="L-1503"><a href="#L-1503"><span class="linenos">1503</span></a>                <span class="k">del</span> <span class="n">Xpi</span>
</span><span id="L-1504"><a href="#L-1504"><span class="linenos">1504</span></a>
</span><span id="L-1505"><a href="#L-1505"><span class="linenos">1505</span></a>            <span class="c1"># Once we have Wps we don&#39;t need W_big and Xpi</span>
</span><span id="L-1506"><a href="#L-1506"><span class="linenos">1506</span></a>            <span class="k">del</span> <span class="n">W_big</span>
</span><span id="L-1507"><a href="#L-1507"><span class="linenos">1507</span></a>
</span><span id="L-1508"><a href="#L-1508"><span class="linenos">1508</span></a>            <span class="c1"># 2.6 seconds</span>
</span><span id="L-1509"><a href="#L-1509"><span class="linenos">1509</span></a>            <span class="n">slice_info_elements</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-1510"><a href="#L-1510"><span class="linenos">1510</span></a>                                                    <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1511"><a href="#L-1511"><span class="linenos">1511</span></a>            <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">cumsum_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">slice_info_elements</span><span class="p">,</span>
</span><span id="L-1512"><a href="#L-1512"><span class="linenos">1512</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1513"><a href="#L-1513"><span class="linenos">1513</span></a>                                             <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1514"><a href="#L-1514"><span class="linenos">1514</span></a>
</span><span id="L-1515"><a href="#L-1515"><span class="linenos">1515</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">Wps</span><span class="p">)</span>
</span><span id="L-1516"><a href="#L-1516"><span class="linenos">1516</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">Wps_sum</span><span class="p">)</span>
</span><span id="L-1517"><a href="#L-1517"><span class="linenos">1517</span></a>
</span><span id="L-1518"><a href="#L-1518"><span class="linenos">1518</span></a>            <span class="k">if</span> <span class="n">cartesian_mode</span><span class="p">:</span>
</span><span id="L-1519"><a href="#L-1519"><span class="linenos">1519</span></a>                <span class="c1"># Note:</span>
</span><span id="L-1520"><a href="#L-1520"><span class="linenos">1520</span></a>                <span class="c1"># These repmats may be avoided if ag.sinc_cos_sparse could take frequencies as a separate input, and broadcast all inputs accordingly.</span>
</span><span id="L-1521"><a href="#L-1521"><span class="linenos">1521</span></a>                <span class="c1"># But sinc_diffs is of the same size as Wps and Wps_sum, so we could reduce the memory usage at most by 2/3, and only in cartesian mode.</span>
</span><span id="L-1522"><a href="#L-1522"><span class="linenos">1522</span></a>                <span class="c1"># This may not worth the effort.</span>
</span><span id="L-1523"><a href="#L-1523"><span class="linenos">1523</span></a>
</span><span id="L-1524"><a href="#L-1524"><span class="linenos">1524</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">freq_axis</span><span class="p">)</span>
</span><span id="L-1525"><a href="#L-1525"><span class="linenos">1525</span></a>                <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps_sum</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">freq_axis</span><span class="p">)</span>
</span><span id="L-1526"><a href="#L-1526"><span class="linenos">1526</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">Xps</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,))</span>
</span><span id="L-1527"><a href="#L-1527"><span class="linenos">1527</span></a>               
</span><span id="L-1528"><a href="#L-1528"><span class="linenos">1528</span></a>            <span class="c1"># Here we use the sum-to-product identity sin(2a)-sin(2b) = 2*sin(a-b)*cos(a+b)            </span>
</span><span id="L-1529"><a href="#L-1529"><span class="linenos">1529</span></a>            <span class="c1"># This formula probably leads to a loss of one significant digit, but it is much easier in the sparse case than using diff().</span>
</span><span id="L-1530"><a href="#L-1530"><span class="linenos">1530</span></a>            
</span><span id="L-1531"><a href="#L-1531"><span class="linenos">1531</span></a>            <span class="c1"># Variant 2 is more memory efficient</span>
</span><span id="L-1532"><a href="#L-1532"><span class="linenos">1532</span></a>            <span class="n">variant</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="L-1533"><a href="#L-1533"><span class="linenos">1533</span></a>
</span><span id="L-1534"><a href="#L-1534"><span class="linenos">1534</span></a>            <span class="k">if</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-1535"><a href="#L-1535"><span class="linenos">1535</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">frequencies</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">Wps_sum</span> <span class="o">-</span> <span class="n">Wps</span><span class="p">)</span>
</span><span id="L-1536"><a href="#L-1536"><span class="linenos">1536</span></a>                <span class="k">del</span> <span class="n">Wps_sum</span>
</span><span id="L-1537"><a href="#L-1537"><span class="linenos">1537</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">arg2</span><span class="p">)</span>
</span><span id="L-1538"><a href="#L-1538"><span class="linenos">1538</span></a>
</span><span id="L-1539"><a href="#L-1539"><span class="linenos">1539</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>                               
</span><span id="L-1540"><a href="#L-1540"><span class="linenos">1540</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">add_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">Wps_sum</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-1541"><a href="#L-1541"><span class="linenos">1541</span></a>                <span class="k">del</span> <span class="n">Wps_sum</span>
</span><span id="L-1542"><a href="#L-1542"><span class="linenos">1542</span></a>                <span class="c1"># 1.22 seconds</span>
</span><span id="L-1543"><a href="#L-1543"><span class="linenos">1543</span></a>                <span class="n">slice_info_freqs</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">),</span>
</span><span id="L-1544"><a href="#L-1544"><span class="linenos">1544</span></a>                                                     <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-1545"><a href="#L-1545"><span class="linenos">1545</span></a>                                                     <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1546"><a href="#L-1546"><span class="linenos">1546</span></a>                                                     <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1547"><a href="#L-1547"><span class="linenos">1547</span></a>                <span class="c1"># 0.15 seconds</span>
</span><span id="L-1548"><a href="#L-1548"><span class="linenos">1548</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">slice_info_freqs</span><span class="p">,</span>
</span><span id="L-1549"><a href="#L-1549"><span class="linenos">1549</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1550"><a href="#L-1550"><span class="linenos">1550</span></a>                                                 <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1551"><a href="#L-1551"><span class="linenos">1551</span></a>
</span><span id="L-1552"><a href="#L-1552"><span class="linenos">1552</span></a>            <span class="c1"># 0.14 seconds</span>
</span><span id="L-1553"><a href="#L-1553"><span class="linenos">1553</span></a>            <span class="n">arg1</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">slice_info_freqs</span><span class="p">,</span>
</span><span id="L-1554"><a href="#L-1554"><span class="linenos">1554</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1555"><a href="#L-1555"><span class="linenos">1555</span></a>                                             <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1556"><a href="#L-1556"><span class="linenos">1556</span></a>
</span><span id="L-1557"><a href="#L-1557"><span class="linenos">1557</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">arg1</span><span class="p">)</span>
</span><span id="L-1558"><a href="#L-1558"><span class="linenos">1558</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">arg2</span><span class="p">)</span>
</span><span id="L-1559"><a href="#L-1559"><span class="linenos">1559</span></a>
</span><span id="L-1560"><a href="#L-1560"><span class="linenos">1560</span></a>            <span class="c1"># 0.53 seconds</span>
</span><span id="L-1561"><a href="#L-1561"><span class="linenos">1561</span></a>            <span class="n">sinc_cos</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sinc_cos_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</span><span id="L-1562"><a href="#L-1562"><span class="linenos">1562</span></a>            <span class="k">del</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span>
</span><span id="L-1563"><a href="#L-1563"><span class="linenos">1563</span></a>            <span class="n">sinc_diffs</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span> <span class="n">Wps</span><span class="p">,</span> <span class="n">sinc_cos</span><span class="p">,</span> <span class="mi">2</span> <span class="p">)</span> 
</span><span id="L-1564"><a href="#L-1564"><span class="linenos">1564</span></a>
</span><span id="L-1565"><a href="#L-1565"><span class="linenos">1565</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">sinc_cos</span><span class="p">)</span>
</span><span id="L-1566"><a href="#L-1566"><span class="linenos">1566</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">)</span>
</span><span id="L-1567"><a href="#L-1567"><span class="linenos">1567</span></a>
</span><span id="L-1568"><a href="#L-1568"><span class="linenos">1568</span></a>            <span class="k">del</span> <span class="n">Wps</span><span class="p">,</span> <span class="n">sinc_cos</span>
</span><span id="L-1569"><a href="#L-1569"><span class="linenos">1569</span></a>           
</span><span id="L-1570"><a href="#L-1570"><span class="linenos">1570</span></a>        <span class="c1"># From here we only need sinc_diffs and Xps               </span>
</span><span id="L-1571"><a href="#L-1571"><span class="linenos">1571</span></a>
</span><span id="L-1572"><a href="#L-1572"><span class="linenos">1572</span></a>        <span class="k">if</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="L-1573"><a href="#L-1573"><span class="linenos">1573</span></a>            <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1574"><a href="#L-1574"><span class="linenos">1574</span></a>                <span class="c1"># 1.4 seconds</span>
</span><span id="L-1575"><a href="#L-1575"><span class="linenos">1575</span></a>                <span class="n">slice_info_Xps</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">),</span>
</span><span id="L-1576"><a href="#L-1576"><span class="linenos">1576</span></a>                                                   <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-1577"><a href="#L-1577"><span class="linenos">1577</span></a>                                                   <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1578"><a href="#L-1578"><span class="linenos">1578</span></a>                                                   <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1579"><a href="#L-1579"><span class="linenos">1579</span></a>                <span class="c1"># 0.26 seconds</span>
</span><span id="L-1580"><a href="#L-1580"><span class="linenos">1580</span></a>                <span class="n">products</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">slice_info_Xps</span><span class="p">,</span>
</span><span id="L-1581"><a href="#L-1581"><span class="linenos">1581</span></a>                                                     <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1582"><a href="#L-1582"><span class="linenos">1582</span></a>                                                     <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1583"><a href="#L-1583"><span class="linenos">1583</span></a>                <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">slice_info_Xps</span>
</span><span id="L-1584"><a href="#L-1584"><span class="linenos">1584</span></a>                <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">products</span><span class="p">)</span>
</span><span id="L-1585"><a href="#L-1585"><span class="linenos">1585</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1586"><a href="#L-1586"><span class="linenos">1586</span></a>                <span class="n">products</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xeps</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1587"><a href="#L-1587"><span class="linenos">1587</span></a>                <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xeps</span>
</span><span id="L-1588"><a href="#L-1588"><span class="linenos">1588</span></a>
</span><span id="L-1589"><a href="#L-1589"><span class="linenos">1589</span></a>            <span class="c1"># 0.49 seconds</span>
</span><span id="L-1590"><a href="#L-1590"><span class="linenos">1590</span></a>            <span class="n">product_sums</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sum_sparseToDense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">products</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">slice_info_elements</span><span class="p">,</span>
</span><span id="L-1591"><a href="#L-1591"><span class="linenos">1591</span></a>                                                      <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1592"><a href="#L-1592"><span class="linenos">1592</span></a>                                                      <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-1593"><a href="#L-1593"><span class="linenos">1593</span></a>            <span class="k">del</span> <span class="n">products</span><span class="p">,</span> <span class="n">slice_info_elements</span>
</span><span id="L-1594"><a href="#L-1594"><span class="linenos">1594</span></a>            
</span><span id="L-1595"><a href="#L-1595"><span class="linenos">1595</span></a>        <span class="k">else</span><span class="p">:</span> <span class="c1"># not sparse</span>
</span><span id="L-1596"><a href="#L-1596"><span class="linenos">1596</span></a>            <span class="n">product_sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sinc_diffs</span> <span class="o">*</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1597"><a href="#L-1597"><span class="linenos">1597</span></a>            <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span>
</span><span id="L-1598"><a href="#L-1598"><span class="linenos">1598</span></a>
</span><span id="L-1599"><a href="#L-1599"><span class="linenos">1599</span></a>        <span class="c1"># We squeeze the element axis after having summed up along it</span>
</span><span id="L-1600"><a href="#L-1600"><span class="linenos">1600</span></a>        <span class="n">product_sums</span> <span class="o">=</span> <span class="n">product_sums</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="L-1601"><a href="#L-1601"><span class="linenos">1601</span></a>        <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="L-1602"><a href="#L-1602"><span class="linenos">1602</span></a>
</span><span id="L-1603"><a href="#L-1603"><span class="linenos">1603</span></a>        <span class="c1"># frequencies and product_sums are always dense</span>
</span><span id="L-1604"><a href="#L-1604"><span class="linenos">1604</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">frequencies</span><span class="p">)</span> <span class="o">*</span> <span class="n">product_sums</span>            
</span><span id="L-1605"><a href="#L-1605"><span class="linenos">1605</span></a>        <span class="k">del</span> <span class="n">product_sums</span>
</span><span id="L-1606"><a href="#L-1606"><span class="linenos">1606</span></a>
</span><span id="L-1607"><a href="#L-1607"><span class="linenos">1607</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1608"><a href="#L-1608"><span class="linenos">1608</span></a>
</span><span id="L-1609"><a href="#L-1609"><span class="linenos">1609</span></a>
</span><span id="L-1610"><a href="#L-1610"><span class="linenos">1610</span></a>
</span><span id="L-1611"><a href="#L-1611"><span class="linenos">1611</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_get_mutual_coherence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-1612"><a href="#L-1612"><span class="linenos">1612</span></a>        <span class="n">gram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1613"><a href="#L-1613"><span class="linenos">1613</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span><span class="p">)</span>
</span><span id="L-1614"><a href="#L-1614"><span class="linenos">1614</span></a>        <span class="n">gram</span><span class="p">[</span><span class="n">inds</span><span class="p">,</span><span class="n">inds</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-1615"><a href="#L-1615"><span class="linenos">1615</span></a>
</span><span id="L-1616"><a href="#L-1616"><span class="linenos">1616</span></a>        <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">gram</span><span class="p">))</span>
</span><span id="L-1617"><a href="#L-1617"><span class="linenos">1617</span></a>        <span class="k">return</span> <span class="n">mu</span>
</span><span id="L-1618"><a href="#L-1618"><span class="linenos">1618</span></a>
</span><span id="L-1619"><a href="#L-1619"><span class="linenos">1619</span></a>
</span><span id="L-1620"><a href="#L-1620"><span class="linenos">1620</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-1621"><a href="#L-1621"><span class="linenos">1621</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_total_mass_homogeneous_legacy_encoding_part1</span><span class="p">(</span><span class="n">totmass</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1622"><a href="#L-1622"><span class="linenos">1622</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">totmass</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">totmass</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="n">totmass</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-1623"><a href="#L-1623"><span class="linenos">1623</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1624"><a href="#L-1624"><span class="linenos">1624</span></a>
</span><span id="L-1625"><a href="#L-1625"><span class="linenos">1625</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-1626"><a href="#L-1626"><span class="linenos">1626</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_total_mass_homogeneous_legacy_encoding_part2</span><span class="p">(</span><span class="n">totmass</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1627"><a href="#L-1627"><span class="linenos">1627</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">totmass</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">totmass</span><span class="o">.</span><span class="n">square</span><span class="p">(),</span> <span class="mi">2</span><span class="o">*</span><span class="n">totmass</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1628"><a href="#L-1628"><span class="linenos">1628</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1629"><a href="#L-1629"><span class="linenos">1629</span></a>
</span><span id="L-1630"><a href="#L-1630"><span class="linenos">1630</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-1631"><a href="#L-1631"><span class="linenos">1631</span></a><span class="c1">##                                                  Tools                                                  ##</span>
</span><span id="L-1632"><a href="#L-1632"><span class="linenos">1632</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-1633"><a href="#L-1633"><span class="linenos">1633</span></a>
</span><span id="L-1634"><a href="#L-1634"><span class="linenos">1634</span></a><span class="k">def</span><span class="w"> </span><span class="nf">timer_start</span><span class="p">():</span>
</span><span id="L-1635"><a href="#L-1635"><span class="linenos">1635</span></a>    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
</span><span id="L-1636"><a href="#L-1636"><span class="linenos">1636</span></a>    <span class="k">global</span> <span class="n">tal_global_timer</span><span class="p">,</span> <span class="n">tal_global_timer_start</span>
</span><span id="L-1637"><a href="#L-1637"><span class="linenos">1637</span></a>    <span class="n">tal_global_timer_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="L-1638"><a href="#L-1638"><span class="linenos">1638</span></a>           
</span><span id="L-1639"><a href="#L-1639"><span class="linenos">1639</span></a>
</span><span id="L-1640"><a href="#L-1640"><span class="linenos">1640</span></a>
</span><span id="L-1641"><a href="#L-1641"><span class="linenos">1641</span></a><span class="k">def</span><span class="w"> </span><span class="nf">timer_stop</span><span class="p">():</span>
</span><span id="L-1642"><a href="#L-1642"><span class="linenos">1642</span></a>    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
</span><span id="L-1643"><a href="#L-1643"><span class="linenos">1643</span></a>    <span class="k">global</span> <span class="n">tal_global_timer</span><span class="p">,</span> <span class="n">tal_global_timer_start</span>
</span><span id="L-1644"><a href="#L-1644"><span class="linenos">1644</span></a>    <span class="n">tal_global_timer</span> <span class="o">+=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">tal_global_timer_start</span>
</span><span id="L-1645"><a href="#L-1645"><span class="linenos">1645</span></a>
</span><span id="L-1646"><a href="#L-1646"><span class="linenos">1646</span></a>
</span><span id="L-1647"><a href="#L-1647"><span class="linenos">1647</span></a>
</span><span id="L-1648"><a href="#L-1648"><span class="linenos">1648</span></a><span class="k">def</span><span class="w"> </span><span class="nf">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
</span><span id="L-1649"><a href="#L-1649"><span class="linenos">1649</span></a>    <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;tensor must be sparse&#39;</span>
</span><span id="L-1650"><a href="#L-1650"><span class="linenos">1650</span></a>
</span><span id="L-1651"><a href="#L-1651"><span class="linenos">1651</span></a>    <span class="n">debug</span> <span class="o">=</span> <span class="n">fsw_embedding_debug_mode</span>
</span><span id="L-1652"><a href="#L-1652"><span class="linenos">1652</span></a>    <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
</span><span id="L-1653"><a href="#L-1653"><span class="linenos">1653</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;tensor is not coalesced&#39;</span>
</span><span id="L-1654"><a href="#L-1654"><span class="linenos">1654</span></a>
</span><span id="L-1655"><a href="#L-1655"><span class="linenos">1655</span></a>
</span><span id="L-1656"><a href="#L-1656"><span class="linenos">1656</span></a><span class="c1"># Computes a finite difference with zero padding</span>
</span><span id="L-1657"><a href="#L-1657"><span class="linenos">1657</span></a><span class="k">def</span><span class="w"> </span><span class="nf">diff_zeropad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span><span id="L-1658"><a href="#L-1658"><span class="linenos">1658</span></a>    <span class="n">pad_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-1659"><a href="#L-1659"><span class="linenos">1659</span></a>    <span class="n">pad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">pad_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1660"><a href="#L-1660"><span class="linenos">1660</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-1661"><a href="#L-1661"><span class="linenos">1661</span></a>    <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1662"><a href="#L-1662"><span class="linenos">1662</span></a>
</span><span id="L-1663"><a href="#L-1663"><span class="linenos">1663</span></a>
</span><span id="L-1664"><a href="#L-1664"><span class="linenos">1664</span></a>
</span><span id="L-1665"><a href="#L-1665"><span class="linenos">1665</span></a><span class="k">def</span><span class="w"> </span><span class="nf">replace_in_tuple</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span><span id="L-1666"><a href="#L-1666"><span class="linenos">1666</span></a>    <span class="n">T</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
</span><span id="L-1667"><a href="#L-1667"><span class="linenos">1667</span></a>    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">index</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-1668"><a href="#L-1668"><span class="linenos">1668</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span><span class="p">,)</span> <span class="o">+</span> <span class="n">T</span><span class="p">[(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
</span><span id="L-1669"><a href="#L-1669"><span class="linenos">1669</span></a>    <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1670"><a href="#L-1670"><span class="linenos">1670</span></a>
</span><span id="L-1671"><a href="#L-1671"><span class="linenos">1671</span></a>
</span><span id="L-1672"><a href="#L-1672"><span class="linenos">1672</span></a>
</span><span id="L-1673"><a href="#L-1673"><span class="linenos">1673</span></a>
</span><span id="L-1674"><a href="#L-1674"><span class="linenos">1674</span></a><span class="k">def</span><span class="w"> </span><span class="nf">qprint</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="L-1675"><a href="#L-1675"><span class="linenos">1675</span></a>    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1676"><a href="#L-1676"><span class="linenos">1676</span></a>
</span><span id="L-1677"><a href="#L-1677"><span class="linenos">1677</span></a>    <span class="k">if</span> <span class="n">q</span><span class="p">:</span>
</span><span id="L-1678"><a href="#L-1678"><span class="linenos">1678</span></a>        <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</span><span id="L-1679"><a href="#L-1679"><span class="linenos">1679</span></a>
</span><span id="L-1680"><a href="#L-1680"><span class="linenos">1680</span></a>
</span><span id="L-1681"><a href="#L-1681"><span class="linenos">1681</span></a>
</span><span id="L-1682"><a href="#L-1682"><span class="linenos">1682</span></a>
</span><span id="L-1683"><a href="#L-1683"><span class="linenos">1683</span></a><span class="k">def</span><span class="w"> </span><span class="nf">qprintln</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
</span><span id="L-1684"><a href="#L-1684"><span class="linenos">1684</span></a>    <span class="n">qprint</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="o">+</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="L-1685"><a href="#L-1685"><span class="linenos">1685</span></a>
</span><span id="L-1686"><a href="#L-1686"><span class="linenos">1686</span></a>
</span><span id="L-1687"><a href="#L-1687"><span class="linenos">1687</span></a>
</span><span id="L-1688"><a href="#L-1688"><span class="linenos">1688</span></a><span class="c1"># Performs something like target[:,:,...,:,inds,:,...,:] = source, where the argument &#39;inds&#39; is given at dimension dim</span>
</span><span id="L-1689"><a href="#L-1689"><span class="linenos">1689</span></a><span class="k">def</span><span class="w"> </span><span class="nf">assign_at</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">inds</span><span class="p">):</span>
</span><span id="L-1690"><a href="#L-1690"><span class="linenos">1690</span></a>    <span class="n">scatter_inds_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds</span><span class="p">))</span>               
</span><span id="L-1691"><a href="#L-1691"><span class="linenos">1691</span></a>    <span class="n">scatter_inds</span> <span class="o">=</span> <span class="n">inds</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scatter_inds_shape</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
</span><span id="L-1692"><a href="#L-1692"><span class="linenos">1692</span></a>    <span class="n">target</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">scatter_inds</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
</span><span id="L-1693"><a href="#L-1693"><span class="linenos">1693</span></a>    
</span><span id="L-1694"><a href="#L-1694"><span class="linenos">1694</span></a>
</span><span id="L-1695"><a href="#L-1695"><span class="linenos">1695</span></a><span class="n">_T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;_T&quot;</span><span class="p">)</span>
</span><span id="L-1696"><a href="#L-1696"><span class="linenos">1696</span></a><span class="n">_U</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;_U&quot;</span><span class="p">)</span>
</span><span id="L-1697"><a href="#L-1697"><span class="linenos">1697</span></a>
</span><span id="L-1698"><a href="#L-1698"><span class="linenos">1698</span></a><span class="nd">@overload</span>
</span><span id="L-1699"><a href="#L-1699"><span class="linenos">1699</span></a><span class="k">def</span><span class="w"> </span><span class="nf">ifnone</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">_T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_T</span><span class="p">:</span> <span class="o">...</span>
</span><span id="L-1700"><a href="#L-1700"><span class="linenos">1700</span></a><span class="nd">@overload</span>
</span><span id="L-1701"><a href="#L-1701"><span class="linenos">1701</span></a><span class="k">def</span><span class="w"> </span><span class="nf">ifnone</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">_T</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">_U</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_T</span><span class="p">:</span> <span class="o">...</span>
</span><span id="L-1702"><a href="#L-1702"><span class="linenos">1702</span></a>
</span><span id="L-1703"><a href="#L-1703"><span class="linenos">1703</span></a><span class="k">def</span><span class="w"> </span><span class="nf">ifnone</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span><span id="L-1704"><a href="#L-1704"><span class="linenos">1704</span></a>    <span class="k">return</span> <span class="n">a</span> <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span>
</span><span id="L-1705"><a href="#L-1705"><span class="linenos">1705</span></a>
</span><span id="L-1706"><a href="#L-1706"><span class="linenos">1706</span></a>
</span><span id="L-1707"><a href="#L-1707"><span class="linenos">1707</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-1708"><a href="#L-1708"><span class="linenos">1708</span></a><span class="c1">##                                        Custom autograd functions                                        ##</span>
</span><span id="L-1709"><a href="#L-1709"><span class="linenos">1709</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-1710"><a href="#L-1710"><span class="linenos">1710</span></a>
</span><span id="L-1711"><a href="#L-1711"><span class="linenos">1711</span></a><span class="c1"># These implementations were based on the example in</span>
</span><span id="L-1712"><a href="#L-1712"><span class="linenos">1712</span></a><span class="c1"># https://pytorch.org/tutorials/beginner/examples_WithAutograd/two_layer_net_custom_function.html</span>
</span><span id="L-1713"><a href="#L-1713"><span class="linenos">1713</span></a>
</span><span id="L-1714"><a href="#L-1714"><span class="linenos">1714</span></a><span class="c1"># This code is based on Torch version 2.1.1, whose current support for gradients of sparse tensors sucks.</span>
</span><span id="L-1715"><a href="#L-1715"><span class="linenos">1715</span></a><span class="c1"># Therefore I had to implement many of basic sparse-tensor operations used in this code myself.</span>
</span><span id="L-1716"><a href="#L-1716"><span class="linenos">1716</span></a><span class="c1"># Should torch add autograd support for some of these operations in later versions, I will replace them adqeuately.</span>
</span><span id="L-1717"><a href="#L-1717"><span class="linenos">1717</span></a>
</span><span id="L-1718"><a href="#L-1718"><span class="linenos">1718</span></a><span class="c1"># Most actions here that take sparse tensor inputs require them to be coalesced, and actions</span>
</span><span id="L-1719"><a href="#L-1719"><span class="linenos">1719</span></a><span class="c1"># that return sparse outputs return coalesced outputs.</span>
</span><span id="L-1720"><a href="#L-1720"><span class="linenos">1720</span></a>
</span><span id="L-1721"><a href="#L-1721"><span class="linenos">1721</span></a><span class="c1"># All custom autograd functions used here are defined under the &#39;ag&#39; class.</span>
</span><span id="L-1722"><a href="#L-1722"><span class="linenos">1722</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ag</span><span class="p">:</span>
</span><span id="L-1723"><a href="#L-1723"><span class="linenos">1723</span></a>    <span class="c1"># Ensure that the output gradient is coalesced</span>
</span><span id="L-1724"><a href="#L-1724"><span class="linenos">1724</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">coalesce_grad</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1725"><a href="#L-1725"><span class="linenos">1725</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1726"><a href="#L-1726"><span class="linenos">1726</span></a>        <span class="nd">@staticmethod</span>        
</span><span id="L-1727"><a href="#L-1727"><span class="linenos">1727</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1728"><a href="#L-1728"><span class="linenos">1728</span></a>            <span class="c1"># TODO: Do we need to return A.clone() rather than A?</span>
</span><span id="L-1729"><a href="#L-1729"><span class="linenos">1729</span></a>            <span class="k">return</span> <span class="n">A</span>
</span><span id="L-1730"><a href="#L-1730"><span class="linenos">1730</span></a>
</span><span id="L-1731"><a href="#L-1731"><span class="linenos">1731</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1732"><a href="#L-1732"><span class="linenos">1732</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-1733"><a href="#L-1733"><span class="linenos">1733</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1734"><a href="#L-1734"><span class="linenos">1734</span></a>            <span class="k">if</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()):</span>
</span><span id="L-1735"><a href="#L-1735"><span class="linenos">1735</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
</span><span id="L-1736"><a href="#L-1736"><span class="linenos">1736</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1737"><a href="#L-1737"><span class="linenos">1737</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span>
</span><span id="L-1738"><a href="#L-1738"><span class="linenos">1738</span></a>
</span><span id="L-1739"><a href="#L-1739"><span class="linenos">1739</span></a>            <span class="k">return</span> <span class="n">grad_input</span>
</span><span id="L-1740"><a href="#L-1740"><span class="linenos">1740</span></a>
</span><span id="L-1741"><a href="#L-1741"><span class="linenos">1741</span></a>
</span><span id="L-1742"><a href="#L-1742"><span class="linenos">1742</span></a>    <span class="c1"># Permutes each 1-dimensional slice of A along dimension dim according to the given permutation in perms.</span>
</span><span id="L-1743"><a href="#L-1743"><span class="linenos">1743</span></a>    <span class="c1"># Perms can be broadcast to the size of A along dimension broadcast_perms_dim.</span>
</span><span id="L-1744"><a href="#L-1744"><span class="linenos">1744</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">permute_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1745"><a href="#L-1745"><span class="linenos">1745</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1746"><a href="#L-1746"><span class="linenos">1746</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1747"><a href="#L-1747"><span class="linenos">1747</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">perms</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">broadcast_perms_dim</span><span class="p">):</span>
</span><span id="L-1748"><a href="#L-1748"><span class="linenos">1748</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-1749"><a href="#L-1749"><span class="linenos">1749</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-1750"><a href="#L-1750"><span class="linenos">1750</span></a>
</span><span id="L-1751"><a href="#L-1751"><span class="linenos">1751</span></a>            <span class="c1"># 1.8 seconds</span>
</span><span id="L-1752"><a href="#L-1752"><span class="linenos">1752</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">perms</span><span class="o">=</span><span class="n">perms</span><span class="p">,</span> <span class="n">broadcast_perms_dim</span><span class="o">=</span><span class="n">broadcast_perms_dim</span><span class="p">,</span> <span class="n">backward_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>            
</span><span id="L-1753"><a href="#L-1753"><span class="linenos">1753</span></a>
</span><span id="L-1754"><a href="#L-1754"><span class="linenos">1754</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-1755"><a href="#L-1755"><span class="linenos">1755</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-1756"><a href="#L-1756"><span class="linenos">1756</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_perms_dim</span> <span class="o">=</span> <span class="n">broadcast_perms_dim</span>
</span><span id="L-1757"><a href="#L-1757"><span class="linenos">1757</span></a>
</span><span id="L-1758"><a href="#L-1758"><span class="linenos">1758</span></a>                <span class="c1"># Try to save space by converting perms to the smallest integer type that can represent it</span>
</span><span id="L-1759"><a href="#L-1759"><span class="linenos">1759</span></a>                <span class="n">perms_max</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>
</span><span id="L-1760"><a href="#L-1760"><span class="linenos">1760</span></a>
</span><span id="L-1761"><a href="#L-1761"><span class="linenos">1761</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-1762"><a href="#L-1762"><span class="linenos">1762</span></a>                    <span class="k">assert</span> <span class="n">perms_max</span> <span class="o">==</span> <span class="n">perms</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="c1"># Sanity check</span>
</span><span id="L-1763"><a href="#L-1763"><span class="linenos">1763</span></a>
</span><span id="L-1764"><a href="#L-1764"><span class="linenos">1764</span></a>                <span class="k">if</span> <span class="n">perms_max</span> <span class="o">&lt;=</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
</span><span id="L-1765"><a href="#L-1765"><span class="linenos">1765</span></a>                    <span class="n">perms</span> <span class="o">=</span> <span class="n">perms</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
</span><span id="L-1766"><a href="#L-1766"><span class="linenos">1766</span></a>                <span class="k">elif</span> <span class="n">perms_max</span> <span class="o">&lt;=</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
</span><span id="L-1767"><a href="#L-1767"><span class="linenos">1767</span></a>                    <span class="n">perms</span> <span class="o">=</span> <span class="n">perms</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="L-1768"><a href="#L-1768"><span class="linenos">1768</span></a>
</span><span id="L-1769"><a href="#L-1769"><span class="linenos">1769</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">perms</span> <span class="o">=</span> <span class="n">perms</span>
</span><span id="L-1770"><a href="#L-1770"><span class="linenos">1770</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">nvals_A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-1771"><a href="#L-1771"><span class="linenos">1771</span></a>
</span><span id="L-1772"><a href="#L-1772"><span class="linenos">1772</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-1773"><a href="#L-1773"><span class="linenos">1773</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1774"><a href="#L-1774"><span class="linenos">1774</span></a>
</span><span id="L-1775"><a href="#L-1775"><span class="linenos">1775</span></a>
</span><span id="L-1776"><a href="#L-1776"><span class="linenos">1776</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1777"><a href="#L-1777"><span class="linenos">1777</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-1778"><a href="#L-1778"><span class="linenos">1778</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
</span><span id="L-1779"><a href="#L-1779"><span class="linenos">1779</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
</span><span id="L-1780"><a href="#L-1780"><span class="linenos">1780</span></a>                <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;This is strange. Autograd requires gradient of permute_sparse() wrt the permutations&#39;</span>
</span><span id="L-1781"><a href="#L-1781"><span class="linenos">1781</span></a>
</span><span id="L-1782"><a href="#L-1782"><span class="linenos">1782</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-1783"><a href="#L-1783"><span class="linenos">1783</span></a>                <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1784"><a href="#L-1784"><span class="linenos">1784</span></a>            
</span><span id="L-1785"><a href="#L-1785"><span class="linenos">1785</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span>
</span><span id="L-1786"><a href="#L-1786"><span class="linenos">1786</span></a>            <span class="n">broadcast_perms_dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_perms_dim</span>
</span><span id="L-1787"><a href="#L-1787"><span class="linenos">1787</span></a>            <span class="n">perms</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">perms</span>
</span><span id="L-1788"><a href="#L-1788"><span class="linenos">1788</span></a>            <span class="n">nvals_grad_output</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-1789"><a href="#L-1789"><span class="linenos">1789</span></a>            <span class="n">nvals_A</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">nvals_A</span>
</span><span id="L-1790"><a href="#L-1790"><span class="linenos">1790</span></a>
</span><span id="L-1791"><a href="#L-1791"><span class="linenos">1791</span></a>            <span class="c1"># Note: The output of the forward() of this function is used in four other functions:</span>
</span><span id="L-1792"><a href="#L-1792"><span class="linenos">1792</span></a>            <span class="c1">#       add_same_pattern, mul_same_pattern, mul_sparse_dense, cumsum_sparse</span>
</span><span id="L-1793"><a href="#L-1793"><span class="linenos">1793</span></a>            <span class="c1">#       and thus grad_output here is the sum of the four different grad_inputs returned by the backward() of these functions.</span>
</span><span id="L-1794"><a href="#L-1794"><span class="linenos">1794</span></a>            <span class="c1">#       It is likely that torch&#39;s autograd does not take into account that these four gradients are coalesced and have the same nonzero pattern,</span>
</span><span id="L-1795"><a href="#L-1795"><span class="linenos">1795</span></a>            <span class="c1">#       and thus we might end up here with an uncoalesced gradient, possibly with repeated entries.</span>
</span><span id="L-1796"><a href="#L-1796"><span class="linenos">1796</span></a>            <span class="c1">#       This happens in my tests, but two-fold rather than four-fold. When this is indeed the case, the two parts are united manually in the code below.</span>
</span><span id="L-1797"><a href="#L-1797"><span class="linenos">1797</span></a>            <span class="c1">#       Until there will be some way to explicitly implement that part of autograd&#39;s summation and exploit the fact that the different gradients</span>
</span><span id="L-1798"><a href="#L-1798"><span class="linenos">1798</span></a>            <span class="c1">#       are coalesced and have the same nonzero pattern. Or implement a class that inherits from torch&#39;s sparse tensors and implements custom</span>
</span><span id="L-1799"><a href="#L-1799"><span class="linenos">1799</span></a>            <span class="c1">#       A + B operator that assumes that A,B are coalesced and have the same nonzero pattern (and verifies it in debug mode).</span>
</span><span id="L-1800"><a href="#L-1800"><span class="linenos">1800</span></a>
</span><span id="L-1801"><a href="#L-1801"><span class="linenos">1801</span></a>            <span class="c1"># TODO: Potential speedup and memory efficiency improvement: </span>
</span><span id="L-1802"><a href="#L-1802"><span class="linenos">1802</span></a>            <span class="c1">#       Consider moving from torch&#39;s sparse tensors to manual (indices,values) representation, which would make autograd produce the correct result</span>
</span><span id="L-1803"><a href="#L-1803"><span class="linenos">1803</span></a>            <span class="c1">#       automatically. This will obviate the need to save two to four different copies of the indices of this huge tensor.</span>
</span><span id="L-1804"><a href="#L-1804"><span class="linenos">1804</span></a>
</span><span id="L-1805"><a href="#L-1805"><span class="linenos">1805</span></a>            <span class="k">if</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">():</span>
</span><span id="L-1806"><a href="#L-1806"><span class="linenos">1806</span></a>                <span class="k">pass</span>
</span><span id="L-1807"><a href="#L-1807"><span class="linenos">1807</span></a>            <span class="k">elif</span> <span class="n">nvals_grad_output</span> <span class="o">%</span> <span class="n">nvals_A</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-1808"><a href="#L-1808"><span class="linenos">1808</span></a>                <span class="n">grad_output</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">coalesce_repeated</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">nvals_A</span><span class="p">)</span>
</span><span id="L-1809"><a href="#L-1809"><span class="linenos">1809</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1810"><a href="#L-1810"><span class="linenos">1810</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-1811"><a href="#L-1811"><span class="linenos">1811</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Strange. Check why this happens&#39;</span><span class="p">)</span>
</span><span id="L-1812"><a href="#L-1812"><span class="linenos">1812</span></a>                <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
</span><span id="L-1813"><a href="#L-1813"><span class="linenos">1813</span></a>                
</span><span id="L-1814"><a href="#L-1814"><span class="linenos">1814</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">perms</span><span class="o">=</span><span class="n">perms</span><span class="p">,</span> <span class="n">broadcast_perms_dim</span><span class="o">=</span><span class="n">broadcast_perms_dim</span><span class="p">,</span> <span class="n">backward_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-1815"><a href="#L-1815"><span class="linenos">1815</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-1816"><a href="#L-1816"><span class="linenos">1816</span></a>
</span><span id="L-1817"><a href="#L-1817"><span class="linenos">1817</span></a>            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1818"><a href="#L-1818"><span class="linenos">1818</span></a>
</span><span id="L-1819"><a href="#L-1819"><span class="linenos">1819</span></a>
</span><span id="L-1820"><a href="#L-1820"><span class="linenos">1820</span></a>
</span><span id="L-1821"><a href="#L-1821"><span class="linenos">1821</span></a>    <span class="c1"># Permutes the values of a sparse tensor according to the input permutation</span>
</span><span id="L-1822"><a href="#L-1822"><span class="linenos">1822</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">permute_sparse_vals</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1823"><a href="#L-1823"><span class="linenos">1823</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1824"><a href="#L-1824"><span class="linenos">1824</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1825"><a href="#L-1825"><span class="linenos">1825</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">perm</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1826"><a href="#L-1826"><span class="linenos">1826</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">sparse_tensor</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-1827"><a href="#L-1827"><span class="linenos">1827</span></a>
</span><span id="L-1828"><a href="#L-1828"><span class="linenos">1828</span></a>            <span class="c1"># Extract the indices and values</span>
</span><span id="L-1829"><a href="#L-1829"><span class="linenos">1829</span></a>            <span class="n">indices</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-1830"><a href="#L-1830"><span class="linenos">1830</span></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1831"><a href="#L-1831"><span class="linenos">1831</span></a>            
</span><span id="L-1832"><a href="#L-1832"><span class="linenos">1832</span></a>            <span class="c1"># Permute the values according to the input permutation</span>
</span><span id="L-1833"><a href="#L-1833"><span class="linenos">1833</span></a>            <span class="n">permuted_values</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
</span><span id="L-1834"><a href="#L-1834"><span class="linenos">1834</span></a>            <span class="k">del</span> <span class="n">values</span>
</span><span id="L-1835"><a href="#L-1835"><span class="linenos">1835</span></a>            
</span><span id="L-1836"><a href="#L-1836"><span class="linenos">1836</span></a>            <span class="c1"># Create a new sparse tensor with permuted values</span>
</span><span id="L-1837"><a href="#L-1837"><span class="linenos">1837</span></a>            <span class="n">output_sparse_tensor</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">permuted_values</span><span class="p">,</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</span><span id="L-1838"><a href="#L-1838"><span class="linenos">1838</span></a>            <span class="k">del</span> <span class="n">indices</span><span class="p">,</span> <span class="n">permuted_values</span>
</span><span id="L-1839"><a href="#L-1839"><span class="linenos">1839</span></a>
</span><span id="L-1840"><a href="#L-1840"><span class="linenos">1840</span></a>            <span class="c1"># Save the permutation for backward</span>
</span><span id="L-1841"><a href="#L-1841"><span class="linenos">1841</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">nvals</span> <span class="o">=</span> <span class="n">sparse_tensor</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-1842"><a href="#L-1842"><span class="linenos">1842</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-1843"><a href="#L-1843"><span class="linenos">1843</span></a>
</span><span id="L-1844"><a href="#L-1844"><span class="linenos">1844</span></a>            <span class="k">return</span> <span class="n">output_sparse_tensor</span>
</span><span id="L-1845"><a href="#L-1845"><span class="linenos">1845</span></a>
</span><span id="L-1846"><a href="#L-1846"><span class="linenos">1846</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1847"><a href="#L-1847"><span class="linenos">1847</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1848"><a href="#L-1848"><span class="linenos">1848</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-1849"><a href="#L-1849"><span class="linenos">1849</span></a>            <span class="c1"># Retrieve the permutation</span>
</span><span id="L-1850"><a href="#L-1850"><span class="linenos">1850</span></a>            <span class="n">perm</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">perm</span>            
</span><span id="L-1851"><a href="#L-1851"><span class="linenos">1851</span></a>
</span><span id="L-1852"><a href="#L-1852"><span class="linenos">1852</span></a>            <span class="n">grad_output</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">coalesce_repeated</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">nvals</span><span class="p">)</span>
</span><span id="L-1853"><a href="#L-1853"><span class="linenos">1853</span></a>
</span><span id="L-1854"><a href="#L-1854"><span class="linenos">1854</span></a>            <span class="c1"># The gradient w.r.t. the input values is the permuted gradient output</span>
</span><span id="L-1855"><a href="#L-1855"><span class="linenos">1855</span></a>            <span class="n">grad_input_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</span><span id="L-1856"><a href="#L-1856"><span class="linenos">1856</span></a>            <span class="n">grad_input_values</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1857"><a href="#L-1857"><span class="linenos">1857</span></a>            
</span><span id="L-1858"><a href="#L-1858"><span class="linenos">1858</span></a>            <span class="c1"># Create a gradient sparse tensor</span>
</span><span id="L-1859"><a href="#L-1859"><span class="linenos">1859</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">grad_input_values</span><span class="p">,</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1860"><a href="#L-1860"><span class="linenos">1860</span></a>            
</span><span id="L-1861"><a href="#L-1861"><span class="linenos">1861</span></a>            <span class="c1"># No gradient for the permutation itself (None)</span>
</span><span id="L-1862"><a href="#L-1862"><span class="linenos">1862</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1863"><a href="#L-1863"><span class="linenos">1863</span></a>
</span><span id="L-1864"><a href="#L-1864"><span class="linenos">1864</span></a>
</span><span id="L-1865"><a href="#L-1865"><span class="linenos">1865</span></a>
</span><span id="L-1866"><a href="#L-1866"><span class="linenos">1866</span></a>    <span class="c1"># Equivalent to: torch.unsqueeze(dim)</span>
</span><span id="L-1867"><a href="#L-1867"><span class="linenos">1867</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">unsqueeze_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1868"><a href="#L-1868"><span class="linenos">1868</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1869"><a href="#L-1869"><span class="linenos">1869</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1870"><a href="#L-1870"><span class="linenos">1870</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span><span id="L-1871"><a href="#L-1871"><span class="linenos">1871</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-1872"><a href="#L-1872"><span class="linenos">1872</span></a>
</span><span id="L-1873"><a href="#L-1873"><span class="linenos">1873</span></a>            <span class="c1"># Note that here it is dim+A.dim()+1 rather than the usual dim + A.dim(), as defined in the unsqueeze() command</span>
</span><span id="L-1874"><a href="#L-1874"><span class="linenos">1874</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">)</span>
</span><span id="L-1875"><a href="#L-1875"><span class="linenos">1875</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-1876"><a href="#L-1876"><span class="linenos">1876</span></a>
</span><span id="L-1877"><a href="#L-1877"><span class="linenos">1877</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>            
</span><span id="L-1878"><a href="#L-1878"><span class="linenos">1878</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-1879"><a href="#L-1879"><span class="linenos">1879</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">inds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">,:],</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">inds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">inds</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">:,:]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span>
</span><span id="L-1880"><a href="#L-1880"><span class="linenos">1880</span></a>            
</span><span id="L-1881"><a href="#L-1881"><span class="linenos">1881</span></a>            <span class="n">output_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ctx</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">:]</span>
</span><span id="L-1882"><a href="#L-1882"><span class="linenos">1882</span></a>
</span><span id="L-1883"><a href="#L-1883"><span class="linenos">1883</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
</span><span id="L-1884"><a href="#L-1884"><span class="linenos">1884</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1885"><a href="#L-1885"><span class="linenos">1885</span></a>
</span><span id="L-1886"><a href="#L-1886"><span class="linenos">1886</span></a>
</span><span id="L-1887"><a href="#L-1887"><span class="linenos">1887</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1888"><a href="#L-1888"><span class="linenos">1888</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-1889"><a href="#L-1889"><span class="linenos">1889</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>        
</span><span id="L-1890"><a href="#L-1890"><span class="linenos">1890</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span>          
</span><span id="L-1891"><a href="#L-1891"><span class="linenos">1891</span></a>            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">input_shape</span>
</span><span id="L-1892"><a href="#L-1892"><span class="linenos">1892</span></a>
</span><span id="L-1893"><a href="#L-1893"><span class="linenos">1893</span></a>            <span class="n">input_dims_in_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-1894"><a href="#L-1894"><span class="linenos">1894</span></a>            <span class="n">input_dims_in_output</span> <span class="o">=</span> <span class="n">input_dims_in_output</span><span class="p">[</span><span class="n">input_dims_in_output</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">]</span>
</span><span id="L-1895"><a href="#L-1895"><span class="linenos">1895</span></a>
</span><span id="L-1896"><a href="#L-1896"><span class="linenos">1896</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input_dims_in_output</span><span class="p">)</span>
</span><span id="L-1897"><a href="#L-1897"><span class="linenos">1897</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-1898"><a href="#L-1898"><span class="linenos">1898</span></a>
</span><span id="L-1899"><a href="#L-1899"><span class="linenos">1899</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
</span><span id="L-1900"><a href="#L-1900"><span class="linenos">1900</span></a>
</span><span id="L-1901"><a href="#L-1901"><span class="linenos">1901</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1902"><a href="#L-1902"><span class="linenos">1902</span></a>
</span><span id="L-1903"><a href="#L-1903"><span class="linenos">1903</span></a>
</span><span id="L-1904"><a href="#L-1904"><span class="linenos">1904</span></a>
</span><span id="L-1905"><a href="#L-1905"><span class="linenos">1905</span></a>    <span class="c1"># Equivalent to: torch.sum(A, dim=dim, keepdim=True)</span>
</span><span id="L-1906"><a href="#L-1906"><span class="linenos">1906</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">sum_sparseToDense</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1907"><a href="#L-1907"><span class="linenos">1907</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1908"><a href="#L-1908"><span class="linenos">1908</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1909"><a href="#L-1909"><span class="linenos">1909</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-1910"><a href="#L-1910"><span class="linenos">1910</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A must be sparse&#39;</span>
</span><span id="L-1911"><a href="#L-1911"><span class="linenos">1911</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-1912"><a href="#L-1912"><span class="linenos">1912</span></a>
</span><span id="L-1913"><a href="#L-1913"><span class="linenos">1913</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sum_sparse</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span>
</span><span id="L-1914"><a href="#L-1914"><span class="linenos">1914</span></a>                                <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-1915"><a href="#L-1915"><span class="linenos">1915</span></a>                                <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-1916"><a href="#L-1916"><span class="linenos">1916</span></a>
</span><span id="L-1917"><a href="#L-1917"><span class="linenos">1917</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-1918"><a href="#L-1918"><span class="linenos">1918</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-1919"><a href="#L-1919"><span class="linenos">1919</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-1920"><a href="#L-1920"><span class="linenos">1920</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">A_inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-1921"><a href="#L-1921"><span class="linenos">1921</span></a>
</span><span id="L-1922"><a href="#L-1922"><span class="linenos">1922</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1923"><a href="#L-1923"><span class="linenos">1923</span></a>
</span><span id="L-1924"><a href="#L-1924"><span class="linenos">1924</span></a>
</span><span id="L-1925"><a href="#L-1925"><span class="linenos">1925</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1926"><a href="#L-1926"><span class="linenos">1926</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-1927"><a href="#L-1927"><span class="linenos">1927</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>        
</span><span id="L-1928"><a href="#L-1928"><span class="linenos">1928</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s2">&quot;This shouldn&#39;t happen&quot;</span>
</span><span id="L-1929"><a href="#L-1929"><span class="linenos">1929</span></a>
</span><span id="L-1930"><a href="#L-1930"><span class="linenos">1930</span></a>            <span class="c1">#dim = ctx.dim</span>
</span><span id="L-1931"><a href="#L-1931"><span class="linenos">1931</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-1932"><a href="#L-1932"><span class="linenos">1932</span></a>            <span class="n">A_inds</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">A_inds</span>
</span><span id="L-1933"><a href="#L-1933"><span class="linenos">1933</span></a>           
</span><span id="L-1934"><a href="#L-1934"><span class="linenos">1934</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A_inds</span><span class="p">)]</span>
</span><span id="L-1935"><a href="#L-1935"><span class="linenos">1935</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A_inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1936"><a href="#L-1936"><span class="linenos">1936</span></a>
</span><span id="L-1937"><a href="#L-1937"><span class="linenos">1937</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
</span><span id="L-1938"><a href="#L-1938"><span class="linenos">1938</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1939"><a href="#L-1939"><span class="linenos">1939</span></a>
</span><span id="L-1940"><a href="#L-1940"><span class="linenos">1940</span></a>
</span><span id="L-1941"><a href="#L-1941"><span class="linenos">1941</span></a>
</span><span id="L-1942"><a href="#L-1942"><span class="linenos">1942</span></a>    <span class="c1"># Given two sparse tensors A,B and two scalars a,b, compute a*A + b*B</span>
</span><span id="L-1943"><a href="#L-1943"><span class="linenos">1943</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">add_same_pattern</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-1944"><a href="#L-1944"><span class="linenos">1944</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-1945"><a href="#L-1945"><span class="linenos">1945</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1946"><a href="#L-1946"><span class="linenos">1946</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
</span><span id="L-1947"><a href="#L-1947"><span class="linenos">1947</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-1948"><a href="#L-1948"><span class="linenos">1948</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-1949"><a href="#L-1949"><span class="linenos">1949</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-1950"><a href="#L-1950"><span class="linenos">1950</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-1951"><a href="#L-1951"><span class="linenos">1951</span></a>
</span><span id="L-1952"><a href="#L-1952"><span class="linenos">1952</span></a>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="L-1953"><a href="#L-1953"><span class="linenos">1953</span></a>                <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">b</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1954"><a href="#L-1954"><span class="linenos">1954</span></a>            <span class="k">elif</span> <span class="n">a</span> <span class="o">==</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">:</span>
</span><span id="L-1955"><a href="#L-1955"><span class="linenos">1955</span></a>                <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">neg</span><span class="p">(),</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">b</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1956"><a href="#L-1956"><span class="linenos">1956</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-1957"><a href="#L-1957"><span class="linenos">1957</span></a>                <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">b</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1958"><a href="#L-1958"><span class="linenos">1958</span></a>
</span><span id="L-1959"><a href="#L-1959"><span class="linenos">1959</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">or</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="s1">&#39;gradients of a,b are currently not implemented&#39;</span>
</span><span id="L-1960"><a href="#L-1960"><span class="linenos">1960</span></a>
</span><span id="L-1961"><a href="#L-1961"><span class="linenos">1961</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
</span><span id="L-1962"><a href="#L-1962"><span class="linenos">1962</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
</span><span id="L-1963"><a href="#L-1963"><span class="linenos">1963</span></a>
</span><span id="L-1964"><a href="#L-1964"><span class="linenos">1964</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-1965"><a href="#L-1965"><span class="linenos">1965</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-1966"><a href="#L-1966"><span class="linenos">1966</span></a>
</span><span id="L-1967"><a href="#L-1967"><span class="linenos">1967</span></a>
</span><span id="L-1968"><a href="#L-1968"><span class="linenos">1968</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-1969"><a href="#L-1969"><span class="linenos">1969</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-1970"><a href="#L-1970"><span class="linenos">1970</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>        
</span><span id="L-1971"><a href="#L-1971"><span class="linenos">1971</span></a>            <span class="n">grad_A</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-1972"><a href="#L-1972"><span class="linenos">1972</span></a>            <span class="n">grad_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-1973"><a href="#L-1973"><span class="linenos">1973</span></a>
</span><span id="L-1974"><a href="#L-1974"><span class="linenos">1974</span></a>            <span class="n">a</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">a</span>
</span><span id="L-1975"><a href="#L-1975"><span class="linenos">1975</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">b</span>
</span><span id="L-1976"><a href="#L-1976"><span class="linenos">1976</span></a>
</span><span id="L-1977"><a href="#L-1977"><span class="linenos">1977</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-1978"><a href="#L-1978"><span class="linenos">1978</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-1979"><a href="#L-1979"><span class="linenos">1979</span></a>
</span><span id="L-1980"><a href="#L-1980"><span class="linenos">1980</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-1981"><a href="#L-1981"><span class="linenos">1981</span></a>                <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="L-1982"><a href="#L-1982"><span class="linenos">1982</span></a>                    <span class="n">grad_A</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-1983"><a href="#L-1983"><span class="linenos">1983</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-1984"><a href="#L-1984"><span class="linenos">1984</span></a>                    <span class="n">grad_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">a</span><span class="o">*</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1985"><a href="#L-1985"><span class="linenos">1985</span></a>            
</span><span id="L-1986"><a href="#L-1986"><span class="linenos">1986</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-1987"><a href="#L-1987"><span class="linenos">1987</span></a>                <span class="k">if</span> <span class="n">b</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="L-1988"><a href="#L-1988"><span class="linenos">1988</span></a>                    <span class="n">grad_B</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-1989"><a href="#L-1989"><span class="linenos">1989</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-1990"><a href="#L-1990"><span class="linenos">1990</span></a>                    <span class="n">grad_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">b</span><span class="o">*</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-1991"><a href="#L-1991"><span class="linenos">1991</span></a>
</span><span id="L-1992"><a href="#L-1992"><span class="linenos">1992</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_A</span><span class="p">)</span>
</span><span id="L-1993"><a href="#L-1993"><span class="linenos">1993</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_B</span><span class="p">)</span>
</span><span id="L-1994"><a href="#L-1994"><span class="linenos">1994</span></a>            <span class="k">return</span> <span class="n">grad_A</span><span class="p">,</span> <span class="n">grad_B</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-1995"><a href="#L-1995"><span class="linenos">1995</span></a>
</span><span id="L-1996"><a href="#L-1996"><span class="linenos">1996</span></a>
</span><span id="L-1997"><a href="#L-1997"><span class="linenos">1997</span></a>
</span><span id="L-1998"><a href="#L-1998"><span class="linenos">1998</span></a>    <span class="c1"># Multiply two sparse tensors that have the same nonzero pattern</span>
</span><span id="L-1999"><a href="#L-1999"><span class="linenos">1999</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">mul_same_pattern</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2000"><a href="#L-2000"><span class="linenos">2000</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2001"><a href="#L-2001"><span class="linenos">2001</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2002"><a href="#L-2002"><span class="linenos">2002</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">fac</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
</span><span id="L-2003"><a href="#L-2003"><span class="linenos">2003</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2004"><a href="#L-2004"><span class="linenos">2004</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-2005"><a href="#L-2005"><span class="linenos">2005</span></a>            
</span><span id="L-2006"><a href="#L-2006"><span class="linenos">2006</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2007"><a href="#L-2007"><span class="linenos">2007</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-2008"><a href="#L-2008"><span class="linenos">2008</span></a>
</span><span id="L-2009"><a href="#L-2009"><span class="linenos">2009</span></a>            <span class="k">if</span> <span class="n">fac</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
</span><span id="L-2010"><a href="#L-2010"><span class="linenos">2010</span></a>                <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">*</span><span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2011"><a href="#L-2011"><span class="linenos">2011</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2012"><a href="#L-2012"><span class="linenos">2012</span></a>                <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">fac</span><span class="o">*</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">*</span><span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2013"><a href="#L-2013"><span class="linenos">2013</span></a>
</span><span id="L-2014"><a href="#L-2014"><span class="linenos">2014</span></a>            <span class="n">grad_mult_A</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2015"><a href="#L-2015"><span class="linenos">2015</span></a>            <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2016"><a href="#L-2016"><span class="linenos">2016</span></a>
</span><span id="L-2017"><a href="#L-2017"><span class="linenos">2017</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2018"><a href="#L-2018"><span class="linenos">2018</span></a>                <span class="n">grad_mult_A</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">fac</span> <span class="o">==</span> <span class="mf">1.0</span> <span class="k">else</span> <span class="n">fac</span> <span class="o">*</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2019"><a href="#L-2019"><span class="linenos">2019</span></a>            
</span><span id="L-2020"><a href="#L-2020"><span class="linenos">2020</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2021"><a href="#L-2021"><span class="linenos">2021</span></a>                <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">fac</span> <span class="o">==</span> <span class="mf">1.0</span> <span class="k">else</span> <span class="n">fac</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2022"><a href="#L-2022"><span class="linenos">2022</span></a>
</span><span id="L-2023"><a href="#L-2023"><span class="linenos">2023</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;gradient of fac is currently not implemented&#39;</span>
</span><span id="L-2024"><a href="#L-2024"><span class="linenos">2024</span></a>
</span><span id="L-2025"><a href="#L-2025"><span class="linenos">2025</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2026"><a href="#L-2026"><span class="linenos">2026</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">grad_mult_A</span><span class="p">,</span> <span class="n">grad_mult_B</span><span class="p">)</span>
</span><span id="L-2027"><a href="#L-2027"><span class="linenos">2027</span></a>
</span><span id="L-2028"><a href="#L-2028"><span class="linenos">2028</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2029"><a href="#L-2029"><span class="linenos">2029</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2030"><a href="#L-2030"><span class="linenos">2030</span></a>
</span><span id="L-2031"><a href="#L-2031"><span class="linenos">2031</span></a>
</span><span id="L-2032"><a href="#L-2032"><span class="linenos">2032</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2033"><a href="#L-2033"><span class="linenos">2033</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2034"><a href="#L-2034"><span class="linenos">2034</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>        
</span><span id="L-2035"><a href="#L-2035"><span class="linenos">2035</span></a>            <span class="n">grad_mult_A</span><span class="p">,</span> <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span><span id="L-2036"><a href="#L-2036"><span class="linenos">2036</span></a>
</span><span id="L-2037"><a href="#L-2037"><span class="linenos">2037</span></a>            <span class="n">grad_A</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2038"><a href="#L-2038"><span class="linenos">2038</span></a>            <span class="n">grad_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2039"><a href="#L-2039"><span class="linenos">2039</span></a>
</span><span id="L-2040"><a href="#L-2040"><span class="linenos">2040</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2041"><a href="#L-2041"><span class="linenos">2041</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2042"><a href="#L-2042"><span class="linenos">2042</span></a>
</span><span id="L-2043"><a href="#L-2043"><span class="linenos">2043</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2044"><a href="#L-2044"><span class="linenos">2044</span></a>                <span class="n">grad_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">grad_mult_A</span><span class="o">*</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2045"><a href="#L-2045"><span class="linenos">2045</span></a>            
</span><span id="L-2046"><a href="#L-2046"><span class="linenos">2046</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2047"><a href="#L-2047"><span class="linenos">2047</span></a>                <span class="n">grad_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">grad_mult_B</span><span class="o">*</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2048"><a href="#L-2048"><span class="linenos">2048</span></a>
</span><span id="L-2049"><a href="#L-2049"><span class="linenos">2049</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_A</span><span class="p">)</span>
</span><span id="L-2050"><a href="#L-2050"><span class="linenos">2050</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_B</span><span class="p">)</span>
</span><span id="L-2051"><a href="#L-2051"><span class="linenos">2051</span></a>            <span class="k">return</span> <span class="n">grad_A</span><span class="p">,</span> <span class="n">grad_B</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2052"><a href="#L-2052"><span class="linenos">2052</span></a>
</span><span id="L-2053"><a href="#L-2053"><span class="linenos">2053</span></a>
</span><span id="L-2054"><a href="#L-2054"><span class="linenos">2054</span></a>
</span><span id="L-2055"><a href="#L-2055"><span class="linenos">2055</span></a>    <span class="c1"># Equivalent to: A*B, where A is sparse, B is dense, and the shape of B is broadcastable to A</span>
</span><span id="L-2056"><a href="#L-2056"><span class="linenos">2056</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">mul_sparse_dense</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2057"><a href="#L-2057"><span class="linenos">2057</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2058"><a href="#L-2058"><span class="linenos">2058</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2059"><a href="#L-2059"><span class="linenos">2059</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-2060"><a href="#L-2060"><span class="linenos">2060</span></a>
</span><span id="L-2061"><a href="#L-2061"><span class="linenos">2061</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A must be sparse&#39;</span>
</span><span id="L-2062"><a href="#L-2062"><span class="linenos">2062</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">B</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;B cannot be sparse&#39;</span>
</span><span id="L-2063"><a href="#L-2063"><span class="linenos">2063</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;B must be of size that allows broadcasting to A&quot;</span>
</span><span id="L-2064"><a href="#L-2064"><span class="linenos">2064</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-2065"><a href="#L-2065"><span class="linenos">2065</span></a>
</span><span id="L-2066"><a href="#L-2066"><span class="linenos">2066</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2067"><a href="#L-2067"><span class="linenos">2067</span></a>
</span><span id="L-2068"><a href="#L-2068"><span class="linenos">2068</span></a>            <span class="c1"># Dimensions along B needs to be broadcast to A</span>
</span><span id="L-2069"><a href="#L-2069"><span class="linenos">2069</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-2070"><a href="#L-2070"><span class="linenos">2070</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">*</span> <span class="n">B</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">())]</span>
</span><span id="L-2071"><a href="#L-2071"><span class="linenos">2071</span></a>
</span><span id="L-2072"><a href="#L-2072"><span class="linenos">2072</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2073"><a href="#L-2073"><span class="linenos">2073</span></a>
</span><span id="L-2074"><a href="#L-2074"><span class="linenos">2074</span></a>            <span class="n">A_save</span> <span class="o">=</span> <span class="n">A</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2075"><a href="#L-2075"><span class="linenos">2075</span></a>            <span class="n">B_save</span> <span class="o">=</span> <span class="n">B</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2076"><a href="#L-2076"><span class="linenos">2076</span></a>
</span><span id="L-2077"><a href="#L-2077"><span class="linenos">2077</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2078"><a href="#L-2078"><span class="linenos">2078</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">A_save</span><span class="p">,</span> <span class="n">B_save</span><span class="p">)</span>
</span><span id="L-2079"><a href="#L-2079"><span class="linenos">2079</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">broadcast_dims</span>
</span><span id="L-2080"><a href="#L-2080"><span class="linenos">2080</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span> <span class="o">=</span> <span class="n">slice_info</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2081"><a href="#L-2081"><span class="linenos">2081</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="L-2082"><a href="#L-2082"><span class="linenos">2082</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="L-2083"><a href="#L-2083"><span class="linenos">2083</span></a>
</span><span id="L-2084"><a href="#L-2084"><span class="linenos">2084</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2085"><a href="#L-2085"><span class="linenos">2085</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2086"><a href="#L-2086"><span class="linenos">2086</span></a>
</span><span id="L-2087"><a href="#L-2087"><span class="linenos">2087</span></a>        <span class="c1"># 1.65 seconds</span>
</span><span id="L-2088"><a href="#L-2088"><span class="linenos">2088</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2089"><a href="#L-2089"><span class="linenos">2089</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2090"><a href="#L-2090"><span class="linenos">2090</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>        
</span><span id="L-2091"><a href="#L-2091"><span class="linenos">2091</span></a>            <span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span><span id="L-2092"><a href="#L-2092"><span class="linenos">2092</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span>
</span><span id="L-2093"><a href="#L-2093"><span class="linenos">2093</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span>
</span><span id="L-2094"><a href="#L-2094"><span class="linenos">2094</span></a>
</span><span id="L-2095"><a href="#L-2095"><span class="linenos">2095</span></a>            <span class="n">out_A</span> <span class="o">=</span> <span class="n">out_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2096"><a href="#L-2096"><span class="linenos">2096</span></a>
</span><span id="L-2097"><a href="#L-2097"><span class="linenos">2097</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2098"><a href="#L-2098"><span class="linenos">2098</span></a>                <span class="c1"># The to_sparse() below is just to make sure that B, which is dense, doesn&#39;t get broadcast</span>
</span><span id="L-2099"><a href="#L-2099"><span class="linenos">2099</span></a>                <span class="c1"># to the size of grad_output; it is the same size of A, which can be huge.</span>
</span><span id="L-2100"><a href="#L-2100"><span class="linenos">2100</span></a>
</span><span id="L-2101"><a href="#L-2101"><span class="linenos">2101</span></a>                <span class="k">assert</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="L-2102"><a href="#L-2102"><span class="linenos">2102</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2103"><a href="#L-2103"><span class="linenos">2103</span></a>
</span><span id="L-2104"><a href="#L-2104"><span class="linenos">2104</span></a>                <span class="n">inds</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>            
</span><span id="L-2105"><a href="#L-2105"><span class="linenos">2105</span></a>                <span class="n">vals</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">*</span> <span class="n">B</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">inds</span><span class="p">)]</span>
</span><span id="L-2106"><a href="#L-2106"><span class="linenos">2106</span></a>                <span class="n">out_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                
</span><span id="L-2107"><a href="#L-2107"><span class="linenos">2107</span></a>
</span><span id="L-2108"><a href="#L-2108"><span class="linenos">2108</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2109"><a href="#L-2109"><span class="linenos">2109</span></a>                <span class="c1"># B is dense, so the gradient with respect to B should also be dense.</span>
</span><span id="L-2110"><a href="#L-2110"><span class="linenos">2110</span></a>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">broadcast_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-2111"><a href="#L-2111"><span class="linenos">2111</span></a>                    <span class="c1"># 0.04 seconds</span>
</span><span id="L-2112"><a href="#L-2112"><span class="linenos">2112</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">same_shape_prod</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2113"><a href="#L-2113"><span class="linenos">2113</span></a>                        
</span><span id="L-2114"><a href="#L-2114"><span class="linenos">2114</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sum_sparse</span><span class="p">(</span><span class="n">out_B</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">broadcast_dims</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span>
</span><span id="L-2115"><a href="#L-2115"><span class="linenos">2115</span></a>                                          <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-2116"><a href="#L-2116"><span class="linenos">2116</span></a>                                          <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-2117"><a href="#L-2117"><span class="linenos">2117</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-2118"><a href="#L-2118"><span class="linenos">2118</span></a>                    <span class="c1"># 0 seconds</span>
</span><span id="L-2119"><a href="#L-2119"><span class="linenos">2119</span></a>                    <span class="c1"># In this case, B didn&#39;t need to be broadcast to A, so all of A,B,grad_output have the same shape and are not huge</span>
</span><span id="L-2120"><a href="#L-2120"><span class="linenos">2120</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">same_shape_prod</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-2121"><a href="#L-2121"><span class="linenos">2121</span></a>
</span><span id="L-2122"><a href="#L-2122"><span class="linenos">2122</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out_A</span><span class="p">)</span>
</span><span id="L-2123"><a href="#L-2123"><span class="linenos">2123</span></a>            <span class="k">return</span> <span class="n">out_A</span><span class="p">,</span> <span class="n">out_B</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2124"><a href="#L-2124"><span class="linenos">2124</span></a>
</span><span id="L-2125"><a href="#L-2125"><span class="linenos">2125</span></a>
</span><span id="L-2126"><a href="#L-2126"><span class="linenos">2126</span></a>
</span><span id="L-2127"><a href="#L-2127"><span class="linenos">2127</span></a>    <span class="c1"># Equivalent to: A/B, where A is sparse, B is dense, and the shape of B is broadcastable to A</span>
</span><span id="L-2128"><a href="#L-2128"><span class="linenos">2128</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">div_sparse_dense</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2129"><a href="#L-2129"><span class="linenos">2129</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2130"><a href="#L-2130"><span class="linenos">2130</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2131"><a href="#L-2131"><span class="linenos">2131</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span> <span class="c1"># 0.06 seconds</span>
</span><span id="L-2132"><a href="#L-2132"><span class="linenos">2132</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A must be sparse&#39;</span>
</span><span id="L-2133"><a href="#L-2133"><span class="linenos">2133</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">B</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;B cannot be sparse&#39;</span>
</span><span id="L-2134"><a href="#L-2134"><span class="linenos">2134</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;B must be of size that allows broadcasting to A&quot;</span>
</span><span id="L-2135"><a href="#L-2135"><span class="linenos">2135</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-2136"><a href="#L-2136"><span class="linenos">2136</span></a>
</span><span id="L-2137"><a href="#L-2137"><span class="linenos">2137</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2138"><a href="#L-2138"><span class="linenos">2138</span></a>
</span><span id="L-2139"><a href="#L-2139"><span class="linenos">2139</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2140"><a href="#L-2140"><span class="linenos">2140</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">B</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;B cannot contain zeros&#39;</span>
</span><span id="L-2141"><a href="#L-2141"><span class="linenos">2141</span></a>
</span><span id="L-2142"><a href="#L-2142"><span class="linenos">2142</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span> <span class="c1"># tuple(torch.nonzero(torch.tensor(B.shape) == 1))</span>
</span><span id="L-2143"><a href="#L-2143"><span class="linenos">2143</span></a>
</span><span id="L-2144"><a href="#L-2144"><span class="linenos">2144</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">/</span> <span class="n">B</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">())]</span>
</span><span id="L-2145"><a href="#L-2145"><span class="linenos">2145</span></a>
</span><span id="L-2146"><a href="#L-2146"><span class="linenos">2146</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2147"><a href="#L-2147"><span class="linenos">2147</span></a>
</span><span id="L-2148"><a href="#L-2148"><span class="linenos">2148</span></a>            <span class="n">A_save</span> <span class="o">=</span> <span class="n">A</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2149"><a href="#L-2149"><span class="linenos">2149</span></a>            <span class="n">B_save</span> <span class="o">=</span> <span class="n">B</span> <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2150"><a href="#L-2150"><span class="linenos">2150</span></a>
</span><span id="L-2151"><a href="#L-2151"><span class="linenos">2151</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2152"><a href="#L-2152"><span class="linenos">2152</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">A_save</span><span class="p">,</span> <span class="n">B_save</span><span class="p">)</span>
</span><span id="L-2153"><a href="#L-2153"><span class="linenos">2153</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">broadcast_dims</span>
</span><span id="L-2154"><a href="#L-2154"><span class="linenos">2154</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span> <span class="o">=</span> <span class="n">slice_info</span>
</span><span id="L-2155"><a href="#L-2155"><span class="linenos">2155</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="L-2156"><a href="#L-2156"><span class="linenos">2156</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="L-2157"><a href="#L-2157"><span class="linenos">2157</span></a>
</span><span id="L-2158"><a href="#L-2158"><span class="linenos">2158</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2159"><a href="#L-2159"><span class="linenos">2159</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2160"><a href="#L-2160"><span class="linenos">2160</span></a>
</span><span id="L-2161"><a href="#L-2161"><span class="linenos">2161</span></a>
</span><span id="L-2162"><a href="#L-2162"><span class="linenos">2162</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2163"><a href="#L-2163"><span class="linenos">2163</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2164"><a href="#L-2164"><span class="linenos">2164</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span> <span class="c1"># 0 seconds</span>
</span><span id="L-2165"><a href="#L-2165"><span class="linenos">2165</span></a>            <span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span><span id="L-2166"><a href="#L-2166"><span class="linenos">2166</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span>
</span><span id="L-2167"><a href="#L-2167"><span class="linenos">2167</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span>
</span><span id="L-2168"><a href="#L-2168"><span class="linenos">2168</span></a>
</span><span id="L-2169"><a href="#L-2169"><span class="linenos">2169</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2170"><a href="#L-2170"><span class="linenos">2170</span></a>
</span><span id="L-2171"><a href="#L-2171"><span class="linenos">2171</span></a>            <span class="n">out_A</span> <span class="o">=</span> <span class="n">out_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2172"><a href="#L-2172"><span class="linenos">2172</span></a>        
</span><span id="L-2173"><a href="#L-2173"><span class="linenos">2173</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>                
</span><span id="L-2174"><a href="#L-2174"><span class="linenos">2174</span></a>                <span class="n">out_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">div_sparse_dense</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</span><span id="L-2175"><a href="#L-2175"><span class="linenos">2175</span></a>            
</span><span id="L-2176"><a href="#L-2176"><span class="linenos">2176</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2177"><a href="#L-2177"><span class="linenos">2177</span></a>                <span class="c1"># B is dense, so the gradient with respect to B should also be dense.</span>
</span><span id="L-2178"><a href="#L-2178"><span class="linenos">2178</span></a>                <span class="c1"># both cases below take 0 seconds</span>
</span><span id="L-2179"><a href="#L-2179"><span class="linenos">2179</span></a>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">broadcast_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-2180"><a href="#L-2180"><span class="linenos">2180</span></a>                    <span class="c1"># 0 seconds</span>
</span><span id="L-2181"><a href="#L-2181"><span class="linenos">2181</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">same_shape_prod</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="c1"># This is still sparse and can be huge</span>
</span><span id="L-2182"><a href="#L-2182"><span class="linenos">2182</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sum_sparse</span><span class="p">(</span><span class="n">out_B</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">broadcast_dims</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span>
</span><span id="L-2183"><a href="#L-2183"><span class="linenos">2183</span></a>                                          <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-2184"><a href="#L-2184"><span class="linenos">2184</span></a>                                          <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-2185"><a href="#L-2185"><span class="linenos">2185</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="o">-</span><span class="n">out_B</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-2186"><a href="#L-2186"><span class="linenos">2186</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-2187"><a href="#L-2187"><span class="linenos">2187</span></a>                    <span class="c1"># In this case, B didn&#39;t need to be broadcast to A, so all of A,B,grad_output have the same shape and are not huge</span>
</span><span id="L-2188"><a href="#L-2188"><span class="linenos">2188</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">same_shape_prod</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">B</span><span class="p">))</span>
</span><span id="L-2189"><a href="#L-2189"><span class="linenos">2189</span></a>                    <span class="k">assert</span> <span class="n">out_B</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-2190"><a href="#L-2190"><span class="linenos">2190</span></a>                    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Did not test this case. Make sure this works correctly&quot;</span>
</span><span id="L-2191"><a href="#L-2191"><span class="linenos">2191</span></a>
</span><span id="L-2192"><a href="#L-2192"><span class="linenos">2192</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out_A</span><span class="p">)</span>
</span><span id="L-2193"><a href="#L-2193"><span class="linenos">2193</span></a>            <span class="k">return</span> <span class="n">out_A</span><span class="p">,</span> <span class="n">out_B</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2194"><a href="#L-2194"><span class="linenos">2194</span></a>
</span><span id="L-2195"><a href="#L-2195"><span class="linenos">2195</span></a>
</span><span id="L-2196"><a href="#L-2196"><span class="linenos">2196</span></a>
</span><span id="L-2197"><a href="#L-2197"><span class="linenos">2197</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">add_sparse_dense</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2198"><a href="#L-2198"><span class="linenos">2198</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2199"><a href="#L-2199"><span class="linenos">2199</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2200"><a href="#L-2200"><span class="linenos">2200</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-2201"><a href="#L-2201"><span class="linenos">2201</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A must be sparse&#39;</span>
</span><span id="L-2202"><a href="#L-2202"><span class="linenos">2202</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">B</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;B cannot be sparse&#39;</span>
</span><span id="L-2203"><a href="#L-2203"><span class="linenos">2203</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;B must be of size that allows broadcasting to A&quot;</span>
</span><span id="L-2204"><a href="#L-2204"><span class="linenos">2204</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-2205"><a href="#L-2205"><span class="linenos">2205</span></a>
</span><span id="L-2206"><a href="#L-2206"><span class="linenos">2206</span></a>            <span class="c1"># Calculate broadcast dimensions</span>
</span><span id="L-2207"><a href="#L-2207"><span class="linenos">2207</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</span><span id="L-2208"><a href="#L-2208"><span class="linenos">2208</span></a>
</span><span id="L-2209"><a href="#L-2209"><span class="linenos">2209</span></a>            <span class="c1"># Perform entrywise addition</span>
</span><span id="L-2210"><a href="#L-2210"><span class="linenos">2210</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">+</span> <span class="n">B</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">())]</span>
</span><span id="L-2211"><a href="#L-2211"><span class="linenos">2211</span></a>
</span><span id="L-2212"><a href="#L-2212"><span class="linenos">2212</span></a>            <span class="c1"># Create the output sparse tensor with the same indices as A</span>
</span><span id="L-2213"><a href="#L-2213"><span class="linenos">2213</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2214"><a href="#L-2214"><span class="linenos">2214</span></a>
</span><span id="L-2215"><a href="#L-2215"><span class="linenos">2215</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2216"><a href="#L-2216"><span class="linenos">2216</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">broadcast_dims</span>
</span><span id="L-2217"><a href="#L-2217"><span class="linenos">2217</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span> <span class="o">=</span> <span class="n">slice_info</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-2218"><a href="#L-2218"><span class="linenos">2218</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="L-2219"><a href="#L-2219"><span class="linenos">2219</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="L-2220"><a href="#L-2220"><span class="linenos">2220</span></a>
</span><span id="L-2221"><a href="#L-2221"><span class="linenos">2221</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2222"><a href="#L-2222"><span class="linenos">2222</span></a>
</span><span id="L-2223"><a href="#L-2223"><span class="linenos">2223</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2224"><a href="#L-2224"><span class="linenos">2224</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2225"><a href="#L-2225"><span class="linenos">2225</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
</span><span id="L-2226"><a href="#L-2226"><span class="linenos">2226</span></a>            <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">broadcast_dims</span>
</span><span id="L-2227"><a href="#L-2227"><span class="linenos">2227</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span>
</span><span id="L-2228"><a href="#L-2228"><span class="linenos">2228</span></a>
</span><span id="L-2229"><a href="#L-2229"><span class="linenos">2229</span></a>            <span class="n">out_A</span> <span class="o">=</span> <span class="n">out_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2230"><a href="#L-2230"><span class="linenos">2230</span></a>
</span><span id="L-2231"><a href="#L-2231"><span class="linenos">2231</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2232"><a href="#L-2232"><span class="linenos">2232</span></a>                <span class="c1"># Gradient with respect to the sparse tensor A is simply the grad_output</span>
</span><span id="L-2233"><a href="#L-2233"><span class="linenos">2233</span></a>                <span class="n">out_A</span> <span class="o">=</span> <span class="n">grad_output</span>
</span><span id="L-2234"><a href="#L-2234"><span class="linenos">2234</span></a>            
</span><span id="L-2235"><a href="#L-2235"><span class="linenos">2235</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2236"><a href="#L-2236"><span class="linenos">2236</span></a>                <span class="c1"># Gradient with respect to the dense tensor B</span>
</span><span id="L-2237"><a href="#L-2237"><span class="linenos">2237</span></a>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">broadcast_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-2238"><a href="#L-2238"><span class="linenos">2238</span></a>                    <span class="c1"># If broadcasting happened, we need to sum over the broadcast dimensions</span>
</span><span id="L-2239"><a href="#L-2239"><span class="linenos">2239</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sum_sparse</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">broadcast_dims</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span>
</span><span id="L-2240"><a href="#L-2240"><span class="linenos">2240</span></a>                                          <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-2241"><a href="#L-2241"><span class="linenos">2241</span></a>                                          <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-2242"><a href="#L-2242"><span class="linenos">2242</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-2243"><a href="#L-2243"><span class="linenos">2243</span></a>                    <span class="c1"># No broadcasting needed, so just convert to dense</span>
</span><span id="L-2244"><a href="#L-2244"><span class="linenos">2244</span></a>                    <span class="n">out_B</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</span><span id="L-2245"><a href="#L-2245"><span class="linenos">2245</span></a>
</span><span id="L-2246"><a href="#L-2246"><span class="linenos">2246</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out_A</span><span class="p">)</span>
</span><span id="L-2247"><a href="#L-2247"><span class="linenos">2247</span></a>            <span class="k">return</span> <span class="n">out_A</span><span class="p">,</span> <span class="n">out_B</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2248"><a href="#L-2248"><span class="linenos">2248</span></a>
</span><span id="L-2249"><a href="#L-2249"><span class="linenos">2249</span></a>
</span><span id="L-2250"><a href="#L-2250"><span class="linenos">2250</span></a>    <span class="c1"># Calculates the function f(x) = max(x, thresh), but with grad f(thresh) = 1</span>
</span><span id="L-2251"><a href="#L-2251"><span class="linenos">2251</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">custom_lowclamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2252"><a href="#L-2252"><span class="linenos">2252</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2253"><a href="#L-2253"><span class="linenos">2253</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2254"><a href="#L-2254"><span class="linenos">2254</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">thresh</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
</span><span id="L-2255"><a href="#L-2255"><span class="linenos">2255</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">&gt;=</span> <span class="n">thresh</span><span class="p">)</span>
</span><span id="L-2256"><a href="#L-2256"><span class="linenos">2256</span></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">thresh</span><span class="p">)</span>
</span><span id="L-2257"><a href="#L-2257"><span class="linenos">2257</span></a>
</span><span id="L-2258"><a href="#L-2258"><span class="linenos">2258</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2259"><a href="#L-2259"><span class="linenos">2259</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2260"><a href="#L-2260"><span class="linenos">2260</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2261"><a href="#L-2261"><span class="linenos">2261</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">active</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad_output</span><span class="p">))</span>
</span><span id="L-2262"><a href="#L-2262"><span class="linenos">2262</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2263"><a href="#L-2263"><span class="linenos">2263</span></a>
</span><span id="L-2264"><a href="#L-2264"><span class="linenos">2264</span></a>
</span><span id="L-2265"><a href="#L-2265"><span class="linenos">2265</span></a>    <span class="c1"># Equivalent to torch.sinc(A) * torch.cos(B), where A and B are spase tensors of the same shape and nonzero pattern.</span>
</span><span id="L-2266"><a href="#L-2266"><span class="linenos">2266</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">sinc_cos_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2267"><a href="#L-2267"><span class="linenos">2267</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2268"><a href="#L-2268"><span class="linenos">2268</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2269"><a href="#L-2269"><span class="linenos">2269</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2270"><a href="#L-2270"><span class="linenos">2270</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">and</span> <span class="n">B</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="L-2271"><a href="#L-2271"><span class="linenos">2271</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;A and B must have the same shape&#39;</span>
</span><span id="L-2272"><a href="#L-2272"><span class="linenos">2272</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-2273"><a href="#L-2273"><span class="linenos">2273</span></a>            <span class="k">assert</span> <span class="n">B</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;B must be coalesced&#39;</span>
</span><span id="L-2274"><a href="#L-2274"><span class="linenos">2274</span></a>
</span><span id="L-2275"><a href="#L-2275"><span class="linenos">2275</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2276"><a href="#L-2276"><span class="linenos">2276</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</span><span id="L-2277"><a href="#L-2277"><span class="linenos">2277</span></a>
</span><span id="L-2278"><a href="#L-2278"><span class="linenos">2278</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2279"><a href="#L-2279"><span class="linenos">2279</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;A and B nonzero indices do not match&#39;</span>
</span><span id="L-2280"><a href="#L-2280"><span class="linenos">2280</span></a>
</span><span id="L-2281"><a href="#L-2281"><span class="linenos">2281</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2282"><a href="#L-2282"><span class="linenos">2282</span></a>
</span><span id="L-2283"><a href="#L-2283"><span class="linenos">2283</span></a>            <span class="n">A_vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2284"><a href="#L-2284"><span class="linenos">2284</span></a>            <span class="n">B_vals</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2285"><a href="#L-2285"><span class="linenos">2285</span></a>
</span><span id="L-2286"><a href="#L-2286"><span class="linenos">2286</span></a>            <span class="n">B_cos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">B_vals</span><span class="p">)</span>
</span><span id="L-2287"><a href="#L-2287"><span class="linenos">2287</span></a>
</span><span id="L-2288"><a href="#L-2288"><span class="linenos">2288</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2289"><a href="#L-2289"><span class="linenos">2289</span></a>                <span class="n">A_dsinc</span><span class="p">,</span> <span class="n">A_sinc</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">dsinc</span><span class="p">(</span><span class="n">A_vals</span><span class="p">,</span> <span class="n">return_sinc</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-2290"><a href="#L-2290"><span class="linenos">2290</span></a>                <span class="n">grad_mult_A</span> <span class="o">=</span> <span class="n">A_dsinc</span><span class="o">*</span><span class="n">B_cos</span>
</span><span id="L-2291"><a href="#L-2291"><span class="linenos">2291</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2292"><a href="#L-2292"><span class="linenos">2292</span></a>                <span class="n">A_sinc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinc</span><span class="p">(</span><span class="n">A_vals</span><span class="p">)</span>
</span><span id="L-2293"><a href="#L-2293"><span class="linenos">2293</span></a>                <span class="c1">#A_dsinc = None</span>
</span><span id="L-2294"><a href="#L-2294"><span class="linenos">2294</span></a>                <span class="n">grad_mult_A</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2295"><a href="#L-2295"><span class="linenos">2295</span></a>
</span><span id="L-2296"><a href="#L-2296"><span class="linenos">2296</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2297"><a href="#L-2297"><span class="linenos">2297</span></a>                <span class="n">B_sin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">B_vals</span><span class="p">)</span>
</span><span id="L-2298"><a href="#L-2298"><span class="linenos">2298</span></a>                <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="n">A_sinc</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">B_sin</span><span class="p">)</span>
</span><span id="L-2299"><a href="#L-2299"><span class="linenos">2299</span></a>                <span class="k">del</span> <span class="n">B_sin</span>
</span><span id="L-2300"><a href="#L-2300"><span class="linenos">2300</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2301"><a href="#L-2301"><span class="linenos">2301</span></a>                <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2302"><a href="#L-2302"><span class="linenos">2302</span></a>
</span><span id="L-2303"><a href="#L-2303"><span class="linenos">2303</span></a>            <span class="n">out_vals</span> <span class="o">=</span> <span class="n">A_sinc</span><span class="o">*</span><span class="n">B_cos</span>
</span><span id="L-2304"><a href="#L-2304"><span class="linenos">2304</span></a>
</span><span id="L-2305"><a href="#L-2305"><span class="linenos">2305</span></a>            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">:</span>
</span><span id="L-2306"><a href="#L-2306"><span class="linenos">2306</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">grad_mult_A</span><span class="p">,</span> <span class="n">grad_mult_B</span><span class="p">)</span>
</span><span id="L-2307"><a href="#L-2307"><span class="linenos">2307</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2308"><a href="#L-2308"><span class="linenos">2308</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">inds</span> <span class="o">=</span> <span class="n">inds</span>
</span><span id="L-2309"><a href="#L-2309"><span class="linenos">2309</span></a>
</span><span id="L-2310"><a href="#L-2310"><span class="linenos">2310</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">out_vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2311"><a href="#L-2311"><span class="linenos">2311</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2312"><a href="#L-2312"><span class="linenos">2312</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2313"><a href="#L-2313"><span class="linenos">2313</span></a>
</span><span id="L-2314"><a href="#L-2314"><span class="linenos">2314</span></a>
</span><span id="L-2315"><a href="#L-2315"><span class="linenos">2315</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2316"><a href="#L-2316"><span class="linenos">2316</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2317"><a href="#L-2317"><span class="linenos">2317</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
</span><span id="L-2318"><a href="#L-2318"><span class="linenos">2318</span></a>            <span class="n">grad_mult_A</span><span class="p">,</span> <span class="n">grad_mult_B</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span><span id="L-2319"><a href="#L-2319"><span class="linenos">2319</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2320"><a href="#L-2320"><span class="linenos">2320</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">inds</span>
</span><span id="L-2321"><a href="#L-2321"><span class="linenos">2321</span></a>
</span><span id="L-2322"><a href="#L-2322"><span class="linenos">2322</span></a>            <span class="n">grad_A</span> <span class="o">=</span> <span class="n">grad_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2323"><a href="#L-2323"><span class="linenos">2323</span></a>
</span><span id="L-2324"><a href="#L-2324"><span class="linenos">2324</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2325"><a href="#L-2325"><span class="linenos">2325</span></a>                <span class="n">grad_vals_A</span> <span class="o">=</span> <span class="n">grad_mult_A</span> <span class="o">*</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2326"><a href="#L-2326"><span class="linenos">2326</span></a>                <span class="n">grad_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">grad_vals_A</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2327"><a href="#L-2327"><span class="linenos">2327</span></a>                <span class="k">del</span> <span class="n">grad_vals_A</span>
</span><span id="L-2328"><a href="#L-2328"><span class="linenos">2328</span></a>
</span><span id="L-2329"><a href="#L-2329"><span class="linenos">2329</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2330"><a href="#L-2330"><span class="linenos">2330</span></a>                <span class="n">grad_vals_B</span> <span class="o">=</span> <span class="n">grad_mult_B</span> <span class="o">*</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2331"><a href="#L-2331"><span class="linenos">2331</span></a>                <span class="n">grad_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">grad_vals_B</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2332"><a href="#L-2332"><span class="linenos">2332</span></a>                <span class="k">del</span> <span class="n">grad_vals_B</span>
</span><span id="L-2333"><a href="#L-2333"><span class="linenos">2333</span></a>
</span><span id="L-2334"><a href="#L-2334"><span class="linenos">2334</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_A</span><span class="p">)</span>
</span><span id="L-2335"><a href="#L-2335"><span class="linenos">2335</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_B</span><span class="p">)</span>
</span><span id="L-2336"><a href="#L-2336"><span class="linenos">2336</span></a>            <span class="k">return</span> <span class="n">grad_A</span><span class="p">,</span> <span class="n">grad_B</span>
</span><span id="L-2337"><a href="#L-2337"><span class="linenos">2337</span></a>
</span><span id="L-2338"><a href="#L-2338"><span class="linenos">2338</span></a>
</span><span id="L-2339"><a href="#L-2339"><span class="linenos">2339</span></a>    <span class="c1"># Concatenates two sparse tensors along their last sparse dimension, similarly to torch.cat((A,B), dim=A.sparse_dim()-1)</span>
</span><span id="L-2340"><a href="#L-2340"><span class="linenos">2340</span></a>    <span class="c1"># Supports tensors with dense_dim &gt; 0. The two tensors should have the same dense_dim() and have the same shape along all dimensions</span>
</span><span id="L-2341"><a href="#L-2341"><span class="linenos">2341</span></a>    <span class="c1"># except for sparse_dim()-1</span>
</span><span id="L-2342"><a href="#L-2342"><span class="linenos">2342</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">concat_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2343"><a href="#L-2343"><span class="linenos">2343</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2344"><a href="#L-2344"><span class="linenos">2344</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2345"><a href="#L-2345"><span class="linenos">2345</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2346"><a href="#L-2346"><span class="linenos">2346</span></a>            <span class="c1"># Ensure both A and B are sparse and coalesced</span>
</span><span id="L-2347"><a href="#L-2347"><span class="linenos">2347</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-2348"><a href="#L-2348"><span class="linenos">2348</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="L-2349"><a href="#L-2349"><span class="linenos">2349</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="ow">and</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()),</span> <span class="s2">&quot;A and B must have the same shape, except for their last sparse dimension, which should be identical&quot;</span>
</span><span id="L-2350"><a href="#L-2350"><span class="linenos">2350</span></a>            <span class="n">sA</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-2351"><a href="#L-2351"><span class="linenos">2351</span></a>            <span class="n">sB</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-2352"><a href="#L-2352"><span class="linenos">2352</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">sA</span> <span class="o">==</span> <span class="n">sB</span><span class="p">),</span> <span class="s2">&quot;A and B must have the same shape, except for their last sparse dimension, which should be identical&quot;</span>
</span><span id="L-2353"><a href="#L-2353"><span class="linenos">2353</span></a>
</span><span id="L-2354"><a href="#L-2354"><span class="linenos">2354</span></a>            <span class="n">sparse_dim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span>
</span><span id="L-2355"><a href="#L-2355"><span class="linenos">2355</span></a>            <span class="n">dense_dim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span>
</span><span id="L-2356"><a href="#L-2356"><span class="linenos">2356</span></a>            <span class="n">A_nvals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2357"><a href="#L-2357"><span class="linenos">2357</span></a>            <span class="n">B_nvals</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2358"><a href="#L-2358"><span class="linenos">2358</span></a>
</span><span id="L-2359"><a href="#L-2359"><span class="linenos">2359</span></a>            <span class="c1"># This is identical for A and B</span>
</span><span id="L-2360"><a href="#L-2360"><span class="linenos">2360</span></a>            <span class="n">dense_shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">sparse_dim</span><span class="p">:]</span>
</span><span id="L-2361"><a href="#L-2361"><span class="linenos">2361</span></a>
</span><span id="L-2362"><a href="#L-2362"><span class="linenos">2362</span></a>            <span class="c1"># Calculate the offset for indices of B</span>
</span><span id="L-2363"><a href="#L-2363"><span class="linenos">2363</span></a>            <span class="n">offset</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">sparse_dim</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2364"><a href="#L-2364"><span class="linenos">2364</span></a>
</span><span id="L-2365"><a href="#L-2365"><span class="linenos">2365</span></a>            <span class="c1"># Concatenate indices and values</span>
</span><span id="L-2366"><a href="#L-2366"><span class="linenos">2366</span></a>            <span class="n">use_variant_one</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-2367"><a href="#L-2367"><span class="linenos">2367</span></a>            <span class="k">if</span> <span class="n">use_variant_one</span><span class="p">:</span>
</span><span id="L-2368"><a href="#L-2368"><span class="linenos">2368</span></a>                <span class="c1"># More efficient variant</span>
</span><span id="L-2369"><a href="#L-2369"><span class="linenos">2369</span></a>                <span class="n">combined_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">sparse_dim</span><span class="p">,</span> <span class="n">A_nvals</span><span class="o">+</span><span class="n">B_nvals</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-2370"><a href="#L-2370"><span class="linenos">2370</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2371"><a href="#L-2371"><span class="linenos">2371</span></a>                <span class="n">combined_indices</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">A_nvals</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2372"><a href="#L-2372"><span class="linenos">2372</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2373"><a href="#L-2373"><span class="linenos">2373</span></a>                <span class="n">combined_indices</span><span class="p">[:,</span> <span class="n">A_nvals</span><span class="p">:]</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2374"><a href="#L-2374"><span class="linenos">2374</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2375"><a href="#L-2375"><span class="linenos">2375</span></a>                <span class="n">combined_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">A_nvals</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">offset</span>
</span><span id="L-2376"><a href="#L-2376"><span class="linenos">2376</span></a>
</span><span id="L-2377"><a href="#L-2377"><span class="linenos">2377</span></a>                <span class="n">combined_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span> <span class="p">(</span><span class="n">A_nvals</span><span class="o">+</span><span class="n">B_nvals</span><span class="p">,)</span> <span class="o">+</span> <span class="n">dense_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-2378"><a href="#L-2378"><span class="linenos">2378</span></a>                <span class="n">combined_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">A_nvals</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2379"><a href="#L-2379"><span class="linenos">2379</span></a>                <span class="n">combined_values</span><span class="p">[</span><span class="n">A_nvals</span><span class="p">:,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2380"><a href="#L-2380"><span class="linenos">2380</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2381"><a href="#L-2381"><span class="linenos">2381</span></a>                <span class="c1"># Clearer variant</span>
</span><span id="L-2382"><a href="#L-2382"><span class="linenos">2382</span></a>                <span class="n">new_B_indices</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-2383"><a href="#L-2383"><span class="linenos">2383</span></a>                <span class="n">new_B_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">offset</span>
</span><span id="L-2384"><a href="#L-2384"><span class="linenos">2384</span></a>                <span class="n">combined_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">new_B_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2385"><a href="#L-2385"><span class="linenos">2385</span></a>                <span class="n">combined_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2386"><a href="#L-2386"><span class="linenos">2386</span></a>
</span><span id="L-2387"><a href="#L-2387"><span class="linenos">2387</span></a>            <span class="c1"># Get the size of the resulting sparse tensor</span>
</span><span id="L-2388"><a href="#L-2388"><span class="linenos">2388</span></a>            <span class="n">combined_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2389"><a href="#L-2389"><span class="linenos">2389</span></a>            <span class="n">combined_size</span><span class="p">[</span><span class="n">sparse_dim</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">sparse_dim</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2390"><a href="#L-2390"><span class="linenos">2390</span></a>
</span><span id="L-2391"><a href="#L-2391"><span class="linenos">2391</span></a>            <span class="c1"># Create the output sparse tensor and coalesce it</span>
</span><span id="L-2392"><a href="#L-2392"><span class="linenos">2392</span></a>            <span class="n">combined_indices</span><span class="p">,</span> <span class="n">combined_values</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sort_inds_vals</span><span class="p">(</span><span class="n">combined_indices</span><span class="p">,</span> <span class="n">combined_values</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span> 
</span><span id="L-2393"><a href="#L-2393"><span class="linenos">2393</span></a>            <span class="n">C</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">combined_indices</span><span class="p">,</span> <span class="n">combined_values</span><span class="p">,</span> <span class="n">combined_size</span><span class="p">)</span>
</span><span id="L-2394"><a href="#L-2394"><span class="linenos">2394</span></a>
</span><span id="L-2395"><a href="#L-2395"><span class="linenos">2395</span></a>            <span class="c1"># Save information for backward pass</span>
</span><span id="L-2396"><a href="#L-2396"><span class="linenos">2396</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">sparse_dim</span> <span class="o">=</span> <span class="n">sparse_dim</span>
</span><span id="L-2397"><a href="#L-2397"><span class="linenos">2397</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">dense_dim</span> <span class="o">=</span> <span class="n">dense_dim</span>
</span><span id="L-2398"><a href="#L-2398"><span class="linenos">2398</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">A_shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2399"><a href="#L-2399"><span class="linenos">2399</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">B_shape</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2400"><a href="#L-2400"><span class="linenos">2400</span></a>
</span><span id="L-2401"><a href="#L-2401"><span class="linenos">2401</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="n">dense_dim</span><span class="p">)</span>
</span><span id="L-2402"><a href="#L-2402"><span class="linenos">2402</span></a>            <span class="k">return</span> <span class="n">C</span>
</span><span id="L-2403"><a href="#L-2403"><span class="linenos">2403</span></a>
</span><span id="L-2404"><a href="#L-2404"><span class="linenos">2404</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2405"><a href="#L-2405"><span class="linenos">2405</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2406"><a href="#L-2406"><span class="linenos">2406</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2407"><a href="#L-2407"><span class="linenos">2407</span></a>            <span class="n">sparse_dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">sparse_dim</span>
</span><span id="L-2408"><a href="#L-2408"><span class="linenos">2408</span></a>            <span class="n">dense_dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dense_dim</span>
</span><span id="L-2409"><a href="#L-2409"><span class="linenos">2409</span></a>            <span class="n">A_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">A_shape</span>
</span><span id="L-2410"><a href="#L-2410"><span class="linenos">2410</span></a>            <span class="n">B_shape</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">B_shape</span>
</span><span id="L-2411"><a href="#L-2411"><span class="linenos">2411</span></a>
</span><span id="L-2412"><a href="#L-2412"><span class="linenos">2412</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="kc">True</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">):</span>
</span><span id="L-2413"><a href="#L-2413"><span class="linenos">2413</span></a>                <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2414"><a href="#L-2414"><span class="linenos">2414</span></a>
</span><span id="L-2415"><a href="#L-2415"><span class="linenos">2415</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="n">dense_dim</span><span class="p">),</span> <span class="s1">&#39;If for some reason we get an uncoalesced grad_output here, we should check why and if justified, coalesce it.&#39;</span>
</span><span id="L-2416"><a href="#L-2416"><span class="linenos">2416</span></a>
</span><span id="L-2417"><a href="#L-2417"><span class="linenos">2417</span></a>            <span class="c1"># Extracting the indices and values from grad_output</span>
</span><span id="L-2418"><a href="#L-2418"><span class="linenos">2418</span></a>            <span class="n">grad_indices</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2419"><a href="#L-2419"><span class="linenos">2419</span></a>            <span class="n">grad_values</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2420"><a href="#L-2420"><span class="linenos">2420</span></a>
</span><span id="L-2421"><a href="#L-2421"><span class="linenos">2421</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2422"><a href="#L-2422"><span class="linenos">2422</span></a>                <span class="n">A_mask</span> <span class="o">=</span> <span class="n">grad_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">A_shape</span><span class="p">[</span><span class="n">sparse_dim</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2423"><a href="#L-2423"><span class="linenos">2423</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2424"><a href="#L-2424"><span class="linenos">2424</span></a>                <span class="n">grad_A_indices</span> <span class="o">=</span> <span class="n">grad_indices</span><span class="p">[:,</span> <span class="n">A_mask</span><span class="p">]</span>
</span><span id="L-2425"><a href="#L-2425"><span class="linenos">2425</span></a>                <span class="n">grad_A_values</span> <span class="o">=</span> <span class="n">grad_values</span><span class="p">[</span><span class="n">A_mask</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="L-2426"><a href="#L-2426"><span class="linenos">2426</span></a>                <span class="n">grad_A</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">grad_A_indices</span><span class="p">,</span> <span class="n">grad_A_values</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A_shape</span><span class="p">)</span>
</span><span id="L-2427"><a href="#L-2427"><span class="linenos">2427</span></a>                <span class="k">del</span> <span class="n">A_mask</span><span class="p">,</span> <span class="n">grad_A_indices</span><span class="p">,</span> <span class="n">grad_A_values</span>
</span><span id="L-2428"><a href="#L-2428"><span class="linenos">2428</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2429"><a href="#L-2429"><span class="linenos">2429</span></a>                <span class="n">grad_A</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2430"><a href="#L-2430"><span class="linenos">2430</span></a>
</span><span id="L-2431"><a href="#L-2431"><span class="linenos">2431</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-2432"><a href="#L-2432"><span class="linenos">2432</span></a>                <span class="n">B_mask</span> <span class="o">=</span> <span class="n">grad_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;=</span> <span class="n">A_shape</span><span class="p">[</span><span class="n">sparse_dim</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2433"><a href="#L-2433"><span class="linenos">2433</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2434"><a href="#L-2434"><span class="linenos">2434</span></a>                <span class="n">grad_B_indices</span> <span class="o">=</span> <span class="n">grad_indices</span><span class="p">[:,</span> <span class="n">B_mask</span><span class="p">]</span>
</span><span id="L-2435"><a href="#L-2435"><span class="linenos">2435</span></a>                <span class="n">grad_B_indices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">A_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2436"><a href="#L-2436"><span class="linenos">2436</span></a>                <span class="n">grad_B_values</span> <span class="o">=</span> <span class="n">grad_values</span><span class="p">[</span><span class="n">B_mask</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="L-2437"><a href="#L-2437"><span class="linenos">2437</span></a>                <span class="n">grad_B</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">grad_B_indices</span><span class="p">,</span> <span class="n">grad_B_values</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">B_shape</span><span class="p">)</span>
</span><span id="L-2438"><a href="#L-2438"><span class="linenos">2438</span></a>                <span class="k">del</span> <span class="n">B_mask</span><span class="p">,</span> <span class="n">grad_B_indices</span><span class="p">,</span> <span class="n">grad_B_values</span>
</span><span id="L-2439"><a href="#L-2439"><span class="linenos">2439</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2440"><a href="#L-2440"><span class="linenos">2440</span></a>                <span class="n">grad_B</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2441"><a href="#L-2441"><span class="linenos">2441</span></a>
</span><span id="L-2442"><a href="#L-2442"><span class="linenos">2442</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_A</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="n">dense_dim</span><span class="p">)</span>
</span><span id="L-2443"><a href="#L-2443"><span class="linenos">2443</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_B</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="n">dense_dim</span><span class="p">)</span>
</span><span id="L-2444"><a href="#L-2444"><span class="linenos">2444</span></a>            <span class="k">return</span> <span class="n">grad_A</span><span class="p">,</span> <span class="n">grad_B</span>
</span><span id="L-2445"><a href="#L-2445"><span class="linenos">2445</span></a>
</span><span id="L-2446"><a href="#L-2446"><span class="linenos">2446</span></a>
</span><span id="L-2447"><a href="#L-2447"><span class="linenos">2447</span></a>
</span><span id="L-2448"><a href="#L-2448"><span class="linenos">2448</span></a>    <span class="c1"># Takes a sparse tensor whose dense dimension is 0 and returns a sparse tensor with the same shape, whose dense dimension is 1.</span>
</span><span id="L-2449"><a href="#L-2449"><span class="linenos">2449</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">unsqueeze_dense_dim</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2450"><a href="#L-2450"><span class="linenos">2450</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2451"><a href="#L-2451"><span class="linenos">2451</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2452"><a href="#L-2452"><span class="linenos">2452</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2453"><a href="#L-2453"><span class="linenos">2453</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2454"><a href="#L-2454"><span class="linenos">2454</span></a>            
</span><span id="L-2455"><a href="#L-2455"><span class="linenos">2455</span></a>            <span class="c1"># Save context for backward pass</span>
</span><span id="L-2456"><a href="#L-2456"><span class="linenos">2456</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">A_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2457"><a href="#L-2457"><span class="linenos">2457</span></a>            
</span><span id="L-2458"><a href="#L-2458"><span class="linenos">2458</span></a>            <span class="c1"># Extract indices and values</span>
</span><span id="L-2459"><a href="#L-2459"><span class="linenos">2459</span></a>            <span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>  <span class="c1"># Shape: (sparse_dim, nvals)</span>
</span><span id="L-2460"><a href="#L-2460"><span class="linenos">2460</span></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>    <span class="c1"># Shape: (nvals,)                </span>
</span><span id="L-2461"><a href="#L-2461"><span class="linenos">2461</span></a>            
</span><span id="L-2462"><a href="#L-2462"><span class="linenos">2462</span></a>            <span class="c1"># Reshape values to have a dense dimension of 1</span>
</span><span id="L-2463"><a href="#L-2463"><span class="linenos">2463</span></a>            <span class="n">values_out</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="L-2464"><a href="#L-2464"><span class="linenos">2464</span></a>            
</span><span id="L-2465"><a href="#L-2465"><span class="linenos">2465</span></a>            <span class="c1"># Create the output sparse tensor            </span>
</span><span id="L-2466"><a href="#L-2466"><span class="linenos">2466</span></a>            <span class="n">output_sparse_tensor</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values_out</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">A_shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
</span><span id="L-2467"><a href="#L-2467"><span class="linenos">2467</span></a>            
</span><span id="L-2468"><a href="#L-2468"><span class="linenos">2468</span></a>            <span class="k">return</span> <span class="n">output_sparse_tensor</span>
</span><span id="L-2469"><a href="#L-2469"><span class="linenos">2469</span></a>
</span><span id="L-2470"><a href="#L-2470"><span class="linenos">2470</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2471"><a href="#L-2471"><span class="linenos">2471</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2472"><a href="#L-2472"><span class="linenos">2472</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2473"><a href="#L-2473"><span class="linenos">2473</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2474"><a href="#L-2474"><span class="linenos">2474</span></a>                <span class="k">return</span> <span class="kc">None</span>
</span><span id="L-2475"><a href="#L-2475"><span class="linenos">2475</span></a>            
</span><span id="L-2476"><a href="#L-2476"><span class="linenos">2476</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2477"><a href="#L-2477"><span class="linenos">2477</span></a>
</span><span id="L-2478"><a href="#L-2478"><span class="linenos">2478</span></a>            <span class="n">indices</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2479"><a href="#L-2479"><span class="linenos">2479</span></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2480"><a href="#L-2480"><span class="linenos">2480</span></a>
</span><span id="L-2481"><a href="#L-2481"><span class="linenos">2481</span></a>            <span class="c1"># Reverse the transformation: remove the added dense dimension</span>
</span><span id="L-2482"><a href="#L-2482"><span class="linenos">2482</span></a>            <span class="n">values_out</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Flatten the values</span>
</span><span id="L-2483"><a href="#L-2483"><span class="linenos">2483</span></a>            
</span><span id="L-2484"><a href="#L-2484"><span class="linenos">2484</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values_out</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">A_shape</span><span class="p">)</span>
</span><span id="L-2485"><a href="#L-2485"><span class="linenos">2485</span></a>            
</span><span id="L-2486"><a href="#L-2486"><span class="linenos">2486</span></a>            <span class="k">return</span> <span class="n">grad_input</span>
</span><span id="L-2487"><a href="#L-2487"><span class="linenos">2487</span></a>        
</span><span id="L-2488"><a href="#L-2488"><span class="linenos">2488</span></a>
</span><span id="L-2489"><a href="#L-2489"><span class="linenos">2489</span></a>
</span><span id="L-2490"><a href="#L-2490"><span class="linenos">2490</span></a>    <span class="c1"># Takes a sparse tensor whose dense dimension is 1 and returns a sparse tensor with the same shape, whose dense dimension is zero.</span>
</span><span id="L-2491"><a href="#L-2491"><span class="linenos">2491</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">flatten_dense_dim</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2492"><a href="#L-2492"><span class="linenos">2492</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2493"><a href="#L-2493"><span class="linenos">2493</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2494"><a href="#L-2494"><span class="linenos">2494</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2495"><a href="#L-2495"><span class="linenos">2495</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2496"><a href="#L-2496"><span class="linenos">2496</span></a>            
</span><span id="L-2497"><a href="#L-2497"><span class="linenos">2497</span></a>            <span class="c1"># Save context for backward pass</span>
</span><span id="L-2498"><a href="#L-2498"><span class="linenos">2498</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2499"><a href="#L-2499"><span class="linenos">2499</span></a>            
</span><span id="L-2500"><a href="#L-2500"><span class="linenos">2500</span></a>            <span class="c1"># Extract indices and values</span>
</span><span id="L-2501"><a href="#L-2501"><span class="linenos">2501</span></a>            <span class="n">indices</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>  <span class="c1"># Shape: (sparse_dim, nvals)</span>
</span><span id="L-2502"><a href="#L-2502"><span class="linenos">2502</span></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>    <span class="c1"># Shape: (nvals, dense_dim)</span>
</span><span id="L-2503"><a href="#L-2503"><span class="linenos">2503</span></a>            
</span><span id="L-2504"><a href="#L-2504"><span class="linenos">2504</span></a>            <span class="c1"># Flatten the dense dimension into the sparse indices</span>
</span><span id="L-2505"><a href="#L-2505"><span class="linenos">2505</span></a>            <span class="n">nvals_in</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2506"><a href="#L-2506"><span class="linenos">2506</span></a>            <span class="n">nvals_out</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-2507"><a href="#L-2507"><span class="linenos">2507</span></a>            <span class="n">dense_dim</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>            
</span><span id="L-2508"><a href="#L-2508"><span class="linenos">2508</span></a>            
</span><span id="L-2509"><a href="#L-2509"><span class="linenos">2509</span></a>            <span class="c1"># Repeat indices for each dense element</span>
</span><span id="L-2510"><a href="#L-2510"><span class="linenos">2510</span></a>            <span class="n">use_variant_one</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-2511"><a href="#L-2511"><span class="linenos">2511</span></a>
</span><span id="L-2512"><a href="#L-2512"><span class="linenos">2512</span></a>            <span class="k">if</span> <span class="n">use_variant_one</span><span class="p">:</span>
</span><span id="L-2513"><a href="#L-2513"><span class="linenos">2513</span></a>                <span class="c1"># More efficient variant</span>
</span><span id="L-2514"><a href="#L-2514"><span class="linenos">2514</span></a>                <span class="n">indices_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">nvals_out</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-2515"><a href="#L-2515"><span class="linenos">2515</span></a>                <span class="n">indices_out</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2516"><a href="#L-2516"><span class="linenos">2516</span></a>                <span class="n">indices_out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">nvals_in</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2517"><a href="#L-2517"><span class="linenos">2517</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2518"><a href="#L-2518"><span class="linenos">2518</span></a>                <span class="c1"># Clearer variant</span>
</span><span id="L-2519"><a href="#L-2519"><span class="linenos">2519</span></a>                <span class="n">expanded_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2520"><a href="#L-2520"><span class="linenos">2520</span></a>                
</span><span id="L-2521"><a href="#L-2521"><span class="linenos">2521</span></a>                <span class="c1"># Create new indices for the dense dimension</span>
</span><span id="L-2522"><a href="#L-2522"><span class="linenos">2522</span></a>                <span class="n">additional_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">nvals_in</span><span class="p">)</span>
</span><span id="L-2523"><a href="#L-2523"><span class="linenos">2523</span></a>                <span class="n">additional_indices</span> <span class="o">=</span> <span class="n">additional_indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2524"><a href="#L-2524"><span class="linenos">2524</span></a>                
</span><span id="L-2525"><a href="#L-2525"><span class="linenos">2525</span></a>                <span class="c1"># Combine sparse and new dense indices</span>
</span><span id="L-2526"><a href="#L-2526"><span class="linenos">2526</span></a>                <span class="n">indices_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">expanded_indices</span><span class="p">,</span> <span class="n">additional_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2527"><a href="#L-2527"><span class="linenos">2527</span></a>         
</span><span id="L-2528"><a href="#L-2528"><span class="linenos">2528</span></a>            <span class="c1"># Flatten values</span>
</span><span id="L-2529"><a href="#L-2529"><span class="linenos">2529</span></a>            <span class="n">values_out</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2530"><a href="#L-2530"><span class="linenos">2530</span></a>            
</span><span id="L-2531"><a href="#L-2531"><span class="linenos">2531</span></a>            <span class="c1"># Create the output sparse tensor            </span>
</span><span id="L-2532"><a href="#L-2532"><span class="linenos">2532</span></a>            <span class="n">output_sparse_tensor</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices_out</span><span class="p">,</span> <span class="n">values_out</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2533"><a href="#L-2533"><span class="linenos">2533</span></a>            
</span><span id="L-2534"><a href="#L-2534"><span class="linenos">2534</span></a>            <span class="k">return</span> <span class="n">output_sparse_tensor</span>
</span><span id="L-2535"><a href="#L-2535"><span class="linenos">2535</span></a>
</span><span id="L-2536"><a href="#L-2536"><span class="linenos">2536</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2537"><a href="#L-2537"><span class="linenos">2537</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2538"><a href="#L-2538"><span class="linenos">2538</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2539"><a href="#L-2539"><span class="linenos">2539</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2540"><a href="#L-2540"><span class="linenos">2540</span></a>                <span class="k">return</span> <span class="kc">None</span>
</span><span id="L-2541"><a href="#L-2541"><span class="linenos">2541</span></a>            
</span><span id="L-2542"><a href="#L-2542"><span class="linenos">2542</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2543"><a href="#L-2543"><span class="linenos">2543</span></a>
</span><span id="L-2544"><a href="#L-2544"><span class="linenos">2544</span></a>            <span class="n">indices</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2545"><a href="#L-2545"><span class="linenos">2545</span></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2546"><a href="#L-2546"><span class="linenos">2546</span></a>
</span><span id="L-2547"><a href="#L-2547"><span class="linenos">2547</span></a>            <span class="c1"># Reverse the transformation: reconstruct the gradient for the original dense dimension</span>
</span><span id="L-2548"><a href="#L-2548"><span class="linenos">2548</span></a>            <span class="n">dense_dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-2549"><a href="#L-2549"><span class="linenos">2549</span></a>
</span><span id="L-2550"><a href="#L-2550"><span class="linenos">2550</span></a>            <span class="n">arange_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-2551"><a href="#L-2551"><span class="linenos">2551</span></a>            <span class="k">assert</span> <span class="n">arange_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="c1"># to silence PyCharm warning</span>
</span><span id="L-2552"><a href="#L-2552"><span class="linenos">2552</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2553"><a href="#L-2553"><span class="linenos">2553</span></a>            <span class="n">reduced_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arange_indices</span><span class="p">]</span>
</span><span id="L-2554"><a href="#L-2554"><span class="linenos">2554</span></a>            <span class="k">del</span> <span class="n">indices</span><span class="p">,</span> <span class="n">arange_indices</span>
</span><span id="L-2555"><a href="#L-2555"><span class="linenos">2555</span></a>
</span><span id="L-2556"><a href="#L-2556"><span class="linenos">2556</span></a>            <span class="n">grad_values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">)</span>
</span><span id="L-2557"><a href="#L-2557"><span class="linenos">2557</span></a>            <span class="k">del</span> <span class="n">values</span>
</span><span id="L-2558"><a href="#L-2558"><span class="linenos">2558</span></a>
</span><span id="L-2559"><a href="#L-2559"><span class="linenos">2559</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">reduced_indices</span><span class="p">,</span> <span class="n">grad_values</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2560"><a href="#L-2560"><span class="linenos">2560</span></a>            
</span><span id="L-2561"><a href="#L-2561"><span class="linenos">2561</span></a>            <span class="k">return</span> <span class="n">grad_input</span>
</span><span id="L-2562"><a href="#L-2562"><span class="linenos">2562</span></a>
</span><span id="L-2563"><a href="#L-2563"><span class="linenos">2563</span></a>
</span><span id="L-2564"><a href="#L-2564"><span class="linenos">2564</span></a>    <span class="c1"># Equivalent to torch.sort(X, dim=dim, descending=descending)</span>
</span><span id="L-2565"><a href="#L-2565"><span class="linenos">2565</span></a>    <span class="c1"># Works only on dense tensors</span>
</span><span id="L-2566"><a href="#L-2566"><span class="linenos">2566</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">sort</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2567"><a href="#L-2567"><span class="linenos">2567</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2568"><a href="#L-2568"><span class="linenos">2568</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2569"><a href="#L-2569"><span class="linenos">2569</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">descending</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
</span><span id="L-2570"><a href="#L-2570"><span class="linenos">2570</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">X</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="L-2571"><a href="#L-2571"><span class="linenos">2571</span></a>            <span class="n">Xs</span><span class="p">,</span> <span class="n">Xi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="n">descending</span><span class="p">)</span>
</span><span id="L-2572"><a href="#L-2572"><span class="linenos">2572</span></a>            
</span><span id="L-2573"><a href="#L-2573"><span class="linenos">2573</span></a>            <span class="c1"># Store the dimension and sorting permutation for back propagation</span>
</span><span id="L-2574"><a href="#L-2574"><span class="linenos">2574</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2575"><a href="#L-2575"><span class="linenos">2575</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-2576"><a href="#L-2576"><span class="linenos">2576</span></a>
</span><span id="L-2577"><a href="#L-2577"><span class="linenos">2577</span></a>                <span class="c1"># Try to save space</span>
</span><span id="L-2578"><a href="#L-2578"><span class="linenos">2578</span></a>                <span class="n">Xi_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>
</span><span id="L-2579"><a href="#L-2579"><span class="linenos">2579</span></a>                <span class="c1">#assert Xi_max == Xi.max() # Sanity check</span>
</span><span id="L-2580"><a href="#L-2580"><span class="linenos">2580</span></a>
</span><span id="L-2581"><a href="#L-2581"><span class="linenos">2581</span></a>                <span class="k">if</span> <span class="n">Xi_max</span> <span class="o">&lt;=</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
</span><span id="L-2582"><a href="#L-2582"><span class="linenos">2582</span></a>                    <span class="n">Xi</span> <span class="o">=</span> <span class="n">Xi</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
</span><span id="L-2583"><a href="#L-2583"><span class="linenos">2583</span></a>                <span class="k">elif</span> <span class="n">Xi_max</span> <span class="o">&lt;=</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">:</span>
</span><span id="L-2584"><a href="#L-2584"><span class="linenos">2584</span></a>                    <span class="n">Xi</span> <span class="o">=</span> <span class="n">Xi</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="L-2585"><a href="#L-2585"><span class="linenos">2585</span></a>
</span><span id="L-2586"><a href="#L-2586"><span class="linenos">2586</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">Xi</span> <span class="o">=</span> <span class="n">Xi</span>
</span><span id="L-2587"><a href="#L-2587"><span class="linenos">2587</span></a>
</span><span id="L-2588"><a href="#L-2588"><span class="linenos">2588</span></a>            <span class="k">return</span> <span class="n">Xs</span><span class="p">,</span> <span class="n">Xi</span>
</span><span id="L-2589"><a href="#L-2589"><span class="linenos">2589</span></a>
</span><span id="L-2590"><a href="#L-2590"><span class="linenos">2590</span></a>
</span><span id="L-2591"><a href="#L-2591"><span class="linenos">2591</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2592"><a href="#L-2592"><span class="linenos">2592</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2593"><a href="#L-2593"><span class="linenos">2593</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">aaa</span><span class="p">):</span>
</span><span id="L-2594"><a href="#L-2594"><span class="linenos">2594</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span>
</span><span id="L-2595"><a href="#L-2595"><span class="linenos">2595</span></a>            <span class="n">Xi</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Xi</span>
</span><span id="L-2596"><a href="#L-2596"><span class="linenos">2596</span></a>
</span><span id="L-2597"><a href="#L-2597"><span class="linenos">2597</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2598"><a href="#L-2598"><span class="linenos">2598</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2599"><a href="#L-2599"><span class="linenos">2599</span></a>            <span class="k">elif</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="L-2600"><a href="#L-2600"><span class="linenos">2600</span></a>                <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;This should not happen&#39;</span>
</span><span id="L-2601"><a href="#L-2601"><span class="linenos">2601</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2602"><a href="#L-2602"><span class="linenos">2602</span></a>                <span class="c1"># 0.089 seconds</span>
</span><span id="L-2603"><a href="#L-2603"><span class="linenos">2603</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2604"><a href="#L-2604"><span class="linenos">2604</span></a>                <span class="n">grad_input</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">Xi</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">src</span><span class="o">=</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2605"><a href="#L-2605"><span class="linenos">2605</span></a>
</span><span id="L-2606"><a href="#L-2606"><span class="linenos">2606</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2607"><a href="#L-2607"><span class="linenos">2607</span></a>
</span><span id="L-2608"><a href="#L-2608"><span class="linenos">2608</span></a>
</span><span id="L-2609"><a href="#L-2609"><span class="linenos">2609</span></a>
</span><span id="L-2610"><a href="#L-2610"><span class="linenos">2610</span></a>    <span class="c1"># Sorts a sparse tensor along a given dimension</span>
</span><span id="L-2611"><a href="#L-2611"><span class="linenos">2611</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">sort_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2612"><a href="#L-2612"><span class="linenos">2612</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2613"><a href="#L-2613"><span class="linenos">2613</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2614"><a href="#L-2614"><span class="linenos">2614</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">descending</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
</span><span id="L-2615"><a href="#L-2615"><span class="linenos">2615</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-2616"><a href="#L-2616"><span class="linenos">2616</span></a>
</span><span id="L-2617"><a href="#L-2617"><span class="linenos">2617</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2618"><a href="#L-2618"><span class="linenos">2618</span></a>
</span><span id="L-2619"><a href="#L-2619"><span class="linenos">2619</span></a>            <span class="c1"># Extract indices and values</span>
</span><span id="L-2620"><a href="#L-2620"><span class="linenos">2620</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2621"><a href="#L-2621"><span class="linenos">2621</span></a>            <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2622"><a href="#L-2622"><span class="linenos">2622</span></a>            
</span><span id="L-2623"><a href="#L-2623"><span class="linenos">2623</span></a>            <span class="n">sortdims</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span> <span class="k">if</span> <span class="n">d</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">]</span>
</span><span id="L-2624"><a href="#L-2624"><span class="linenos">2624</span></a>
</span><span id="L-2625"><a href="#L-2625"><span class="linenos">2625</span></a>            <span class="n">inds2</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[</span><span class="n">sortdims</span><span class="p">,:]</span>
</span><span id="L-2626"><a href="#L-2626"><span class="linenos">2626</span></a>            <span class="n">shape2</span> <span class="o">=</span> <span class="p">[</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">sortdims</span><span class="p">]</span>
</span><span id="L-2627"><a href="#L-2627"><span class="linenos">2627</span></a>
</span><span id="L-2628"><a href="#L-2628"><span class="linenos">2628</span></a>            <span class="n">inds1d</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">ravel_index</span><span class="p">(</span><span class="n">inds2</span><span class="p">,</span> <span class="n">shape2</span><span class="p">)</span>
</span><span id="L-2629"><a href="#L-2629"><span class="linenos">2629</span></a>            <span class="k">del</span> <span class="n">inds2</span><span class="p">,</span> <span class="n">shape2</span>
</span><span id="L-2630"><a href="#L-2630"><span class="linenos">2630</span></a>
</span><span id="L-2631"><a href="#L-2631"><span class="linenos">2631</span></a>            <span class="n">perm_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="n">descending</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-2632"><a href="#L-2632"><span class="linenos">2632</span></a>            <span class="n">perm_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">inds1d</span><span class="p">[</span><span class="n">perm_vals</span><span class="p">],</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>            
</span><span id="L-2633"><a href="#L-2633"><span class="linenos">2633</span></a>
</span><span id="L-2634"><a href="#L-2634"><span class="linenos">2634</span></a>            <span class="k">if</span> <span class="n">dim</span><span class="o">==</span><span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span><span id="L-2635"><a href="#L-2635"><span class="linenos">2635</span></a>                <span class="k">del</span> <span class="n">inds1d</span>
</span><span id="L-2636"><a href="#L-2636"><span class="linenos">2636</span></a>                <span class="n">perm</span> <span class="o">=</span> <span class="n">perm_vals</span><span class="p">[</span><span class="n">perm_inds</span><span class="p">]</span>
</span><span id="L-2637"><a href="#L-2637"><span class="linenos">2637</span></a>                <span class="k">del</span> <span class="n">perm_vals</span><span class="p">,</span> <span class="n">perm_inds</span>
</span><span id="L-2638"><a href="#L-2638"><span class="linenos">2638</span></a>            <span class="k">else</span><span class="p">:</span>                
</span><span id="L-2639"><a href="#L-2639"><span class="linenos">2639</span></a>                <span class="n">perm_inds2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">inds1d</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-2640"><a href="#L-2640"><span class="linenos">2640</span></a>                <span class="k">del</span> <span class="n">inds1d</span>
</span><span id="L-2641"><a href="#L-2641"><span class="linenos">2641</span></a>                <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">perm_vals</span><span class="p">)</span>
</span><span id="L-2642"><a href="#L-2642"><span class="linenos">2642</span></a>                <span class="n">perm_dest</span> <span class="o">=</span> <span class="n">perm_vals</span><span class="p">[</span><span class="n">perm_inds</span><span class="p">]</span>
</span><span id="L-2643"><a href="#L-2643"><span class="linenos">2643</span></a>                <span class="k">del</span> <span class="n">perm_vals</span><span class="p">,</span> <span class="n">perm_inds</span>                
</span><span id="L-2644"><a href="#L-2644"><span class="linenos">2644</span></a>                <span class="n">perm</span><span class="p">[</span><span class="n">perm_inds2</span><span class="p">]</span> <span class="o">=</span> <span class="n">perm_dest</span>
</span><span id="L-2645"><a href="#L-2645"><span class="linenos">2645</span></a>                <span class="k">del</span> <span class="n">perm_inds2</span>
</span><span id="L-2646"><a href="#L-2646"><span class="linenos">2646</span></a>                      
</span><span id="L-2647"><a href="#L-2647"><span class="linenos">2647</span></a>            <span class="c1"># Reconstruct sorted sparse tensor</span>
</span><span id="L-2648"><a href="#L-2648"><span class="linenos">2648</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">vals</span><span class="p">[</span><span class="n">perm</span><span class="p">],</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2649"><a href="#L-2649"><span class="linenos">2649</span></a>            
</span><span id="L-2650"><a href="#L-2650"><span class="linenos">2650</span></a>            <span class="c1"># Save the original indices and sorted indices for backward</span>
</span><span id="L-2651"><a href="#L-2651"><span class="linenos">2651</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2652"><a href="#L-2652"><span class="linenos">2652</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span>
</span><span id="L-2653"><a href="#L-2653"><span class="linenos">2653</span></a>            
</span><span id="L-2654"><a href="#L-2654"><span class="linenos">2654</span></a>            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">perm</span>
</span><span id="L-2655"><a href="#L-2655"><span class="linenos">2655</span></a>
</span><span id="L-2656"><a href="#L-2656"><span class="linenos">2656</span></a>
</span><span id="L-2657"><a href="#L-2657"><span class="linenos">2657</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2658"><a href="#L-2658"><span class="linenos">2658</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2659"><a href="#L-2659"><span class="linenos">2659</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">aaa</span><span class="p">:</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-2660"><a href="#L-2660"><span class="linenos">2660</span></a>
</span><span id="L-2661"><a href="#L-2661"><span class="linenos">2661</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2662"><a href="#L-2662"><span class="linenos">2662</span></a>                <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2663"><a href="#L-2663"><span class="linenos">2663</span></a>
</span><span id="L-2664"><a href="#L-2664"><span class="linenos">2664</span></a>                <span class="n">inds</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-2665"><a href="#L-2665"><span class="linenos">2665</span></a>                <span class="n">vals</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2666"><a href="#L-2666"><span class="linenos">2666</span></a>
</span><span id="L-2667"><a href="#L-2667"><span class="linenos">2667</span></a>                <span class="n">vals_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">vals</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-2668"><a href="#L-2668"><span class="linenos">2668</span></a>                <span class="n">vals_out</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">perm</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
</span><span id="L-2669"><a href="#L-2669"><span class="linenos">2669</span></a>                <span class="k">del</span> <span class="n">vals</span>
</span><span id="L-2670"><a href="#L-2670"><span class="linenos">2670</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">vals_out</span><span class="p">,</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2671"><a href="#L-2671"><span class="linenos">2671</span></a>                <span class="k">del</span> <span class="n">inds</span>
</span><span id="L-2672"><a href="#L-2672"><span class="linenos">2672</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2673"><a href="#L-2673"><span class="linenos">2673</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2674"><a href="#L-2674"><span class="linenos">2674</span></a>                      
</span><span id="L-2675"><a href="#L-2675"><span class="linenos">2675</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2676"><a href="#L-2676"><span class="linenos">2676</span></a>    
</span><span id="L-2677"><a href="#L-2677"><span class="linenos">2677</span></a>
</span><span id="L-2678"><a href="#L-2678"><span class="linenos">2678</span></a>
</span><span id="L-2679"><a href="#L-2679"><span class="linenos">2679</span></a>    <span class="c1"># Equivalent to torch.cumsum(X, dim=dim)</span>
</span><span id="L-2680"><a href="#L-2680"><span class="linenos">2680</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">cumsum_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2681"><a href="#L-2681"><span class="linenos">2681</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2682"><a href="#L-2682"><span class="linenos">2682</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2683"><a href="#L-2683"><span class="linenos">2683</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-2684"><a href="#L-2684"><span class="linenos">2684</span></a>            <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;X must be coalesced&#39;</span>
</span><span id="L-2685"><a href="#L-2685"><span class="linenos">2685</span></a>
</span><span id="L-2686"><a href="#L-2686"><span class="linenos">2686</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span id="L-2687"><a href="#L-2687"><span class="linenos">2687</span></a>
</span><span id="L-2688"><a href="#L-2688"><span class="linenos">2688</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">cumsum_sparse</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-2689"><a href="#L-2689"><span class="linenos">2689</span></a>                                   <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-2690"><a href="#L-2690"><span class="linenos">2690</span></a>                                   <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-2691"><a href="#L-2691"><span class="linenos">2691</span></a>
</span><span id="L-2692"><a href="#L-2692"><span class="linenos">2692</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2693"><a href="#L-2693"><span class="linenos">2693</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="L-2694"><a href="#L-2694"><span class="linenos">2694</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span> <span class="o">=</span> <span class="n">slice_info</span>
</span><span id="L-2695"><a href="#L-2695"><span class="linenos">2695</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="L-2696"><a href="#L-2696"><span class="linenos">2696</span></a>                <span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="L-2697"><a href="#L-2697"><span class="linenos">2697</span></a>
</span><span id="L-2698"><a href="#L-2698"><span class="linenos">2698</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2699"><a href="#L-2699"><span class="linenos">2699</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2700"><a href="#L-2700"><span class="linenos">2700</span></a>
</span><span id="L-2701"><a href="#L-2701"><span class="linenos">2701</span></a>
</span><span id="L-2702"><a href="#L-2702"><span class="linenos">2702</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2703"><a href="#L-2703"><span class="linenos">2703</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2704"><a href="#L-2704"><span class="linenos">2704</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>            
</span><span id="L-2705"><a href="#L-2705"><span class="linenos">2705</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2706"><a href="#L-2706"><span class="linenos">2706</span></a>                <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2707"><a href="#L-2707"><span class="linenos">2707</span></a>
</span><span id="L-2708"><a href="#L-2708"><span class="linenos">2708</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2709"><a href="#L-2709"><span class="linenos">2709</span></a>
</span><span id="L-2710"><a href="#L-2710"><span class="linenos">2710</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span>
</span><span id="L-2711"><a href="#L-2711"><span class="linenos">2711</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">slice_info</span>
</span><span id="L-2712"><a href="#L-2712"><span class="linenos">2712</span></a>            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">cumsum_sparse</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-2713"><a href="#L-2713"><span class="linenos">2713</span></a>                                          <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-2714"><a href="#L-2714"><span class="linenos">2714</span></a>                                          <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-2715"><a href="#L-2715"><span class="linenos">2715</span></a>
</span><span id="L-2716"><a href="#L-2716"><span class="linenos">2716</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
</span><span id="L-2717"><a href="#L-2717"><span class="linenos">2717</span></a>
</span><span id="L-2718"><a href="#L-2718"><span class="linenos">2718</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2719"><a href="#L-2719"><span class="linenos">2719</span></a>
</span><span id="L-2720"><a href="#L-2720"><span class="linenos">2720</span></a>
</span><span id="L-2721"><a href="#L-2721"><span class="linenos">2721</span></a>
</span><span id="L-2722"><a href="#L-2722"><span class="linenos">2722</span></a>    <span class="c1"># Replicates the sparse tensor A n times along the dimension dim</span>
</span><span id="L-2723"><a href="#L-2723"><span class="linenos">2723</span></a>    <span class="c1"># Worsk fastest if dim=0 or dim=-1; see comments inside.</span>
</span><span id="L-2724"><a href="#L-2724"><span class="linenos">2724</span></a>    <span class="k">class</span><span class="w"> </span><span class="nc">repmat_sparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span><span id="L-2725"><a href="#L-2725"><span class="linenos">2725</span></a>        <span class="c1"># noinspection PyMethodOverriding</span>
</span><span id="L-2726"><a href="#L-2726"><span class="linenos">2726</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2727"><a href="#L-2727"><span class="linenos">2727</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span><span id="L-2728"><a href="#L-2728"><span class="linenos">2728</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-2729"><a href="#L-2729"><span class="linenos">2729</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dim</span> <span class="o">&lt;</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</span><span id="L-2730"><a href="#L-2730"><span class="linenos">2730</span></a>            
</span><span id="L-2731"><a href="#L-2731"><span class="linenos">2731</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="L-2732"><a href="#L-2732"><span class="linenos">2732</span></a>            <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2733"><a href="#L-2733"><span class="linenos">2733</span></a>
</span><span id="L-2734"><a href="#L-2734"><span class="linenos">2734</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span>
</span><span id="L-2735"><a href="#L-2735"><span class="linenos">2735</span></a>
</span><span id="L-2736"><a href="#L-2736"><span class="linenos">2736</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2737"><a href="#L-2737"><span class="linenos">2737</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-2738"><a href="#L-2738"><span class="linenos">2738</span></a>
</span><span id="L-2739"><a href="#L-2739"><span class="linenos">2739</span></a>            <span class="c1"># If dim=0, then variant 1 keeps the output tensor sorted.</span>
</span><span id="L-2740"><a href="#L-2740"><span class="linenos">2740</span></a>            <span class="c1"># Similarly, if dim=-1, then variant 2 keeps the output tensor sorted.</span>
</span><span id="L-2741"><a href="#L-2741"><span class="linenos">2741</span></a>            <span class="c1"># So in these cases, there is no need to call coalesce().</span>
</span><span id="L-2742"><a href="#L-2742"><span class="linenos">2742</span></a>            <span class="c1"># Variant 2 is negligibly faster than 1.</span>
</span><span id="L-2743"><a href="#L-2743"><span class="linenos">2743</span></a>            
</span><span id="L-2744"><a href="#L-2744"><span class="linenos">2744</span></a>            <span class="c1"># Changing this choice might break the correctness of the code.</span>
</span><span id="L-2745"><a href="#L-2745"><span class="linenos">2745</span></a>            <span class="n">variant</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span>
</span><span id="L-2746"><a href="#L-2746"><span class="linenos">2746</span></a>
</span><span id="L-2747"><a href="#L-2747"><span class="linenos">2747</span></a>            <span class="k">if</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-2748"><a href="#L-2748"><span class="linenos">2748</span></a>                <span class="n">v</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-2749"><a href="#L-2749"><span class="linenos">2749</span></a>                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                
</span><span id="L-2750"><a href="#L-2750"><span class="linenos">2750</span></a>
</span><span id="L-2751"><a href="#L-2751"><span class="linenos">2751</span></a>                <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>                
</span><span id="L-2752"><a href="#L-2752"><span class="linenos">2752</span></a>                <span class="n">inds</span><span class="p">[</span><span class="n">dim</span><span class="p">,:]</span> <span class="o">+=</span> <span class="n">v</span>
</span><span id="L-2753"><a href="#L-2753"><span class="linenos">2753</span></a>                <span class="k">del</span> <span class="n">v</span>
</span><span id="L-2754"><a href="#L-2754"><span class="linenos">2754</span></a>
</span><span id="L-2755"><a href="#L-2755"><span class="linenos">2755</span></a>                <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">n</span><span class="p">,])</span>
</span><span id="L-2756"><a href="#L-2756"><span class="linenos">2756</span></a>
</span><span id="L-2757"><a href="#L-2757"><span class="linenos">2757</span></a>                <span class="n">out_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2758"><a href="#L-2758"><span class="linenos">2758</span></a>                <span class="n">out_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span><span class="n">out_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
</span><span id="L-2759"><a href="#L-2759"><span class="linenos">2759</span></a>
</span><span id="L-2760"><a href="#L-2760"><span class="linenos">2760</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-2761"><a href="#L-2761"><span class="linenos">2761</span></a>                <span class="n">v</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-2762"><a href="#L-2762"><span class="linenos">2762</span></a>                <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
</span><span id="L-2763"><a href="#L-2763"><span class="linenos">2763</span></a>
</span><span id="L-2764"><a href="#L-2764"><span class="linenos">2764</span></a>                <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">repeats</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2765"><a href="#L-2765"><span class="linenos">2765</span></a>
</span><span id="L-2766"><a href="#L-2766"><span class="linenos">2766</span></a>                <span class="n">inds</span><span class="p">[</span><span class="n">dim</span><span class="p">,:]</span> <span class="o">+=</span> <span class="n">v</span>
</span><span id="L-2767"><a href="#L-2767"><span class="linenos">2767</span></a>                <span class="k">del</span> <span class="n">v</span>
</span><span id="L-2768"><a href="#L-2768"><span class="linenos">2768</span></a>
</span><span id="L-2769"><a href="#L-2769"><span class="linenos">2769</span></a>                <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">repeats</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2770"><a href="#L-2770"><span class="linenos">2770</span></a>
</span><span id="L-2771"><a href="#L-2771"><span class="linenos">2771</span></a>                <span class="n">out_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2772"><a href="#L-2772"><span class="linenos">2772</span></a>                <span class="n">out_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span><span class="n">out_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
</span><span id="L-2773"><a href="#L-2773"><span class="linenos">2773</span></a>
</span><span id="L-2774"><a href="#L-2774"><span class="linenos">2774</span></a>            <span class="k">del</span> <span class="n">A</span>
</span><span id="L-2775"><a href="#L-2775"><span class="linenos">2775</span></a>
</span><span id="L-2776"><a href="#L-2776"><span class="linenos">2776</span></a>            <span class="c1"># If dim=-1 or 0, and the correct variant was used, then inds are already sorted.</span>
</span><span id="L-2777"><a href="#L-2777"><span class="linenos">2777</span></a>            <span class="k">if</span> <span class="n">dim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="L-2778"><a href="#L-2778"><span class="linenos">2778</span></a>                <span class="n">inds</span><span class="p">,</span> <span class="n">vals</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sort_inds_vals</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span>
</span><span id="L-2779"><a href="#L-2779"><span class="linenos">2779</span></a>
</span><span id="L-2780"><a href="#L-2780"><span class="linenos">2780</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">out_shape</span><span class="p">)</span>
</span><span id="L-2781"><a href="#L-2781"><span class="linenos">2781</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span><span id="L-2782"><a href="#L-2782"><span class="linenos">2782</span></a>            <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2783"><a href="#L-2783"><span class="linenos">2783</span></a>
</span><span id="L-2784"><a href="#L-2784"><span class="linenos">2784</span></a>
</span><span id="L-2785"><a href="#L-2785"><span class="linenos">2785</span></a>        <span class="nd">@staticmethod</span>
</span><span id="L-2786"><a href="#L-2786"><span class="linenos">2786</span></a>        <span class="nd">@once_differentiable</span>
</span><span id="L-2787"><a href="#L-2787"><span class="linenos">2787</span></a>        <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
</span><span id="L-2788"><a href="#L-2788"><span class="linenos">2788</span></a>            <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span><span id="L-2789"><a href="#L-2789"><span class="linenos">2789</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="L-2790"><a href="#L-2790"><span class="linenos">2790</span></a>                <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_output</span>
</span><span id="L-2791"><a href="#L-2791"><span class="linenos">2791</span></a>                <span class="n">inds</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-2792"><a href="#L-2792"><span class="linenos">2792</span></a>                <span class="n">inds</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">inds</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">])</span>
</span><span id="L-2793"><a href="#L-2793"><span class="linenos">2793</span></a>
</span><span id="L-2794"><a href="#L-2794"><span class="linenos">2794</span></a>                <span class="c1"># TODO: Maybe this can be sped up when dim=0 or dim=-1 because we can calculate the sum explicitly instead of calling coalesce()</span>
</span><span id="L-2795"><a href="#L-2795"><span class="linenos">2795</span></a>                <span class="c1"># 0 seconds</span>
</span><span id="L-2796"><a href="#L-2796"><span class="linenos">2796</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">grad_output</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2797"><a href="#L-2797"><span class="linenos">2797</span></a>                <span class="c1"># Here we must coalesce</span>
</span><span id="L-2798"><a href="#L-2798"><span class="linenos">2798</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_input</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
</span><span id="L-2799"><a href="#L-2799"><span class="linenos">2799</span></a>
</span><span id="L-2800"><a href="#L-2800"><span class="linenos">2800</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-2801"><a href="#L-2801"><span class="linenos">2801</span></a>                <span class="n">grad_input</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-2802"><a href="#L-2802"><span class="linenos">2802</span></a>
</span><span id="L-2803"><a href="#L-2803"><span class="linenos">2803</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">grad_input</span><span class="p">)</span>
</span><span id="L-2804"><a href="#L-2804"><span class="linenos">2804</span></a>            <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-2805"><a href="#L-2805"><span class="linenos">2805</span></a>
</span><span id="L-2806"><a href="#L-2806"><span class="linenos">2806</span></a>
</span><span id="L-2807"><a href="#L-2807"><span class="linenos">2807</span></a>
</span><span id="L-2808"><a href="#L-2808"><span class="linenos">2808</span></a>
</span><span id="L-2809"><a href="#L-2809"><span class="linenos">2809</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-2810"><a href="#L-2810"><span class="linenos">2810</span></a><span class="c1">##                                        Sparse tensor operations                                         ##</span>
</span><span id="L-2811"><a href="#L-2811"><span class="linenos">2811</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-2812"><a href="#L-2812"><span class="linenos">2812</span></a>
</span><span id="L-2813"><a href="#L-2813"><span class="linenos">2813</span></a><span class="k">class</span><span class="w"> </span><span class="nc">sp</span><span class="p">:</span>
</span><span id="L-2814"><a href="#L-2814"><span class="linenos">2814</span></a>    <span class="c1"># Create a COO sparse tensor in a coalesced state, assuming that the input indices are already coalesced.</span>
</span><span id="L-2815"><a href="#L-2815"><span class="linenos">2815</span></a>    <span class="c1"># If the command sp.verify_coalescence(out) is not commented, the tensor is verified for being correctly coalesced.</span>
</span><span id="L-2816"><a href="#L-2816"><span class="linenos">2816</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2817"><a href="#L-2817"><span class="linenos">2817</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="L-2818"><a href="#L-2818"><span class="linenos">2818</span></a>        <span class="n">sparse_dims</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-2819"><a href="#L-2819"><span class="linenos">2819</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2820"><a href="#L-2820"><span class="linenos">2820</span></a>            <span class="n">inds1d</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">ravel_index</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">sparse_dims</span><span class="p">])</span>
</span><span id="L-2821"><a href="#L-2821"><span class="linenos">2821</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">inds1d</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">inds1d</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="s1">&#39;indices are not unique&#39;</span>
</span><span id="L-2822"><a href="#L-2822"><span class="linenos">2822</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">inds1d</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">inds1d</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;indices are unique but not sorted&#39;</span>
</span><span id="L-2823"><a href="#L-2823"><span class="linenos">2823</span></a>
</span><span id="L-2824"><a href="#L-2824"><span class="linenos">2824</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">is_coalesced</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-2825"><a href="#L-2825"><span class="linenos">2825</span></a>        <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dense_dim</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">())</span>
</span><span id="L-2826"><a href="#L-2826"><span class="linenos">2826</span></a>        <span class="k">return</span> <span class="n">out</span>   
</span><span id="L-2827"><a href="#L-2827"><span class="linenos">2827</span></a>
</span><span id="L-2828"><a href="#L-2828"><span class="linenos">2828</span></a>
</span><span id="L-2829"><a href="#L-2829"><span class="linenos">2829</span></a>
</span><span id="L-2830"><a href="#L-2830"><span class="linenos">2830</span></a>    <span class="c1"># returns a coalesced copy of A assuming that the indices of A are unique but just possibly unsorted</span>
</span><span id="L-2831"><a href="#L-2831"><span class="linenos">2831</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2832"><a href="#L-2832"><span class="linenos">2832</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">coalesce_unique</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2833"><a href="#L-2833"><span class="linenos">2833</span></a>        <span class="n">A_shape</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2834"><a href="#L-2834"><span class="linenos">2834</span></a>        <span class="n">inds2</span><span class="p">,</span> <span class="n">vals2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sort_inds_vals</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span> <span class="n">shape</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2835"><a href="#L-2835"><span class="linenos">2835</span></a>        <span class="k">del</span> <span class="n">A</span>
</span><span id="L-2836"><a href="#L-2836"><span class="linenos">2836</span></a>        <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">inds2</span><span class="p">,</span> <span class="n">vals2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A_shape</span><span class="p">)</span>
</span><span id="L-2837"><a href="#L-2837"><span class="linenos">2837</span></a>
</span><span id="L-2838"><a href="#L-2838"><span class="linenos">2838</span></a>
</span><span id="L-2839"><a href="#L-2839"><span class="linenos">2839</span></a>
</span><span id="L-2840"><a href="#L-2840"><span class="linenos">2840</span></a>    <span class="c1"># returns a coalesced copy of A assuming that A contains several repetitions of the same set of indices, with each set being already coalesced</span>
</span><span id="L-2841"><a href="#L-2841"><span class="linenos">2841</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2842"><a href="#L-2842"><span class="linenos">2842</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">coalesce_repeated</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="L-2843"><a href="#L-2843"><span class="linenos">2843</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">window_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;coalesce_repeated: number of indices is not an integer multiple of copy_size&#39;</span>
</span><span id="L-2844"><a href="#L-2844"><span class="linenos">2844</span></a>        <span class="n">num_repeats</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">window_size</span>
</span><span id="L-2845"><a href="#L-2845"><span class="linenos">2845</span></a>
</span><span id="L-2846"><a href="#L-2846"><span class="linenos">2846</span></a>        <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2847"><a href="#L-2847"><span class="linenos">2847</span></a>        <span class="n">inds_out</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">window_size</span><span class="p">]</span>
</span><span id="L-2848"><a href="#L-2848"><span class="linenos">2848</span></a>        <span class="n">vals_out</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">_values</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="n">window_size</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="L-2849"><a href="#L-2849"><span class="linenos">2849</span></a>
</span><span id="L-2850"><a href="#L-2850"><span class="linenos">2850</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_repeats</span><span class="p">):</span>
</span><span id="L-2851"><a href="#L-2851"><span class="linenos">2851</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2852"><a href="#L-2852"><span class="linenos">2852</span></a>                <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2853"><a href="#L-2853"><span class="linenos">2853</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">inds_out</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">_indices</span><span class="p">()[:,</span> <span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">window_size</span><span class="p">):((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">window_size</span><span class="p">)])</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;coalesce_repeated: index mismatch between differnt windows&#39;</span>
</span><span id="L-2854"><a href="#L-2854"><span class="linenos">2854</span></a>            <span class="n">vals_out</span> <span class="o">+=</span> <span class="n">A</span><span class="o">.</span><span class="n">_values</span><span class="p">()[(</span><span class="n">i</span><span class="o">*</span><span class="n">window_size</span><span class="p">):((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">window_size</span><span class="p">),</span> <span class="o">...</span><span class="p">]</span>
</span><span id="L-2855"><a href="#L-2855"><span class="linenos">2855</span></a>        
</span><span id="L-2856"><a href="#L-2856"><span class="linenos">2856</span></a>        <span class="c1">#inds2, vals2 = sp.sort_inds_vals(indices=A._indices(), values=A._values(), shape=A.shape)</span>
</span><span id="L-2857"><a href="#L-2857"><span class="linenos">2857</span></a>        <span class="c1">#del A</span>
</span><span id="L-2858"><a href="#L-2858"><span class="linenos">2858</span></a>        <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">inds_out</span><span class="p">,</span> <span class="n">vals_out</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2859"><a href="#L-2859"><span class="linenos">2859</span></a>
</span><span id="L-2860"><a href="#L-2860"><span class="linenos">2860</span></a>
</span><span id="L-2861"><a href="#L-2861"><span class="linenos">2861</span></a>
</span><span id="L-2862"><a href="#L-2862"><span class="linenos">2862</span></a>    <span class="c1"># Verify that a sparse input tensor A is correctly coalesced.</span>
</span><span id="L-2863"><a href="#L-2863"><span class="linenos">2863</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2864"><a href="#L-2864"><span class="linenos">2864</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="L-2865"><a href="#L-2865"><span class="linenos">2865</span></a>        <span class="k">if</span> <span class="n">A</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-2866"><a href="#L-2866"><span class="linenos">2866</span></a>            <span class="k">return</span>
</span><span id="L-2867"><a href="#L-2867"><span class="linenos">2867</span></a>        
</span><span id="L-2868"><a href="#L-2868"><span class="linenos">2868</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;verify_coalescence: input tensor is not sparse&#39;</span>
</span><span id="L-2869"><a href="#L-2869"><span class="linenos">2869</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">dense_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">dense_dim</span><span class="p">),</span> <span class="s1">&#39;verify_coalescence: incorrect dense_dim(); expected </span><span class="si">%d</span><span class="s1">, got </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">())</span>
</span><span id="L-2870"><a href="#L-2870"><span class="linenos">2870</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;verify_coalescence: input tensor is not coalesced&#39;</span>
</span><span id="L-2871"><a href="#L-2871"><span class="linenos">2871</span></a>
</span><span id="L-2872"><a href="#L-2872"><span class="linenos">2872</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>            
</span><span id="L-2873"><a href="#L-2873"><span class="linenos">2873</span></a>            <span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
</span><span id="L-2874"><a href="#L-2874"><span class="linenos">2874</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;verify_coalescence: index mismatch in input&#39;</span>
</span><span id="L-2875"><a href="#L-2875"><span class="linenos">2875</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;verify_coalescence: value mismatch in input&#39;</span>
</span><span id="L-2876"><a href="#L-2876"><span class="linenos">2876</span></a>
</span><span id="L-2877"><a href="#L-2877"><span class="linenos">2877</span></a>
</span><span id="L-2878"><a href="#L-2878"><span class="linenos">2878</span></a>
</span><span id="L-2879"><a href="#L-2879"><span class="linenos">2879</span></a>    <span class="c1"># Takes an input dense tensor A and returns a sparse tensor that contains *all* entries in A, including zeros</span>
</span><span id="L-2880"><a href="#L-2880"><span class="linenos">2880</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2881"><a href="#L-2881"><span class="linenos">2881</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">to_sparse_full</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2882"><a href="#L-2882"><span class="linenos">2882</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A should be dense&#39;</span>
</span><span id="L-2883"><a href="#L-2883"><span class="linenos">2883</span></a>
</span><span id="L-2884"><a href="#L-2884"><span class="linenos">2884</span></a>        <span class="c1"># Get the indices of all elements</span>
</span><span id="L-2885"><a href="#L-2885"><span class="linenos">2885</span></a>        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
</span><span id="L-2886"><a href="#L-2886"><span class="linenos">2886</span></a>
</span><span id="L-2887"><a href="#L-2887"><span class="linenos">2887</span></a>        <span class="c1"># Flatten the dense tensor to create values for all elements, including zeros</span>
</span><span id="L-2888"><a href="#L-2888"><span class="linenos">2888</span></a>        <span class="n">values</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span id="L-2889"><a href="#L-2889"><span class="linenos">2889</span></a>
</span><span id="L-2890"><a href="#L-2890"><span class="linenos">2890</span></a>        <span class="c1"># Create the sparse tensor, including all values</span>
</span><span id="L-2891"><a href="#L-2891"><span class="linenos">2891</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2892"><a href="#L-2892"><span class="linenos">2892</span></a>
</span><span id="L-2893"><a href="#L-2893"><span class="linenos">2893</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2894"><a href="#L-2894"><span class="linenos">2894</span></a>
</span><span id="L-2895"><a href="#L-2895"><span class="linenos">2895</span></a>
</span><span id="L-2896"><a href="#L-2896"><span class="linenos">2896</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2897"><a href="#L-2897"><span class="linenos">2897</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">same_shape_prod</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2898"><a href="#L-2898"><span class="linenos">2898</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2899"><a href="#L-2899"><span class="linenos">2899</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="s1">&#39;A, B have different numbers of nonzero values&#39;</span>
</span><span id="L-2900"><a href="#L-2900"><span class="linenos">2900</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2901"><a href="#L-2901"><span class="linenos">2901</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;A, B nonzero pattern mismatch&#39;</span>
</span><span id="L-2902"><a href="#L-2902"><span class="linenos">2902</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">*</span><span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2903"><a href="#L-2903"><span class="linenos">2903</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2904"><a href="#L-2904"><span class="linenos">2904</span></a>
</span><span id="L-2905"><a href="#L-2905"><span class="linenos">2905</span></a>
</span><span id="L-2906"><a href="#L-2906"><span class="linenos">2906</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2907"><a href="#L-2907"><span class="linenos">2907</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">same_shape_prod_</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2908"><a href="#L-2908"><span class="linenos">2908</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-2909"><a href="#L-2909"><span class="linenos">2909</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="s1">&#39;A, B have different numbers of nonzero values&#39;</span>
</span><span id="L-2910"><a href="#L-2910"><span class="linenos">2910</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2911"><a href="#L-2911"><span class="linenos">2911</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;A, B nonzero pattern mismatch&#39;</span>
</span><span id="L-2912"><a href="#L-2912"><span class="linenos">2912</span></a>        <span class="n">A_vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2913"><a href="#L-2913"><span class="linenos">2913</span></a>        <span class="n">A_vals</span> <span class="o">*=</span> <span class="n">B</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-2914"><a href="#L-2914"><span class="linenos">2914</span></a>        <span class="k">return</span> <span class="n">A</span>
</span><span id="L-2915"><a href="#L-2915"><span class="linenos">2915</span></a>
</span><span id="L-2916"><a href="#L-2916"><span class="linenos">2916</span></a>
</span><span id="L-2917"><a href="#L-2917"><span class="linenos">2917</span></a>
</span><span id="L-2918"><a href="#L-2918"><span class="linenos">2918</span></a>    <span class="c1"># The inverse of torch.unravel_index()</span>
</span><span id="L-2919"><a href="#L-2919"><span class="linenos">2919</span></a>    <span class="c1"># Torch JIT does not speed up this function</span>
</span><span id="L-2920"><a href="#L-2920"><span class="linenos">2920</span></a>    <span class="c1"># 1.08 seconds</span>
</span><span id="L-2921"><a href="#L-2921"><span class="linenos">2921</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2922"><a href="#L-2922"><span class="linenos">2922</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">ravel_index</span><span class="p">(</span><span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="L-2923"><a href="#L-2923"><span class="linenos">2923</span></a>        <span class="k">assert</span> <span class="n">indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;indices must be a 2-dimensional tensor&#39;</span>
</span><span id="L-2924"><a href="#L-2924"><span class="linenos">2924</span></a>        <span class="n">nd</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-2925"><a href="#L-2925"><span class="linenos">2925</span></a>
</span><span id="L-2926"><a href="#L-2926"><span class="linenos">2926</span></a>        <span class="k">assert</span> <span class="n">nd</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;input indices are of shape 0 x &lt;something&gt;&#39;</span>
</span><span id="L-2927"><a href="#L-2927"><span class="linenos">2927</span></a>
</span><span id="L-2928"><a href="#L-2928"><span class="linenos">2928</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-2929"><a href="#L-2929"><span class="linenos">2929</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-2930"><a href="#L-2930"><span class="linenos">2930</span></a>
</span><span id="L-2931"><a href="#L-2931"><span class="linenos">2931</span></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">nd</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
</span><span id="L-2932"><a href="#L-2932"><span class="linenos">2932</span></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">weights</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2933"><a href="#L-2933"><span class="linenos">2933</span></a>
</span><span id="L-2934"><a href="#L-2934"><span class="linenos">2934</span></a>        <span class="c1"># Variant 1 works in all cases, but it is not the most memory-efficient one, due to broadcasting</span>
</span><span id="L-2935"><a href="#L-2935"><span class="linenos">2935</span></a>        <span class="c1"># Variants 2 and 3 use torch matrix/tensor multiplication, but when called with Long inputs, they return the error:</span>
</span><span id="L-2936"><a href="#L-2936"><span class="linenos">2936</span></a>        <span class="c1"># RuntimeError: &quot;addmm_cuda&quot; not implemented for &#39;Long&#39;</span>
</span><span id="L-2937"><a href="#L-2937"><span class="linenos">2937</span></a>        <span class="c1"># I thought Variant 4 would work well in that case as well, but it doesn&#39;t support some other operation on Longs.</span>
</span><span id="L-2938"><a href="#L-2938"><span class="linenos">2938</span></a>        <span class="c1"># Therefore sticking with variant 1.</span>
</span><span id="L-2939"><a href="#L-2939"><span class="linenos">2939</span></a>        <span class="c1">#</span>
</span><span id="L-2940"><a href="#L-2940"><span class="linenos">2940</span></a>        <span class="c1"># Alternatively, to save memory, just iterate over the dimension and compute the product explicitly. It shouldn&#39;t take that long.</span>
</span><span id="L-2941"><a href="#L-2941"><span class="linenos">2941</span></a>
</span><span id="L-2942"><a href="#L-2942"><span class="linenos">2942</span></a>        <span class="n">variant</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-2943"><a href="#L-2943"><span class="linenos">2943</span></a>
</span><span id="L-2944"><a href="#L-2944"><span class="linenos">2944</span></a>        <span class="k">if</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-2945"><a href="#L-2945"><span class="linenos">2945</span></a>            <span class="c1"># 0.7 seconds, but no better way to do it</span>
</span><span id="L-2946"><a href="#L-2946"><span class="linenos">2946</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">indices</span><span class="o">*</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-2947"><a href="#L-2947"><span class="linenos">2947</span></a>        <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-2948"><a href="#L-2948"><span class="linenos">2948</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="p">(</span> <span class="n">weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">indices</span> <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2949"><a href="#L-2949"><span class="linenos">2949</span></a>        <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span><span id="L-2950"><a href="#L-2950"><span class="linenos">2950</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">,],[</span><span class="mi">0</span><span class="p">,]))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2951"><a href="#L-2951"><span class="linenos">2951</span></a>        <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span><span id="L-2952"><a href="#L-2952"><span class="linenos">2952</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;nd,nd-&gt;d&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</span><span id="L-2953"><a href="#L-2953"><span class="linenos">2953</span></a>        <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span><span id="L-2954"><a href="#L-2954"><span class="linenos">2954</span></a>            <span class="c1"># not sure this one works</span>
</span><span id="L-2955"><a href="#L-2955"><span class="linenos">2955</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">indices</span>
</span><span id="L-2956"><a href="#L-2956"><span class="linenos">2956</span></a>
</span><span id="L-2957"><a href="#L-2957"><span class="linenos">2957</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-2958"><a href="#L-2958"><span class="linenos">2958</span></a>
</span><span id="L-2959"><a href="#L-2959"><span class="linenos">2959</span></a>
</span><span id="L-2960"><a href="#L-2960"><span class="linenos">2960</span></a>
</span><span id="L-2961"><a href="#L-2961"><span class="linenos">2961</span></a>    <span class="c1"># Sort indices and values. Similar to coalesce(), but does not assume nor impose uniqueness.</span>
</span><span id="L-2962"><a href="#L-2962"><span class="linenos">2962</span></a>    <span class="c1"># 2.93 seconds</span>
</span><span id="L-2963"><a href="#L-2963"><span class="linenos">2963</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-2964"><a href="#L-2964"><span class="linenos">2964</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sort_inds_vals</span><span class="p">(</span><span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span><span id="L-2965"><a href="#L-2965"><span class="linenos">2965</span></a>        <span class="c1">#dense_dims = values.dim()-1</span>
</span><span id="L-2966"><a href="#L-2966"><span class="linenos">2966</span></a>        <span class="n">sparse_dims</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-2967"><a href="#L-2967"><span class="linenos">2967</span></a>
</span><span id="L-2968"><a href="#L-2968"><span class="linenos">2968</span></a>        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-2969"><a href="#L-2969"><span class="linenos">2969</span></a>            <span class="n">shape</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-2970"><a href="#L-2970"><span class="linenos">2970</span></a>            <span class="n">shape</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="L-2971"><a href="#L-2971"><span class="linenos">2971</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2972"><a href="#L-2972"><span class="linenos">2972</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-2973"><a href="#L-2973"><span class="linenos">2973</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-2974"><a href="#L-2974"><span class="linenos">2974</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">sparse_dims</span><span class="p">]</span>        
</span><span id="L-2975"><a href="#L-2975"><span class="linenos">2975</span></a>
</span><span id="L-2976"><a href="#L-2976"><span class="linenos">2976</span></a>        <span class="c1"># 0.54 seconds</span>
</span><span id="L-2977"><a href="#L-2977"><span class="linenos">2977</span></a>        <span class="n">inds1d</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">ravel_index</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
</span><span id="L-2978"><a href="#L-2978"><span class="linenos">2978</span></a>
</span><span id="L-2979"><a href="#L-2979"><span class="linenos">2979</span></a>        <span class="n">debug</span> <span class="o">=</span> <span class="n">fsw_embedding_debug_mode</span>
</span><span id="L-2980"><a href="#L-2980"><span class="linenos">2980</span></a>        <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
</span><span id="L-2981"><a href="#L-2981"><span class="linenos">2981</span></a>            <span class="k">assert</span> <span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">inds1d</span><span class="p">))</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">inds1d</span><span class="p">)</span> <span class="p">),</span> <span class="s1">&#39;indices are not unique&#39;</span>
</span><span id="L-2982"><a href="#L-2982"><span class="linenos">2982</span></a>
</span><span id="L-2983"><a href="#L-2983"><span class="linenos">2983</span></a>        <span class="c1"># 1.8 seconds</span>
</span><span id="L-2984"><a href="#L-2984"><span class="linenos">2984</span></a>        <span class="n">sort_perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">inds1d</span><span class="p">)</span>
</span><span id="L-2985"><a href="#L-2985"><span class="linenos">2985</span></a>
</span><span id="L-2986"><a href="#L-2986"><span class="linenos">2986</span></a>        <span class="k">del</span> <span class="n">inds1d</span>
</span><span id="L-2987"><a href="#L-2987"><span class="linenos">2987</span></a>
</span><span id="L-2988"><a href="#L-2988"><span class="linenos">2988</span></a>        <span class="c1"># 0.6 seconds</span>
</span><span id="L-2989"><a href="#L-2989"><span class="linenos">2989</span></a>        <span class="k">if</span> <span class="n">sort_perm</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-2990"><a href="#L-2990"><span class="linenos">2990</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2991"><a href="#L-2991"><span class="linenos">2991</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">[:,</span><span class="n">sort_perm</span><span class="p">],</span> <span class="n">values</span><span class="p">[</span><span class="n">sort_perm</span><span class="p">])</span>
</span><span id="L-2992"><a href="#L-2992"><span class="linenos">2992</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-2993"><a href="#L-2993"><span class="linenos">2993</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-2994"><a href="#L-2994"><span class="linenos">2994</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">[:,</span><span class="n">sort_perm</span><span class="p">],</span> <span class="n">values</span><span class="p">[</span><span class="n">sort_perm</span><span class="p">,:])</span>
</span><span id="L-2995"><a href="#L-2995"><span class="linenos">2995</span></a>
</span><span id="L-2996"><a href="#L-2996"><span class="linenos">2996</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-2997"><a href="#L-2997"><span class="linenos">2997</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">indices</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;input was already sorted. the code can be sped up by avoiding sort in this particular case&#39;</span>
</span><span id="L-2998"><a href="#L-2998"><span class="linenos">2998</span></a>
</span><span id="L-2999"><a href="#L-2999"><span class="linenos">2999</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3000"><a href="#L-3000"><span class="linenos">3000</span></a>
</span><span id="L-3001"><a href="#L-3001"><span class="linenos">3001</span></a>
</span><span id="L-3002"><a href="#L-3002"><span class="linenos">3002</span></a>
</span><span id="L-3003"><a href="#L-3003"><span class="linenos">3003</span></a>    <span class="c1"># Entrywise division of sparse A by dense A. Supports broadcasting of B to A.</span>
</span><span id="L-3004"><a href="#L-3004"><span class="linenos">3004</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3005"><a href="#L-3005"><span class="linenos">3005</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">div_sparse_dense</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">):</span> 
</span><span id="L-3006"><a href="#L-3006"><span class="linenos">3006</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;A must be sparse&#39;</span>
</span><span id="L-3007"><a href="#L-3007"><span class="linenos">3007</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">B</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;B must be dense&#39;</span>
</span><span id="L-3008"><a href="#L-3008"><span class="linenos">3008</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;B must be of size that allows broadcasting to A&quot;</span>
</span><span id="L-3009"><a href="#L-3009"><span class="linenos">3009</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;A must be coalesced&#39;</span>
</span><span id="L-3010"><a href="#L-3010"><span class="linenos">3010</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">(),</span> <span class="s1">&#39;This function can only be called within torch.no_grad(), as it is not meant to calculate gradients.&#39;</span>
</span><span id="L-3011"><a href="#L-3011"><span class="linenos">3011</span></a>
</span><span id="L-3012"><a href="#L-3012"><span class="linenos">3012</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-3013"><a href="#L-3013"><span class="linenos">3013</span></a>        <span class="n">A_vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-3014"><a href="#L-3014"><span class="linenos">3014</span></a>
</span><span id="L-3015"><a href="#L-3015"><span class="linenos">3015</span></a>        <span class="n">out_vals</span> <span class="o">=</span> <span class="n">A_vals</span> <span class="o">/</span> <span class="n">B</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">inds</span><span class="p">)]</span>
</span><span id="L-3016"><a href="#L-3016"><span class="linenos">3016</span></a>
</span><span id="L-3017"><a href="#L-3017"><span class="linenos">3017</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">out_vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3018"><a href="#L-3018"><span class="linenos">3018</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3019"><a href="#L-3019"><span class="linenos">3019</span></a>
</span><span id="L-3020"><a href="#L-3020"><span class="linenos">3020</span></a>
</span><span id="L-3021"><a href="#L-3021"><span class="linenos">3021</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3022"><a href="#L-3022"><span class="linenos">3022</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sum_sparse</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-3023"><a href="#L-3023"><span class="linenos">3023</span></a>
</span><span id="L-3024"><a href="#L-3024"><span class="linenos">3024</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s2">&quot;input tensor must be sparse&quot;</span>
</span><span id="L-3025"><a href="#L-3025"><span class="linenos">3025</span></a>        <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-3026"><a href="#L-3026"><span class="linenos">3026</span></a>
</span><span id="L-3027"><a href="#L-3027"><span class="linenos">3027</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_debug_mode</span><span class="p">:</span>
</span><span id="L-3028"><a href="#L-3028"><span class="linenos">3028</span></a>            <span class="k">assert</span> <span class="n">slice_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;if this happens, there might be an inefficiency in the code&#39;</span>
</span><span id="L-3029"><a href="#L-3029"><span class="linenos">3029</span></a>
</span><span id="L-3030"><a href="#L-3030"><span class="linenos">3030</span></a>        <span class="c1"># Shape of the sparse tensor</span>
</span><span id="L-3031"><a href="#L-3031"><span class="linenos">3031</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3032"><a href="#L-3032"><span class="linenos">3032</span></a>        <span class="n">dim</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">dim_to_list</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
</span><span id="L-3033"><a href="#L-3033"><span class="linenos">3033</span></a>
</span><span id="L-3034"><a href="#L-3034"><span class="linenos">3034</span></a>        <span class="k">if</span> <span class="n">slice_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3035"><a href="#L-3035"><span class="linenos">3035</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span>
</span><span id="L-3036"><a href="#L-3036"><span class="linenos">3036</span></a>                                           <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-3037"><a href="#L-3037"><span class="linenos">3037</span></a>                                           <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-3038"><a href="#L-3038"><span class="linenos">3038</span></a>                                           <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-3039"><a href="#L-3039"><span class="linenos">3039</span></a>
</span><span id="L-3040"><a href="#L-3040"><span class="linenos">3040</span></a>        <span class="c1"># Verify slice info matches the input tensor</span>
</span><span id="L-3041"><a href="#L-3041"><span class="linenos">3041</span></a>        <span class="n">sp</span><span class="o">.</span><span class="n">verify_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">)</span>        
</span><span id="L-3042"><a href="#L-3042"><span class="linenos">3042</span></a>
</span><span id="L-3043"><a href="#L-3043"><span class="linenos">3043</span></a>        <span class="n">si</span> <span class="o">=</span> <span class="n">slice_info</span>
</span><span id="L-3044"><a href="#L-3044"><span class="linenos">3044</span></a>
</span><span id="L-3045"><a href="#L-3045"><span class="linenos">3045</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-3046"><a href="#L-3046"><span class="linenos">3046</span></a>        <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-3047"><a href="#L-3047"><span class="linenos">3047</span></a>        
</span><span id="L-3048"><a href="#L-3048"><span class="linenos">3048</span></a>        <span class="c1"># Sort the values according to the keys, to form contiguous segments</span>
</span><span id="L-3049"><a href="#L-3049"><span class="linenos">3049</span></a>        <span class="n">vals_sorted</span> <span class="o">=</span> <span class="n">vals</span><span class="p">[</span><span class="n">si</span><span class="p">[</span><span class="s1">&#39;sort_inds&#39;</span><span class="p">]]</span>
</span><span id="L-3050"><a href="#L-3050"><span class="linenos">3050</span></a>
</span><span id="L-3051"><a href="#L-3051"><span class="linenos">3051</span></a>        <span class="c1"># # Get the linear index in vals_sorted of the last index of each slice along dims</span>
</span><span id="L-3052"><a href="#L-3052"><span class="linenos">3052</span></a>        <span class="c1"># slice_ends = counts_consecutive.cumsum(dim=0)-1</span>
</span><span id="L-3053"><a href="#L-3053"><span class="linenos">3053</span></a>
</span><span id="L-3054"><a href="#L-3054"><span class="linenos">3054</span></a>        <span class="k">if</span> <span class="n">si</span><span class="p">[</span><span class="s1">&#39;num_slices&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-3055"><a href="#L-3055"><span class="linenos">3055</span></a>            <span class="n">vals_sorted_cumsum</span> <span class="o">=</span> <span class="n">segcumsum</span><span class="p">(</span><span class="n">vals_sorted</span><span class="p">,</span> <span class="n">si</span><span class="p">[</span><span class="s1">&#39;keys_sorted&#39;</span><span class="p">],</span>
</span><span id="L-3056"><a href="#L-3056"><span class="linenos">3056</span></a>                                           <span class="n">in_place</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-3057"><a href="#L-3057"><span class="linenos">3057</span></a>                                           <span class="n">max_seg_size</span><span class="o">=</span><span class="n">si</span><span class="p">[</span><span class="s1">&#39;max_slice_nonzeros&#39;</span><span class="p">],</span>
</span><span id="L-3058"><a href="#L-3058"><span class="linenos">3058</span></a>                                           <span class="n">thorough_verify_input</span><span class="o">=</span><span class="n">fsw_embedding_debug_mode</span><span class="p">,</span>
</span><span id="L-3059"><a href="#L-3059"><span class="linenos">3059</span></a>                                           <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-3060"><a href="#L-3060"><span class="linenos">3060</span></a>                                           <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-3061"><a href="#L-3061"><span class="linenos">3061</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3062"><a href="#L-3062"><span class="linenos">3062</span></a>            <span class="c1"># If there is only one slice, we don&#39;t need to use segmented cumsum</span>
</span><span id="L-3063"><a href="#L-3063"><span class="linenos">3063</span></a>            <span class="n">vals_sorted_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">vals_sorted</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">vals_sorted</span><span class="p">)</span>
</span><span id="L-3064"><a href="#L-3064"><span class="linenos">3064</span></a>
</span><span id="L-3065"><a href="#L-3065"><span class="linenos">3065</span></a>        <span class="k">del</span> <span class="n">vals_sorted</span>
</span><span id="L-3066"><a href="#L-3066"><span class="linenos">3066</span></a>
</span><span id="L-3067"><a href="#L-3067"><span class="linenos">3067</span></a>        <span class="c1"># Calculate output shape</span>
</span><span id="L-3068"><a href="#L-3068"><span class="linenos">3068</span></a>        <span class="n">shape_out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3069"><a href="#L-3069"><span class="linenos">3069</span></a>
</span><span id="L-3070"><a href="#L-3070"><span class="linenos">3070</span></a>        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dim</span><span class="p">:</span>
</span><span id="L-3071"><a href="#L-3071"><span class="linenos">3071</span></a>            <span class="n">shape_out</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-3072"><a href="#L-3072"><span class="linenos">3072</span></a>
</span><span id="L-3073"><a href="#L-3073"><span class="linenos">3073</span></a>        <span class="c1"># Prepare output values and indices</span>
</span><span id="L-3074"><a href="#L-3074"><span class="linenos">3074</span></a>        <span class="n">vals_out</span> <span class="o">=</span> <span class="n">vals_sorted_cumsum</span><span class="p">[</span><span class="n">si</span><span class="p">[</span><span class="s1">&#39;slice_ends&#39;</span><span class="p">]]</span>
</span><span id="L-3075"><a href="#L-3075"><span class="linenos">3075</span></a>
</span><span id="L-3076"><a href="#L-3076"><span class="linenos">3076</span></a>        <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="L-3077"><a href="#L-3077"><span class="linenos">3077</span></a>        <span class="n">inds_out</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[:,</span> <span class="n">si</span><span class="p">[</span><span class="s1">&#39;sort_inds&#39;</span><span class="p">][</span><span class="n">si</span><span class="p">[</span><span class="s1">&#39;slice_ends&#39;</span><span class="p">]]]</span>
</span><span id="L-3078"><a href="#L-3078"><span class="linenos">3078</span></a>        <span class="n">inds_out</span><span class="p">[</span><span class="n">dim</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3079"><a href="#L-3079"><span class="linenos">3079</span></a>
</span><span id="L-3080"><a href="#L-3080"><span class="linenos">3080</span></a>        <span class="c1"># Create a new sparse tensor with cumulative sum values</span>
</span><span id="L-3081"><a href="#L-3081"><span class="linenos">3081</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds_out</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals_out</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape_out</span><span class="p">)</span>
</span><span id="L-3082"><a href="#L-3082"><span class="linenos">3082</span></a>
</span><span id="L-3083"><a href="#L-3083"><span class="linenos">3083</span></a>        <span class="k">return</span> <span class="n">out</span>        
</span><span id="L-3084"><a href="#L-3084"><span class="linenos">3084</span></a>
</span><span id="L-3085"><a href="#L-3085"><span class="linenos">3085</span></a>
</span><span id="L-3086"><a href="#L-3086"><span class="linenos">3086</span></a>
</span><span id="L-3087"><a href="#L-3087"><span class="linenos">3087</span></a>    <span class="c1"># 1.2 seconds</span>
</span><span id="L-3088"><a href="#L-3088"><span class="linenos">3088</span></a>    <span class="c1"># Computes the cumsum on the nonzero entries of a sparse tensor A along dimension dim</span>
</span><span id="L-3089"><a href="#L-3089"><span class="linenos">3089</span></a>    <span class="c1"># max_slice_nonzers: An upper bound on the maximal number of nonzeros of A along dimension dim, taken over all slices of A along dim</span>
</span><span id="L-3090"><a href="#L-3090"><span class="linenos">3090</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3091"><a href="#L-3091"><span class="linenos">3091</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">cumsum_sparse</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-3092"><a href="#L-3092"><span class="linenos">3092</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s2">&quot;input tensor must be sparse&quot;</span>        
</span><span id="L-3093"><a href="#L-3093"><span class="linenos">3093</span></a>        <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-3094"><a href="#L-3094"><span class="linenos">3094</span></a>        
</span><span id="L-3095"><a href="#L-3095"><span class="linenos">3095</span></a>        <span class="k">assert</span> <span class="n">slice_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;if this happens, there might be an inefficiency in the code&#39;</span>
</span><span id="L-3096"><a href="#L-3096"><span class="linenos">3096</span></a>
</span><span id="L-3097"><a href="#L-3097"><span class="linenos">3097</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-3098"><a href="#L-3098"><span class="linenos">3098</span></a>        <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-3099"><a href="#L-3099"><span class="linenos">3099</span></a>
</span><span id="L-3100"><a href="#L-3100"><span class="linenos">3100</span></a>        <span class="c1"># Shape of the sparse tensor</span>
</span><span id="L-3101"><a href="#L-3101"><span class="linenos">3101</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3102"><a href="#L-3102"><span class="linenos">3102</span></a>
</span><span id="L-3103"><a href="#L-3103"><span class="linenos">3103</span></a>        <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="p">)</span>
</span><span id="L-3104"><a href="#L-3104"><span class="linenos">3104</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">dim</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dim</span> <span class="o">&lt;</span> <span class="n">inds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="L-3105"><a href="#L-3105"><span class="linenos">3105</span></a>
</span><span id="L-3106"><a href="#L-3106"><span class="linenos">3106</span></a>        <span class="c1"># Get the other dimensions excluding the one we&#39;re summing over,</span>
</span><span id="L-3107"><a href="#L-3107"><span class="linenos">3107</span></a>        <span class="c1"># and get the shape along these dimensions.</span>
</span><span id="L-3108"><a href="#L-3108"><span class="linenos">3108</span></a>        <span class="c1"># dims2 = [d for d in range(len(shape)) if d != dim]</span>
</span><span id="L-3109"><a href="#L-3109"><span class="linenos">3109</span></a>        <span class="c1"># shape2 = [shape[d] for d in range(len(shape)) if d != dim]</span>
</span><span id="L-3110"><a href="#L-3110"><span class="linenos">3110</span></a>
</span><span id="L-3111"><a href="#L-3111"><span class="linenos">3111</span></a>        <span class="k">if</span> <span class="n">slice_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3112"><a href="#L-3112"><span class="linenos">3112</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span>
</span><span id="L-3113"><a href="#L-3113"><span class="linenos">3113</span></a>                                           <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-3114"><a href="#L-3114"><span class="linenos">3114</span></a>                                           <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-3115"><a href="#L-3115"><span class="linenos">3115</span></a>                                           <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-3116"><a href="#L-3116"><span class="linenos">3116</span></a>        
</span><span id="L-3117"><a href="#L-3117"><span class="linenos">3117</span></a>        <span class="c1"># Verify slice info matches the input tensor</span>
</span><span id="L-3118"><a href="#L-3118"><span class="linenos">3118</span></a>        <span class="n">sp</span><span class="o">.</span><span class="n">verify_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">)</span>        
</span><span id="L-3119"><a href="#L-3119"><span class="linenos">3119</span></a>          
</span><span id="L-3120"><a href="#L-3120"><span class="linenos">3120</span></a>        <span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
</span><span id="L-3121"><a href="#L-3121"><span class="linenos">3121</span></a>            <span class="n">keys_sorted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;keys_sorted&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,])</span>
</span><span id="L-3122"><a href="#L-3122"><span class="linenos">3122</span></a>            <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;sort_inds&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,])</span>
</span><span id="L-3123"><a href="#L-3123"><span class="linenos">3123</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3124"><a href="#L-3124"><span class="linenos">3124</span></a>            <span class="n">keys_sorted</span> <span class="o">=</span> <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;keys_sorted&#39;</span><span class="p">]</span>
</span><span id="L-3125"><a href="#L-3125"><span class="linenos">3125</span></a>            <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;sort_inds&#39;</span><span class="p">]</span>
</span><span id="L-3126"><a href="#L-3126"><span class="linenos">3126</span></a>
</span><span id="L-3127"><a href="#L-3127"><span class="linenos">3127</span></a>        <span class="n">max_slice_nonzeros</span> <span class="o">=</span> <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;max_slice_nonzeros&#39;</span><span class="p">]</span>
</span><span id="L-3128"><a href="#L-3128"><span class="linenos">3128</span></a>
</span><span id="L-3129"><a href="#L-3129"><span class="linenos">3129</span></a>        <span class="c1"># 0.05 seconds</span>
</span><span id="L-3130"><a href="#L-3130"><span class="linenos">3130</span></a>        <span class="c1"># Sort the values according to the keys, to form contiguous segments</span>
</span><span id="L-3131"><a href="#L-3131"><span class="linenos">3131</span></a>        <span class="n">vals_sorted</span> <span class="o">=</span> <span class="n">vals</span><span class="p">[</span><span class="n">sort_inds</span><span class="p">]</span>
</span><span id="L-3132"><a href="#L-3132"><span class="linenos">3132</span></a>
</span><span id="L-3133"><a href="#L-3133"><span class="linenos">3133</span></a>        <span class="k">if</span> <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;num_slices&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-3134"><a href="#L-3134"><span class="linenos">3134</span></a>            <span class="n">vals_sorted_cumsum</span> <span class="o">=</span> <span class="n">segcumsum</span><span class="p">(</span><span class="n">vals_sorted</span><span class="p">,</span>
</span><span id="L-3135"><a href="#L-3135"><span class="linenos">3135</span></a>                                           <span class="n">keys_sorted</span><span class="p">,</span>
</span><span id="L-3136"><a href="#L-3136"><span class="linenos">3136</span></a>                                           <span class="n">in_place</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-3137"><a href="#L-3137"><span class="linenos">3137</span></a>                                           <span class="n">max_seg_size</span><span class="o">=</span><span class="n">max_slice_nonzeros</span><span class="p">,</span>
</span><span id="L-3138"><a href="#L-3138"><span class="linenos">3138</span></a>                                           <span class="n">thorough_verify_input</span><span class="o">=</span><span class="n">fsw_embedding_debug_mode</span><span class="p">,</span>
</span><span id="L-3139"><a href="#L-3139"><span class="linenos">3139</span></a>                                           <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-3140"><a href="#L-3140"><span class="linenos">3140</span></a>                                           <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="L-3141"><a href="#L-3141"><span class="linenos">3141</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3142"><a href="#L-3142"><span class="linenos">3142</span></a>            <span class="c1"># If there is only one slice, we don&#39;t need to use segmented cumsum</span>
</span><span id="L-3143"><a href="#L-3143"><span class="linenos">3143</span></a>            <span class="n">vals_sorted_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">vals_sorted</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">vals_sorted</span><span class="p">)</span>
</span><span id="L-3144"><a href="#L-3144"><span class="linenos">3144</span></a>
</span><span id="L-3145"><a href="#L-3145"><span class="linenos">3145</span></a>        <span class="k">del</span> <span class="n">vals_sorted</span>
</span><span id="L-3146"><a href="#L-3146"><span class="linenos">3146</span></a>
</span><span id="L-3147"><a href="#L-3147"><span class="linenos">3147</span></a>        <span class="c1"># 0.05 seconds</span>
</span><span id="L-3148"><a href="#L-3148"><span class="linenos">3148</span></a>        <span class="n">vals_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">vals_sorted_cumsum</span><span class="p">)</span>
</span><span id="L-3149"><a href="#L-3149"><span class="linenos">3149</span></a>        <span class="n">vals_out</span><span class="p">[</span><span class="n">sort_inds</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals_sorted_cumsum</span>
</span><span id="L-3150"><a href="#L-3150"><span class="linenos">3150</span></a>
</span><span id="L-3151"><a href="#L-3151"><span class="linenos">3151</span></a>        <span class="c1"># Create a new sparse tensor with cumulative sum values</span>
</span><span id="L-3152"><a href="#L-3152"><span class="linenos">3152</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals_out</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3153"><a href="#L-3153"><span class="linenos">3153</span></a>        
</span><span id="L-3154"><a href="#L-3154"><span class="linenos">3154</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3155"><a href="#L-3155"><span class="linenos">3155</span></a>        
</span><span id="L-3156"><a href="#L-3156"><span class="linenos">3156</span></a>
</span><span id="L-3157"><a href="#L-3157"><span class="linenos">3157</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3158"><a href="#L-3158"><span class="linenos">3158</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">sparse_flip</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span><span id="L-3159"><a href="#L-3159"><span class="linenos">3159</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">(),</span> <span class="s1">&#39;This function can only be called within torch.no_grad(), as it is not meant to calculate gradients.&#39;</span>
</span><span id="L-3160"><a href="#L-3160"><span class="linenos">3160</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="L-3161"><a href="#L-3161"><span class="linenos">3161</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()</span>
</span><span id="L-3162"><a href="#L-3162"><span class="linenos">3162</span></a>
</span><span id="L-3163"><a href="#L-3163"><span class="linenos">3163</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="L-3164"><a href="#L-3164"><span class="linenos">3164</span></a>        <span class="n">vals</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="L-3165"><a href="#L-3165"><span class="linenos">3165</span></a>
</span><span id="L-3166"><a href="#L-3166"><span class="linenos">3166</span></a>        <span class="n">inds</span><span class="p">[</span><span class="n">dim</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="n">inds</span><span class="p">[</span><span class="n">dim</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="L-3167"><a href="#L-3167"><span class="linenos">3167</span></a>
</span><span id="L-3168"><a href="#L-3168"><span class="linenos">3168</span></a>        <span class="n">inds2</span><span class="p">,</span> <span class="n">vals2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sort_inds_vals</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3169"><a href="#L-3169"><span class="linenos">3169</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds2</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3170"><a href="#L-3170"><span class="linenos">3170</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3171"><a href="#L-3171"><span class="linenos">3171</span></a>
</span><span id="L-3172"><a href="#L-3172"><span class="linenos">3172</span></a>
</span><span id="L-3173"><a href="#L-3173"><span class="linenos">3173</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3174"><a href="#L-3174"><span class="linenos">3174</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="p">,</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="L-3175"><a href="#L-3175"><span class="linenos">3175</span></a>        <span class="c1"># Note: calc_nnz_per_slice by default should be False</span>
</span><span id="L-3176"><a href="#L-3176"><span class="linenos">3176</span></a>
</span><span id="L-3177"><a href="#L-3177"><span class="linenos">3177</span></a>        <span class="c1"># We run with no gradients to ensure speed</span>
</span><span id="L-3178"><a href="#L-3178"><span class="linenos">3178</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="L-3179"><a href="#L-3179"><span class="linenos">3179</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s2">&quot;input tensor must be sparse&quot;</span>
</span><span id="L-3180"><a href="#L-3180"><span class="linenos">3180</span></a>            <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-3181"><a href="#L-3181"><span class="linenos">3181</span></a>            
</span><span id="L-3182"><a href="#L-3182"><span class="linenos">3182</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="L-3183"><a href="#L-3183"><span class="linenos">3183</span></a>
</span><span id="L-3184"><a href="#L-3184"><span class="linenos">3184</span></a>            <span class="c1"># Shape of the sparse tensor</span>
</span><span id="L-3185"><a href="#L-3185"><span class="linenos">3185</span></a>            <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3186"><a href="#L-3186"><span class="linenos">3186</span></a>
</span><span id="L-3187"><a href="#L-3187"><span class="linenos">3187</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">dim_to_list</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
</span><span id="L-3188"><a href="#L-3188"><span class="linenos">3188</span></a>            
</span><span id="L-3189"><a href="#L-3189"><span class="linenos">3189</span></a>            <span class="c1"># Get the other dimensions excluding the one we&#39;re summing over,</span>
</span><span id="L-3190"><a href="#L-3190"><span class="linenos">3190</span></a>            <span class="c1"># and get the shape along these dimensions.</span>
</span><span id="L-3191"><a href="#L-3191"><span class="linenos">3191</span></a>            <span class="n">dims2</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dim</span><span class="p">]</span>
</span><span id="L-3192"><a href="#L-3192"><span class="linenos">3192</span></a>            <span class="n">shape2</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dim</span><span class="p">]</span>
</span><span id="L-3193"><a href="#L-3193"><span class="linenos">3193</span></a>
</span><span id="L-3194"><a href="#L-3194"><span class="linenos">3194</span></a>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3195"><a href="#L-3195"><span class="linenos">3195</span></a>                <span class="n">use_variant_one</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-3196"><a href="#L-3196"><span class="linenos">3196</span></a>
</span><span id="L-3197"><a href="#L-3197"><span class="linenos">3197</span></a>                <span class="k">if</span> <span class="n">use_variant_one</span><span class="p">:</span>
</span><span id="L-3198"><a href="#L-3198"><span class="linenos">3198</span></a>                    <span class="n">keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>            
</span><span id="L-3199"><a href="#L-3199"><span class="linenos">3199</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="L-3200"><a href="#L-3200"><span class="linenos">3200</span></a>                    <span class="c1"># This variant is slightly faster than the above, since it creates keys as an expanded scalar tensor.</span>
</span><span id="L-3201"><a href="#L-3201"><span class="linenos">3201</span></a>                    <span class="c1"># Note that consequently, keys will not be contiguous.</span>
</span><span id="L-3202"><a href="#L-3202"><span class="linenos">3202</span></a>                    <span class="c1"># To deal with it, sum_sparse should call cumsum() instead of segcumsum() in this case, since there is only one segment.</span>
</span><span id="L-3203"><a href="#L-3203"><span class="linenos">3203</span></a>                    <span class="n">keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
</span><span id="L-3204"><a href="#L-3204"><span class="linenos">3204</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-3205"><a href="#L-3205"><span class="linenos">3205</span></a>                <span class="n">keys</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">ravel_index</span><span class="p">(</span><span class="n">inds</span><span class="p">[</span><span class="n">dims2</span><span class="p">,:],</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape2</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-3206"><a href="#L-3206"><span class="linenos">3206</span></a>
</span><span id="L-3207"><a href="#L-3207"><span class="linenos">3207</span></a>
</span><span id="L-3208"><a href="#L-3208"><span class="linenos">3208</span></a>            <span class="n">variant</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-3209"><a href="#L-3209"><span class="linenos">3209</span></a>
</span><span id="L-3210"><a href="#L-3210"><span class="linenos">3210</span></a>            <span class="c1"># No need to sort if len(dims2) == 0</span>
</span><span id="L-3211"><a href="#L-3211"><span class="linenos">3211</span></a>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3212"><a href="#L-3212"><span class="linenos">3212</span></a>                <span class="n">keys_sorted</span> <span class="o">=</span> <span class="n">keys</span>
</span><span id="L-3213"><a href="#L-3213"><span class="linenos">3213</span></a>                <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">keys</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">keys</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">keys</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-3214"><a href="#L-3214"><span class="linenos">3214</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-3215"><a href="#L-3215"><span class="linenos">3215</span></a>                <span class="c1"># 2.74 seconds</span>
</span><span id="L-3216"><a href="#L-3216"><span class="linenos">3216</span></a>                <span class="n">keys_sorted</span><span class="p">,</span> <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-3217"><a href="#L-3217"><span class="linenos">3217</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-3218"><a href="#L-3218"><span class="linenos">3218</span></a>                <span class="c1"># Note: If there is a memory problem, try to use this variant</span>
</span><span id="L-3219"><a href="#L-3219"><span class="linenos">3219</span></a>                <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
</span><span id="L-3220"><a href="#L-3220"><span class="linenos">3220</span></a>                <span class="n">keys_sorted</span><span class="p">,</span> <span class="n">sort_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">sort_inds</span><span class="p">))</span>
</span><span id="L-3221"><a href="#L-3221"><span class="linenos">3221</span></a>
</span><span id="L-3222"><a href="#L-3222"><span class="linenos">3222</span></a>            <span class="k">del</span> <span class="n">keys</span>
</span><span id="L-3223"><a href="#L-3223"><span class="linenos">3223</span></a>
</span><span id="L-3224"><a href="#L-3224"><span class="linenos">3224</span></a>            <span class="c1"># variant 2 is roughly twice faster</span>
</span><span id="L-3225"><a href="#L-3225"><span class="linenos">3225</span></a>            <span class="n">variant</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="L-3226"><a href="#L-3226"><span class="linenos">3226</span></a>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3227"><a href="#L-3227"><span class="linenos">3227</span></a>                <span class="n">one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-3228"><a href="#L-3228"><span class="linenos">3228</span></a>                <span class="n">slice_ends</span> <span class="o">=</span> <span class="p">(</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">one</span>
</span><span id="L-3229"><a href="#L-3229"><span class="linenos">3229</span></a>                <span class="c1">#slice_sizes = (keys_sorted.numel())*one</span>
</span><span id="L-3230"><a href="#L-3230"><span class="linenos">3230</span></a>                <span class="n">max_slice_nonzeros</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
</span><span id="L-3231"><a href="#L-3231"><span class="linenos">3231</span></a>
</span><span id="L-3232"><a href="#L-3232"><span class="linenos">3232</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-3233"><a href="#L-3233"><span class="linenos">3233</span></a>                <span class="n">one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="L-3234"><a href="#L-3234"><span class="linenos">3234</span></a>
</span><span id="L-3235"><a href="#L-3235"><span class="linenos">3235</span></a>                <span class="c1"># Get the linear index in vals_sorted of the last index of each slice along dims</span>
</span><span id="L-3236"><a href="#L-3236"><span class="linenos">3236</span></a>                <span class="c1"># 0.52 seconds</span>
</span><span id="L-3237"><a href="#L-3237"><span class="linenos">3237</span></a>                <span class="n">slice_ends</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">keys_sorted</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="p">(</span><span class="n">keys_sorted</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">one</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-3238"><a href="#L-3238"><span class="linenos">3238</span></a>                <span class="c1"># 0.1 seconds</span>
</span><span id="L-3239"><a href="#L-3239"><span class="linenos">3239</span></a>                <span class="n">slice_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">slice_ends</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=-</span><span class="n">one</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-3240"><a href="#L-3240"><span class="linenos">3240</span></a>                <span class="n">max_slice_nonzeros</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">slice_sizes</span><span class="p">))</span>
</span><span id="L-3241"><a href="#L-3241"><span class="linenos">3241</span></a>
</span><span id="L-3242"><a href="#L-3242"><span class="linenos">3242</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-3243"><a href="#L-3243"><span class="linenos">3243</span></a>                <span class="n">_</span><span class="p">,</span> <span class="n">counts_consecutive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">keys_sorted</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-3244"><a href="#L-3244"><span class="linenos">3244</span></a>                <span class="k">del</span> <span class="n">_</span>
</span><span id="L-3245"><a href="#L-3245"><span class="linenos">3245</span></a>
</span><span id="L-3246"><a href="#L-3246"><span class="linenos">3246</span></a>                <span class="n">slice_ends</span> <span class="o">=</span> <span class="n">counts_consecutive</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
</span><span id="L-3247"><a href="#L-3247"><span class="linenos">3247</span></a>                <span class="n">max_slice_nonzeros</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">counts_consecutive</span><span class="p">))</span>            
</span><span id="L-3248"><a href="#L-3248"><span class="linenos">3248</span></a>
</span><span id="L-3249"><a href="#L-3249"><span class="linenos">3249</span></a>            <span class="n">slice_info</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">shape</span><span class="p">,</span>
</span><span id="L-3250"><a href="#L-3250"><span class="linenos">3250</span></a>                        <span class="s1">&#39;dims&#39;</span><span class="p">:</span> <span class="n">dim</span><span class="p">,</span>
</span><span id="L-3251"><a href="#L-3251"><span class="linenos">3251</span></a>                        <span class="s1">&#39;keys_sorted&#39;</span><span class="p">:</span> <span class="n">keys_sorted</span><span class="p">,</span>
</span><span id="L-3252"><a href="#L-3252"><span class="linenos">3252</span></a>                        <span class="s1">&#39;sort_inds&#39;</span><span class="p">:</span> <span class="n">sort_inds</span><span class="p">,</span>
</span><span id="L-3253"><a href="#L-3253"><span class="linenos">3253</span></a>                        <span class="s1">&#39;slice_ends&#39;</span><span class="p">:</span> <span class="n">slice_ends</span><span class="p">,</span>
</span><span id="L-3254"><a href="#L-3254"><span class="linenos">3254</span></a>                        <span class="s1">&#39;num_slices&#39;</span><span class="p">:</span> <span class="n">slice_ends</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
</span><span id="L-3255"><a href="#L-3255"><span class="linenos">3255</span></a>                        <span class="s1">&#39;max_slice_nonzeros&#39;</span><span class="p">:</span> <span class="n">max_slice_nonzeros</span><span class="p">}</span>
</span><span id="L-3256"><a href="#L-3256"><span class="linenos">3256</span></a>
</span><span id="L-3257"><a href="#L-3257"><span class="linenos">3257</span></a>
</span><span id="L-3258"><a href="#L-3258"><span class="linenos">3258</span></a>            <span class="k">if</span> <span class="n">calc_nnz_per_slice</span><span class="p">:</span>
</span><span id="L-3259"><a href="#L-3259"><span class="linenos">3259</span></a>                <span class="c1"># TODO: Make sure this works</span>
</span><span id="L-3260"><a href="#L-3260"><span class="linenos">3260</span></a>                <span class="n">one</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="L-3261"><a href="#L-3261"><span class="linenos">3261</span></a>                <span class="n">vals</span> <span class="o">=</span> <span class="n">one</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
</span><span id="L-3262"><a href="#L-3262"><span class="linenos">3262</span></a>                <span class="n">A_mask</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3263"><a href="#L-3263"><span class="linenos">3263</span></a>                <span class="n">nnz_per_slice</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sum_sparse</span><span class="p">(</span><span class="n">A_mask</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="o">=</span><span class="n">slice_info</span><span class="p">,</span>
</span><span id="L-3264"><a href="#L-3264"><span class="linenos">3264</span></a>                                <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="L-3265"><a href="#L-3265"><span class="linenos">3265</span></a>                                <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span> <span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span><span id="L-3266"><a href="#L-3266"><span class="linenos">3266</span></a>                <span class="k">del</span> <span class="n">A_mask</span>
</span><span id="L-3267"><a href="#L-3267"><span class="linenos">3267</span></a>                
</span><span id="L-3268"><a href="#L-3268"><span class="linenos">3268</span></a>                <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;nnz_per_slice&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nnz_per_slice</span>
</span><span id="L-3269"><a href="#L-3269"><span class="linenos">3269</span></a>            <span class="k">else</span><span class="p">:</span> 
</span><span id="L-3270"><a href="#L-3270"><span class="linenos">3270</span></a>                <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;nnz_per_slice&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3271"><a href="#L-3271"><span class="linenos">3271</span></a>
</span><span id="L-3272"><a href="#L-3272"><span class="linenos">3272</span></a>        <span class="k">return</span> <span class="n">slice_info</span>
</span><span id="L-3273"><a href="#L-3273"><span class="linenos">3273</span></a>
</span><span id="L-3274"><a href="#L-3274"><span class="linenos">3274</span></a>
</span><span id="L-3275"><a href="#L-3275"><span class="linenos">3275</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3276"><a href="#L-3276"><span class="linenos">3276</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">verify_slice_info</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">slice_info</span><span class="p">):</span>
</span><span id="L-3277"><a href="#L-3277"><span class="linenos">3277</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3278"><a href="#L-3278"><span class="linenos">3278</span></a>        <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">sp</span><span class="o">.</span><span class="n">dim_to_list</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
</span><span id="L-3279"><a href="#L-3279"><span class="linenos">3279</span></a>
</span><span id="L-3280"><a href="#L-3280"><span class="linenos">3280</span></a>        <span class="k">assert</span> <span class="n">shape</span> <span class="o">==</span> <span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="s1">&#39;slice_info is inconsistent with input tensor and dim&#39;</span>
</span><span id="L-3281"><a href="#L-3281"><span class="linenos">3281</span></a>        <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">slice_info</span><span class="p">[</span><span class="s1">&#39;dims&#39;</span><span class="p">]),</span> <span class="s1">&#39;slice_info is inconsistent with input tensor and dim&#39;</span>
</span><span id="L-3282"><a href="#L-3282"><span class="linenos">3282</span></a>
</span><span id="L-3283"><a href="#L-3283"><span class="linenos">3283</span></a>
</span><span id="L-3284"><a href="#L-3284"><span class="linenos">3284</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3285"><a href="#L-3285"><span class="linenos">3285</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">dim_to_list</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="L-3286"><a href="#L-3286"><span class="linenos">3286</span></a>        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3287"><a href="#L-3287"><span class="linenos">3287</span></a>
</span><span id="L-3288"><a href="#L-3288"><span class="linenos">3288</span></a>        <span class="c1"># Process dim</span>
</span><span id="L-3289"><a href="#L-3289"><span class="linenos">3289</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
</span><span id="L-3290"><a href="#L-3290"><span class="linenos">3290</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span><span class="p">,]</span>
</span><span id="L-3291"><a href="#L-3291"><span class="linenos">3291</span></a>        <span class="k">else</span><span class="p">:</span>            
</span><span id="L-3292"><a href="#L-3292"><span class="linenos">3292</span></a>            <span class="n">dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="L-3293"><a href="#L-3293"><span class="linenos">3293</span></a>
</span><span id="L-3294"><a href="#L-3294"><span class="linenos">3294</span></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
</span><span id="L-3295"><a href="#L-3295"><span class="linenos">3295</span></a>            <span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span> <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span> <span class="n">d</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3296"><a href="#L-3296"><span class="linenos">3296</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
</span><span id="L-3297"><a href="#L-3297"><span class="linenos">3297</span></a>        
</span><span id="L-3298"><a href="#L-3298"><span class="linenos">3298</span></a>        <span class="k">return</span> <span class="n">dim</span>
</span><span id="L-3299"><a href="#L-3299"><span class="linenos">3299</span></a>
</span><span id="L-3300"><a href="#L-3300"><span class="linenos">3300</span></a>
</span><span id="L-3301"><a href="#L-3301"><span class="linenos">3301</span></a>
</span><span id="L-3302"><a href="#L-3302"><span class="linenos">3302</span></a>    <span class="c1"># When B needs to be broadcast to A, returns the list of dimensions in B that need to be broadcast</span>
</span><span id="L-3303"><a href="#L-3303"><span class="linenos">3303</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3304"><a href="#L-3304"><span class="linenos">3304</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
</span><span id="L-3305"><a href="#L-3305"><span class="linenos">3305</span></a>        <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="s1">&#39;A and B must have the same number of dimensions&#39;</span>
</span><span id="L-3306"><a href="#L-3306"><span class="linenos">3306</span></a>        <span class="n">ndims</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
</span><span id="L-3307"><a href="#L-3307"><span class="linenos">3307</span></a>        <span class="n">broadcast_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)]</span>
</span><span id="L-3308"><a href="#L-3308"><span class="linenos">3308</span></a>        
</span><span id="L-3309"><a href="#L-3309"><span class="linenos">3309</span></a>        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">):</span>
</span><span id="L-3310"><a href="#L-3310"><span class="linenos">3310</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">==</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">])</span> <span class="ow">or</span> <span class="p">(</span><span class="n">d</span> <span class="ow">in</span> <span class="n">broadcast_dims</span><span class="p">),</span> <span class="s1">&#39;A must be of the same size as B, except for dimensions in B that equal 1&#39;</span>
</span><span id="L-3311"><a href="#L-3311"><span class="linenos">3311</span></a>
</span><span id="L-3312"><a href="#L-3312"><span class="linenos">3312</span></a>        <span class="k">return</span> <span class="n">broadcast_dims</span>
</span><span id="L-3313"><a href="#L-3313"><span class="linenos">3313</span></a>
</span><span id="L-3314"><a href="#L-3314"><span class="linenos">3314</span></a>
</span><span id="L-3315"><a href="#L-3315"><span class="linenos">3315</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3316"><a href="#L-3316"><span class="linenos">3316</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">permute</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">perms</span><span class="p">,</span> <span class="n">broadcast_perms_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">backward_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="L-3317"><a href="#L-3317"><span class="linenos">3317</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">(),</span> <span class="s1">&#39;This function can only be called within torch.no_grad(), as it is not meant to calculate gradients.&#39;</span>
</span><span id="L-3318"><a href="#L-3318"><span class="linenos">3318</span></a>        <span class="k">if</span> <span class="n">broadcast_perms_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3319"><a href="#L-3319"><span class="linenos">3319</span></a>            <span class="k">assert</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span>
</span><span id="L-3320"><a href="#L-3320"><span class="linenos">3320</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3321"><a href="#L-3321"><span class="linenos">3321</span></a>            <span class="k">assert</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">broadcast_perms_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span>        
</span><span id="L-3322"><a href="#L-3322"><span class="linenos">3322</span></a>
</span><span id="L-3323"><a href="#L-3323"><span class="linenos">3323</span></a>        <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</span><span id="L-3324"><a href="#L-3324"><span class="linenos">3324</span></a>
</span><span id="L-3325"><a href="#L-3325"><span class="linenos">3325</span></a>        <span class="c1"># Invert the given permutations.</span>
</span><span id="L-3326"><a href="#L-3326"><span class="linenos">3326</span></a>        <span class="c1"># Although the second option has linear time complexity, the first one is simpler and there is no difference in the actual running times, even on huge inputs.</span>
</span><span id="L-3327"><a href="#L-3327"><span class="linenos">3327</span></a>        <span class="k">if</span> <span class="n">backward_mode</span><span class="p">:</span>
</span><span id="L-3328"><a href="#L-3328"><span class="linenos">3328</span></a>            <span class="c1"># If we&#39;re in backward mode, do not invert</span>
</span><span id="L-3329"><a href="#L-3329"><span class="linenos">3329</span></a>            <span class="n">perm_invs</span> <span class="o">=</span> <span class="n">perms</span>
</span><span id="L-3330"><a href="#L-3330"><span class="linenos">3330</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3331"><a href="#L-3331"><span class="linenos">3331</span></a>                <span class="c1"># Variant 2 is faster</span>
</span><span id="L-3332"><a href="#L-3332"><span class="linenos">3332</span></a>                <span class="n">variant</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="L-3333"><a href="#L-3333"><span class="linenos">3333</span></a>
</span><span id="L-3334"><a href="#L-3334"><span class="linenos">3334</span></a>                <span class="k">if</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="L-3335"><a href="#L-3335"><span class="linenos">3335</span></a>                    <span class="n">perm_invs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">perms</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
</span><span id="L-3336"><a href="#L-3336"><span class="linenos">3336</span></a>                <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># 0.1 seconds</span>
</span><span id="L-3337"><a href="#L-3337"><span class="linenos">3337</span></a>                    <span class="n">perm_invs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">perms</span><span class="p">)</span>
</span><span id="L-3338"><a href="#L-3338"><span class="linenos">3338</span></a>                    <span class="n">ar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">perms</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">perms</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-3339"><a href="#L-3339"><span class="linenos">3339</span></a>                    <span class="n">ar</span> <span class="o">=</span> <span class="n">ar</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span><span class="o">*</span><span class="n">dim</span> <span class="o">+</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ar</span><span class="p">),)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span><span class="o">*</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="n">dim</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">perms</span><span class="p">)</span>                    
</span><span id="L-3340"><a href="#L-3340"><span class="linenos">3340</span></a>                    <span class="n">perms</span> <span class="o">=</span> <span class="n">perms</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="c1"># Note: This conversion is required for scatter_(), and it takes negligible time.</span>
</span><span id="L-3341"><a href="#L-3341"><span class="linenos">3341</span></a>                    <span class="n">perm_invs</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">perms</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">ar</span><span class="p">)</span>
</span><span id="L-3342"><a href="#L-3342"><span class="linenos">3342</span></a>                    <span class="k">del</span> <span class="n">ar</span>
</span><span id="L-3343"><a href="#L-3343"><span class="linenos">3343</span></a>
</span><span id="L-3344"><a href="#L-3344"><span class="linenos">3344</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>            
</span><span id="L-3345"><a href="#L-3345"><span class="linenos">3345</span></a>        <span class="n">inds</span><span class="p">[</span><span class="n">dim</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">perm_invs</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">())]</span>
</span><span id="L-3346"><a href="#L-3346"><span class="linenos">3346</span></a>
</span><span id="L-3347"><a href="#L-3347"><span class="linenos">3347</span></a>        <span class="c1"># 1.45 seconds</span>
</span><span id="L-3348"><a href="#L-3348"><span class="linenos">3348</span></a>        <span class="n">inds2</span><span class="p">,</span> <span class="n">vals2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sort_inds_vals</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">shape</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3349"><a href="#L-3349"><span class="linenos">3349</span></a>
</span><span id="L-3350"><a href="#L-3350"><span class="linenos">3350</span></a>        <span class="k">del</span> <span class="n">inds</span><span class="p">,</span> <span class="n">perms</span><span class="p">,</span> <span class="n">perm_invs</span>
</span><span id="L-3351"><a href="#L-3351"><span class="linenos">3351</span></a>
</span><span id="L-3352"><a href="#L-3352"><span class="linenos">3352</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">inds2</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">vals2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="L-3353"><a href="#L-3353"><span class="linenos">3353</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3354"><a href="#L-3354"><span class="linenos">3354</span></a>
</span><span id="L-3355"><a href="#L-3355"><span class="linenos">3355</span></a>
</span><span id="L-3356"><a href="#L-3356"><span class="linenos">3356</span></a>
</span><span id="L-3357"><a href="#L-3357"><span class="linenos">3357</span></a>    <span class="c1"># Returns a tensor of the same size as x, containing the values of d/dx sinc(x)</span>
</span><span id="L-3358"><a href="#L-3358"><span class="linenos">3358</span></a>    <span class="nd">@staticmethod</span>
</span><span id="L-3359"><a href="#L-3359"><span class="linenos">3359</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">dsinc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_sinc</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span><span id="L-3360"><a href="#L-3360"><span class="linenos">3360</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
</span><span id="L-3361"><a href="#L-3361"><span class="linenos">3361</span></a>            <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># Not sure this .clone() is needed, but removing it saves little time anyway</span>
</span><span id="L-3362"><a href="#L-3362"><span class="linenos">3362</span></a>            <span class="n">x2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-3363"><a href="#L-3363"><span class="linenos">3363</span></a>            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinc</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</span><span id="L-3364"><a href="#L-3364"><span class="linenos">3364</span></a>            <span class="n">grad_sinc_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="L-3365"><a href="#L-3365"><span class="linenos">3365</span></a>            <span class="n">dy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grad_sinc_out</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-3366"><a href="#L-3366"><span class="linenos">3366</span></a>
</span><span id="L-3367"><a href="#L-3367"><span class="linenos">3367</span></a>        <span class="k">if</span> <span class="n">return_sinc</span><span class="p">:</span>
</span><span id="L-3368"><a href="#L-3368"><span class="linenos">3368</span></a>            <span class="k">return</span> <span class="n">dy</span><span class="p">,</span> <span class="n">y</span>
</span><span id="L-3369"><a href="#L-3369"><span class="linenos">3369</span></a>        <span class="k">else</span><span class="p">:</span>        
</span><span id="L-3370"><a href="#L-3370"><span class="linenos">3370</span></a>            <span class="k">return</span> <span class="n">dy</span>
</span><span id="L-3371"><a href="#L-3371"><span class="linenos">3371</span></a>
</span><span id="L-3372"><a href="#L-3372"><span class="linenos">3372</span></a>
</span><span id="L-3373"><a href="#L-3373"><span class="linenos">3373</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3374"><a href="#L-3374"><span class="linenos">3374</span></a><span class="c1">##                                            Custom CUDA backend                                          ##</span>
</span><span id="L-3375"><a href="#L-3375"><span class="linenos">3375</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3376"><a href="#L-3376"><span class="linenos">3376</span></a>
</span><span id="L-3377"><a href="#L-3377"><span class="linenos">3377</span></a>
</span><span id="L-3378"><a href="#L-3378"><span class="linenos">3378</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FSWCustomCudaExtensionLoadWarning</span><span class="p">(</span><span class="ne">UserWarning</span><span class="p">):</span>
</span><span id="L-3379"><a href="#L-3379"><span class="linenos">3379</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Raised when the custom CUDA extension could not be loaded and the fallback torch code is used.&quot;&quot;&quot;</span>
</span><span id="L-3380"><a href="#L-3380"><span class="linenos">3380</span></a>    <span class="k">pass</span>
</span><span id="L-3381"><a href="#L-3381"><span class="linenos">3381</span></a>
</span><span id="L-3382"><a href="#L-3382"><span class="linenos">3382</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FSWCustomCudaExtensionLoadError</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">):</span>
</span><span id="L-3383"><a href="#L-3383"><span class="linenos">3383</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Raised when the custom CUDA extension could not be loaded and fallback behavior is disabled.&quot;&quot;&quot;</span>
</span><span id="L-3384"><a href="#L-3384"><span class="linenos">3384</span></a>    <span class="k">pass</span>
</span><span id="L-3385"><a href="#L-3385"><span class="linenos">3385</span></a>
</span><span id="L-3386"><a href="#L-3386"><span class="linenos">3386</span></a>
</span><span id="L-3387"><a href="#L-3387"><span class="linenos">3387</span></a><span class="k">def</span><span class="w"> </span><span class="nf">load_custom_cuda_extension</span><span class="p">(</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-3388"><a href="#L-3388"><span class="linenos">3388</span></a>                               <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="L-3389"><a href="#L-3389"><span class="linenos">3389</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-3390"><a href="#L-3390"><span class="linenos">3390</span></a><span class="sd">    Attempts to load the custom CUDA extension (libfsw_embedding.so).</span>
</span><span id="L-3391"><a href="#L-3391"><span class="linenos">3391</span></a><span class="sd">    Emits a warning if loading fails, or raises an error depending on config.</span>
</span><span id="L-3392"><a href="#L-3392"><span class="linenos">3392</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-3393"><a href="#L-3393"><span class="linenos">3393</span></a>    <span class="k">global</span> <span class="n">_tried_to_load_lib</span><span class="p">,</span> <span class="n">_lib_handle</span><span class="p">,</span> <span class="n">_lib_path</span>
</span><span id="L-3394"><a href="#L-3394"><span class="linenos">3394</span></a>
</span><span id="L-3395"><a href="#L-3395"><span class="linenos">3395</span></a>    <span class="k">if</span> <span class="n">_tried_to_load_lib</span><span class="p">:</span>
</span><span id="L-3396"><a href="#L-3396"><span class="linenos">3396</span></a>        <span class="k">return</span> <span class="n">_lib_handle</span>
</span><span id="L-3397"><a href="#L-3397"><span class="linenos">3397</span></a>
</span><span id="L-3398"><a href="#L-3398"><span class="linenos">3398</span></a>    <span class="n">_tried_to_load_lib</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-3399"><a href="#L-3399"><span class="linenos">3399</span></a>    <span class="n">_lib_handle</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3400"><a href="#L-3400"><span class="linenos">3400</span></a>
</span><span id="L-3401"><a href="#L-3401"><span class="linenos">3401</span></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span><span id="L-3402"><a href="#L-3402"><span class="linenos">3402</span></a>        <span class="k">return</span> <span class="kc">None</span>
</span><span id="L-3403"><a href="#L-3403"><span class="linenos">3403</span></a>
</span><span id="L-3404"><a href="#L-3404"><span class="linenos">3404</span></a>    <span class="k">try</span><span class="p">:</span>
</span><span id="L-3405"><a href="#L-3405"><span class="linenos">3405</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">_lib_path</span><span class="p">):</span>
</span><span id="L-3406"><a href="#L-3406"><span class="linenos">3406</span></a>            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Could not find custom CUDA extension &quot;</span><span class="si">{</span><span class="n">_lib_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
</span><span id="L-3407"><a href="#L-3407"><span class="linenos">3407</span></a>        <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">_lib_path</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3408"><a href="#L-3408"><span class="linenos">3408</span></a>            <span class="c1"># The package comes with a dummy placeholder bin file of size 0.</span>
</span><span id="L-3409"><a href="#L-3409"><span class="linenos">3409</span></a>            <span class="c1"># Here we handle this case.</span>
</span><span id="L-3410"><a href="#L-3410"><span class="linenos">3410</span></a>            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Custom CUDA extension not compiled&#39;</span>
</span><span id="L-3411"><a href="#L-3411"><span class="linenos">3411</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3412"><a href="#L-3412"><span class="linenos">3412</span></a>            <span class="n">_lib_handle</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">_lib_path</span><span class="p">)</span>
</span><span id="L-3413"><a href="#L-3413"><span class="linenos">3413</span></a>
</span><span id="L-3414"><a href="#L-3414"><span class="linenos">3414</span></a>            <span class="k">if</span> <span class="n">_lib_handle</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3415"><a href="#L-3415"><span class="linenos">3415</span></a>                <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Could not load custom CUDA extension &quot;</span><span class="si">{</span><span class="n">_lib_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
</span><span id="L-3416"><a href="#L-3416"><span class="linenos">3416</span></a>            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_lib_handle</span><span class="p">,</span> <span class="s2">&quot;segcumsum_wrapper&quot;</span><span class="p">):</span>
</span><span id="L-3417"><a href="#L-3417"><span class="linenos">3417</span></a>                <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Invalid custom CUDA extension &quot;</span><span class="si">{</span><span class="n">_lib_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
</span><span id="L-3418"><a href="#L-3418"><span class="linenos">3418</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-3419"><a href="#L-3419"><span class="linenos">3419</span></a>                <span class="c1"># Successfully loaded</span>
</span><span id="L-3420"><a href="#L-3420"><span class="linenos">3420</span></a>                <span class="k">if</span> <span class="n">report</span><span class="p">:</span>
</span><span id="L-3421"><a href="#L-3421"><span class="linenos">3421</span></a>                    <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Loaded custom CUDA extension &quot;</span><span class="si">{</span><span class="n">_lib_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</span><span id="L-3422"><a href="#L-3422"><span class="linenos">3422</span></a>                <span class="k">return</span> <span class="n">_lib_handle</span>
</span><span id="L-3423"><a href="#L-3423"><span class="linenos">3423</span></a>
</span><span id="L-3424"><a href="#L-3424"><span class="linenos">3424</span></a>        <span class="c1"># If we got here, something went wrong</span>
</span><span id="L-3425"><a href="#L-3425"><span class="linenos">3425</span></a>        <span class="n">_lib_handle</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3426"><a href="#L-3426"><span class="linenos">3426</span></a>
</span><span id="L-3427"><a href="#L-3427"><span class="linenos">3427</span></a>        <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
</span><span id="L-3428"><a href="#L-3428"><span class="linenos">3428</span></a>
</span><span id="L-3429"><a href="#L-3429"><span class="linenos">3429</span></a>        <span class="k">if</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span>
</span><span id="L-3430"><a href="#L-3430"><span class="linenos">3430</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;Try rebuilding the custom CUDA extension using command fswlib-build, or allow pure-torch fallback code by setting fail_if_cuda_extension_load_fails=False&quot;</span>
</span><span id="L-3431"><a href="#L-3431"><span class="linenos">3431</span></a>            <span class="k">raise</span> <span class="n">FSWCustomCudaExtensionLoadError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="L-3432"><a href="#L-3432"><span class="linenos">3432</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3433"><a href="#L-3433"><span class="linenos">3433</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;Falling back to the pure PyTorch implementation (roughly ~2x slower).&quot;</span>
</span><span id="L-3434"><a href="#L-3434"><span class="linenos">3434</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Try rebuilding the custom CUDA extension using the command `fswlib-build`, or always use fallback code by setting &quot;</span>\
</span><span id="L-3435"><a href="#L-3435"><span class="linenos">3435</span></a>                   <span class="s2">&quot;`use_custom_cuda_extension_if_available=False`.&quot;</span>
</span><span id="L-3436"><a href="#L-3436"><span class="linenos">3436</span></a>            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">FSWCustomCudaExtensionLoadWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-3437"><a href="#L-3437"><span class="linenos">3437</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="L-3438"><a href="#L-3438"><span class="linenos">3438</span></a>
</span><span id="L-3439"><a href="#L-3439"><span class="linenos">3439</span></a>    <span class="c1"># Repeat the same warning/raising behavior if trying to load the library produced a runtime error:</span>
</span><span id="L-3440"><a href="#L-3440"><span class="linenos">3440</span></a>    <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span><span id="L-3441"><a href="#L-3441"><span class="linenos">3441</span></a>        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Could not load custom CUDA extension &quot;</span><span class="si">{</span><span class="n">_lib_path</span><span class="si">}</span><span class="s1">&quot;.</span><span class="se">\n</span><span class="s1">Trying to load produced an exception: </span><span class="se">\n</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
</span><span id="L-3442"><a href="#L-3442"><span class="linenos">3442</span></a>
</span><span id="L-3443"><a href="#L-3443"><span class="linenos">3443</span></a>        <span class="k">if</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span>
</span><span id="L-3444"><a href="#L-3444"><span class="linenos">3444</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;Try rebuilding the custom CUDA extension using command fswlib-build.&quot;</span>
</span><span id="L-3445"><a href="#L-3445"><span class="linenos">3445</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot; Alternatively, allow using the pure-torch fallback code by setting fail_if_cuda_extension_load_fails=False&quot;</span>
</span><span id="L-3446"><a href="#L-3446"><span class="linenos">3446</span></a>            <span class="k">raise</span> <span class="n">FSWCustomCudaExtensionLoadError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="L-3447"><a href="#L-3447"><span class="linenos">3447</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3448"><a href="#L-3448"><span class="linenos">3448</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;Falling back to the pure PyTorch implementation (roughly ~2x slower).&quot;</span>
</span><span id="L-3449"><a href="#L-3449"><span class="linenos">3449</span></a>            <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot; Try rebuilding the custom CUDA extension using the command `fswlib-build`.&quot;</span>\
</span><span id="L-3450"><a href="#L-3450"><span class="linenos">3450</span></a>                   <span class="s2">&quot; Alternatively, always use the fallback code by setting &quot;</span>\
</span><span id="L-3451"><a href="#L-3451"><span class="linenos">3451</span></a>                   <span class="s2">&quot;`use_custom_cuda_extension_if_available=False`.&quot;</span>
</span><span id="L-3452"><a href="#L-3452"><span class="linenos">3452</span></a>            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">FSWCustomCudaExtensionLoadWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-3453"><a href="#L-3453"><span class="linenos">3453</span></a>            <span class="k">return</span> <span class="kc">None</span>
</span><span id="L-3454"><a href="#L-3454"><span class="linenos">3454</span></a>
</span><span id="L-3455"><a href="#L-3455"><span class="linenos">3455</span></a>
</span><span id="L-3456"><a href="#L-3456"><span class="linenos">3456</span></a>
</span><span id="L-3457"><a href="#L-3457"><span class="linenos">3457</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3458"><a href="#L-3458"><span class="linenos">3458</span></a><span class="c1">##                                            Segmented Cumsum                                             ##</span>
</span><span id="L-3459"><a href="#L-3459"><span class="linenos">3459</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3460"><a href="#L-3460"><span class="linenos">3460</span></a>
</span><span id="L-3461"><a href="#L-3461"><span class="linenos">3461</span></a><span class="c1"># This is the main function that calculates the segmented cumsum.</span>
</span><span id="L-3462"><a href="#L-3462"><span class="linenos">3462</span></a><span class="c1"># Input arguments: </span>
</span><span id="L-3463"><a href="#L-3463"><span class="linenos">3463</span></a><span class="c1">#   max_seg_size: an upper bound on the maximal length of a contiguous segment in &lt;segment_ids&gt;. If not provided, detected automatically.</span>
</span><span id="L-3464"><a href="#L-3464"><span class="linenos">3464</span></a><span class="c1">#   in_place:     if set to True, writes the output directly to &lt;values&gt; instead of allocating new memory.</span>
</span><span id="L-3465"><a href="#L-3465"><span class="linenos">3465</span></a><span class="c1">#   thorough_verify_input: verifies the input for correctness. meant for debugging purposes. in particular, checks &lt;segment_ids&gt;</span>
</span><span id="L-3466"><a href="#L-3466"><span class="linenos">3466</span></a><span class="c1">#                          for repeated ids of different segments, and looks for infs and nans in &lt;values&gt;.</span>
</span><span id="L-3467"><a href="#L-3467"><span class="linenos">3467</span></a><span class="c1">#   always_use_pure_torch: when set to True, always uses the pure torch implementation.</span>
</span><span id="L-3468"><a href="#L-3468"><span class="linenos">3468</span></a><span class="c1">#                          otherwise, when the input is on a cuda device, uses a custom cuda implementation.</span>
</span><span id="L-3469"><a href="#L-3469"><span class="linenos">3469</span></a><span class="c1">#                          the cuda implementation has a better memory bottleneck.</span>
</span><span id="L-3470"><a href="#L-3470"><span class="linenos">3470</span></a><span class="c1">#                          in terms of running time, both are comparable, with two-fold differences for one over</span>
</span><span id="L-3471"><a href="#L-3471"><span class="linenos">3471</span></a><span class="c1">#                          the other or vice versa.</span>
</span><span id="L-3472"><a href="#L-3472"><span class="linenos">3472</span></a><span class="c1">#</span>
</span><span id="L-3473"><a href="#L-3473"><span class="linenos">3473</span></a><span class="c1"># Output: The segmented cumsum of &lt;values&gt; according to &lt;segment_ids&gt;.</span>
</span><span id="L-3474"><a href="#L-3474"><span class="linenos">3474</span></a><span class="k">def</span><span class="w"> </span><span class="nf">segcumsum</span><span class="p">(</span><span class="n">values</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-3475"><a href="#L-3475"><span class="linenos">3475</span></a>              <span class="n">segment_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-3476"><a href="#L-3476"><span class="linenos">3476</span></a>              <span class="n">max_seg_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># default: None</span>
</span><span id="L-3477"><a href="#L-3477"><span class="linenos">3477</span></a>              <span class="n">in_place</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="c1"># default: False</span>
</span><span id="L-3478"><a href="#L-3478"><span class="linenos">3478</span></a>              <span class="n">thorough_verify_input</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="c1"># default: False</span>
</span><span id="L-3479"><a href="#L-3479"><span class="linenos">3479</span></a>              <span class="n">use_custom_cuda_extension_if_available</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-3480"><a href="#L-3480"><span class="linenos">3480</span></a>              <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
</span><span id="L-3481"><a href="#L-3481"><span class="linenos">3481</span></a>
</span><span id="L-3482"><a href="#L-3482"><span class="linenos">3482</span></a>    <span class="c1"># Verify input device, dtypes and shapes</span>
</span><span id="L-3483"><a href="#L-3483"><span class="linenos">3483</span></a>    <span class="k">assert</span> <span class="n">values</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;values must be a 1-dimensional tensor&#39;</span>
</span><span id="L-3484"><a href="#L-3484"><span class="linenos">3484</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;segment_ids must be a 1-dimensional tensor&#39;</span>
</span><span id="L-3485"><a href="#L-3485"><span class="linenos">3485</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="s1">&#39;values and segment_ids must contain the same number of elements&#39;</span>
</span><span id="L-3486"><a href="#L-3486"><span class="linenos">3486</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="s1">&#39;segment_ids must have int32 or int64 dtype&#39;</span>
</span><span id="L-3487"><a href="#L-3487"><span class="linenos">3487</span></a>    <span class="k">assert</span> <span class="n">values</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="s1">&#39;values and segment_ids must be on the same device&#39;</span>
</span><span id="L-3488"><a href="#L-3488"><span class="linenos">3488</span></a>
</span><span id="L-3489"><a href="#L-3489"><span class="linenos">3489</span></a>    <span class="c1"># Ensure all data is contiguous</span>
</span><span id="L-3490"><a href="#L-3490"><span class="linenos">3490</span></a>    <span class="k">assert</span> <span class="ow">not</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;segment_ids cannot be sparse&#39;</span>
</span><span id="L-3491"><a href="#L-3491"><span class="linenos">3491</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s1">&#39;segment_ids must be in contiguous format&#39;</span>
</span><span id="L-3492"><a href="#L-3492"><span class="linenos">3492</span></a>
</span><span id="L-3493"><a href="#L-3493"><span class="linenos">3493</span></a>    <span class="k">assert</span> <span class="ow">not</span> <span class="n">values</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;values cannot be sparse&#39;</span>
</span><span id="L-3494"><a href="#L-3494"><span class="linenos">3494</span></a>    <span class="k">assert</span> <span class="p">(</span><span class="ow">not</span> <span class="n">in_place</span><span class="p">)</span> <span class="ow">or</span> <span class="n">values</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s1">&#39;when in_place==True, values must be in contiguous format&#39;</span>
</span><span id="L-3495"><a href="#L-3495"><span class="linenos">3495</span></a>
</span><span id="L-3496"><a href="#L-3496"><span class="linenos">3496</span></a>    <span class="n">num_segments</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3497"><a href="#L-3497"><span class="linenos">3497</span></a>
</span><span id="L-3498"><a href="#L-3498"><span class="linenos">3498</span></a>    <span class="k">if</span> <span class="n">max_seg_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3499"><a href="#L-3499"><span class="linenos">3499</span></a>        <span class="c1"># 0 seconds</span>
</span><span id="L-3500"><a href="#L-3500"><span class="linenos">3500</span></a>        <span class="c1"># Calculate maximal segmet size</span>
</span><span id="L-3501"><a href="#L-3501"><span class="linenos">3501</span></a>        <span class="n">_</span><span class="p">,</span> <span class="n">counts_consecutive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-3502"><a href="#L-3502"><span class="linenos">3502</span></a>        <span class="k">del</span> <span class="n">_</span>
</span><span id="L-3503"><a href="#L-3503"><span class="linenos">3503</span></a>        <span class="n">num_segments</span> <span class="o">=</span> <span class="n">counts_consecutive</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-3504"><a href="#L-3504"><span class="linenos">3504</span></a>        <span class="n">max_seg_size_real</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">counts_consecutive</span><span class="p">))</span>
</span><span id="L-3505"><a href="#L-3505"><span class="linenos">3505</span></a>        <span class="n">max_seg_size</span> <span class="o">=</span> <span class="n">max_seg_size_real</span>
</span><span id="L-3506"><a href="#L-3506"><span class="linenos">3506</span></a>        <span class="k">del</span> <span class="n">counts_consecutive</span>
</span><span id="L-3507"><a href="#L-3507"><span class="linenos">3507</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3508"><a href="#L-3508"><span class="linenos">3508</span></a>        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_seg_size</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span>
</span><span id="L-3509"><a href="#L-3509"><span class="linenos">3509</span></a>        <span class="k">assert</span> <span class="n">max_seg_size</span> <span class="o">&gt;=</span> <span class="mi">1</span>
</span><span id="L-3510"><a href="#L-3510"><span class="linenos">3510</span></a>
</span><span id="L-3511"><a href="#L-3511"><span class="linenos">3511</span></a>    <span class="k">if</span> <span class="n">thorough_verify_input</span><span class="p">:</span>
</span><span id="L-3512"><a href="#L-3512"><span class="linenos">3512</span></a>        <span class="k">if</span> <span class="n">num_segments</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3513"><a href="#L-3513"><span class="linenos">3513</span></a>            <span class="n">_</span><span class="p">,</span> <span class="n">counts_consecutive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-3514"><a href="#L-3514"><span class="linenos">3514</span></a>            <span class="k">del</span> <span class="n">_</span>
</span><span id="L-3515"><a href="#L-3515"><span class="linenos">3515</span></a>            <span class="n">num_segments</span> <span class="o">=</span> <span class="n">counts_consecutive</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-3516"><a href="#L-3516"><span class="linenos">3516</span></a>            <span class="n">max_seg_size_real</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">counts_consecutive</span><span class="p">))</span>
</span><span id="L-3517"><a href="#L-3517"><span class="linenos">3517</span></a>            <span class="k">del</span> <span class="n">counts_consecutive</span>
</span><span id="L-3518"><a href="#L-3518"><span class="linenos">3518</span></a>
</span><span id="L-3519"><a href="#L-3519"><span class="linenos">3519</span></a>        <span class="n">_</span><span class="p">,</span> <span class="n">counts_total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">segment_ids</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-3520"><a href="#L-3520"><span class="linenos">3520</span></a>        <span class="k">del</span> <span class="n">_</span>
</span><span id="L-3521"><a href="#L-3521"><span class="linenos">3521</span></a>        <span class="n">num_segments_unique</span> <span class="o">=</span> <span class="n">counts_total</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-3522"><a href="#L-3522"><span class="linenos">3522</span></a>        <span class="k">del</span> <span class="n">counts_total</span>
</span><span id="L-3523"><a href="#L-3523"><span class="linenos">3523</span></a>
</span><span id="L-3524"><a href="#L-3524"><span class="linenos">3524</span></a>        <span class="k">assert</span> <span class="n">num_segments</span> <span class="o">==</span> <span class="n">num_segments_unique</span><span class="p">,</span> <span class="s1">&#39;repeated segment IDs detected&#39;</span>
</span><span id="L-3525"><a href="#L-3525"><span class="linenos">3525</span></a>        <span class="k">assert</span> <span class="n">max_seg_size</span> <span class="o">==</span> <span class="n">max_seg_size_real</span><span class="p">,</span> <span class="s1">&#39;incorrect max_seg_size detected (got </span><span class="si">%d</span><span class="s1">, correct is </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">max_seg_size</span><span class="p">,</span> <span class="n">max_seg_size_real</span><span class="p">)</span>
</span><span id="L-3526"><a href="#L-3526"><span class="linenos">3526</span></a>
</span><span id="L-3527"><a href="#L-3527"><span class="linenos">3527</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found infs in &#39;&#39;values&#39;&#39;&quot;</span>
</span><span id="L-3528"><a href="#L-3528"><span class="linenos">3528</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found nans in &#39;&#39;values&#39;&#39;&quot;</span>
</span><span id="L-3529"><a href="#L-3529"><span class="linenos">3529</span></a>
</span><span id="L-3530"><a href="#L-3530"><span class="linenos">3530</span></a>    <span class="k">if</span> <span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">use_custom_cuda_extension_if_available</span><span class="p">:</span>
</span><span id="L-3531"><a href="#L-3531"><span class="linenos">3531</span></a>        <span class="n">lib_handle</span> <span class="o">=</span> <span class="n">load_custom_cuda_extension</span><span class="p">(</span><span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">,</span>
</span><span id="L-3532"><a href="#L-3532"><span class="linenos">3532</span></a>                                                <span class="n">report</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-3533"><a href="#L-3533"><span class="linenos">3533</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3534"><a href="#L-3534"><span class="linenos">3534</span></a>        <span class="n">lib_handle</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3535"><a href="#L-3535"><span class="linenos">3535</span></a>
</span><span id="L-3536"><a href="#L-3536"><span class="linenos">3536</span></a>    <span class="c1"># Calculate and return the segmented cumsum</span>
</span><span id="L-3537"><a href="#L-3537"><span class="linenos">3537</span></a>    <span class="k">if</span> <span class="n">lib_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-3538"><a href="#L-3538"><span class="linenos">3538</span></a>        <span class="k">return</span> <span class="n">segcumsum_cuda</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">,</span> <span class="n">in_place</span><span class="p">)</span>
</span><span id="L-3539"><a href="#L-3539"><span class="linenos">3539</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3540"><a href="#L-3540"><span class="linenos">3540</span></a>        <span class="k">return</span> <span class="n">segcumsum_torch</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">,</span> <span class="n">in_place</span><span class="p">)</span>
</span><span id="L-3541"><a href="#L-3541"><span class="linenos">3541</span></a>    
</span><span id="L-3542"><a href="#L-3542"><span class="linenos">3542</span></a><span class="c1"># torch implementation</span>
</span><span id="L-3543"><a href="#L-3543"><span class="linenos">3543</span></a><span class="k">def</span><span class="w"> </span><span class="nf">segcumsum_torch</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">,</span> <span class="n">in_place</span><span class="p">):</span>
</span><span id="L-3544"><a href="#L-3544"><span class="linenos">3544</span></a>    <span class="k">assert</span> <span class="n">values</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s1">&#39;in the segcumsum_torch implementation, &#39;&#39;values&#39;&#39; must be in contiguous format&#39;</span>
</span><span id="L-3545"><a href="#L-3545"><span class="linenos">3545</span></a>
</span><span id="L-3546"><a href="#L-3546"><span class="linenos">3546</span></a>    <span class="k">if</span> <span class="n">in_place</span><span class="p">:</span>
</span><span id="L-3547"><a href="#L-3547"><span class="linenos">3547</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">values</span>
</span><span id="L-3548"><a href="#L-3548"><span class="linenos">3548</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3549"><a href="#L-3549"><span class="linenos">3549</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
</span><span id="L-3550"><a href="#L-3550"><span class="linenos">3550</span></a>
</span><span id="L-3551"><a href="#L-3551"><span class="linenos">3551</span></a>    <span class="k">return</span> <span class="n">segcumsum_torch_main</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">)</span>
</span><span id="L-3552"><a href="#L-3552"><span class="linenos">3552</span></a>
</span><span id="L-3553"><a href="#L-3553"><span class="linenos">3553</span></a>
</span><span id="L-3554"><a href="#L-3554"><span class="linenos">3554</span></a><span class="c1"># main loop of torch implementation</span>
</span><span id="L-3555"><a href="#L-3555"><span class="linenos">3555</span></a><span class="c1"># Note: using torch jit here makes it empirically slower</span>
</span><span id="L-3556"><a href="#L-3556"><span class="linenos">3556</span></a><span class="k">def</span><span class="w"> </span><span class="nf">segcumsum_torch_main</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="L-3557"><a href="#L-3557"><span class="linenos">3557</span></a>    <span class="n">n</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-3558"><a href="#L-3558"><span class="linenos">3558</span></a>
</span><span id="L-3559"><a href="#L-3559"><span class="linenos">3559</span></a>    <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3560"><a href="#L-3560"><span class="linenos">3560</span></a>    <span class="k">while</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">max_seg_size</span><span class="p">:</span>
</span><span id="L-3561"><a href="#L-3561"><span class="linenos">3561</span></a>        <span class="n">stride</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">stride</span><span class="p">)</span>
</span><span id="L-3562"><a href="#L-3562"><span class="linenos">3562</span></a>        <span class="n">values</span><span class="p">[</span><span class="n">stride</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">segment_ids</span><span class="p">[</span><span class="n">stride</span><span class="p">:</span><span class="n">n</span><span class="p">]</span> <span class="o">==</span> <span class="n">segment_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">:(</span><span class="n">n</span><span class="o">-</span><span class="n">stride</span><span class="p">)])</span> <span class="o">*</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">:(</span><span class="n">n</span><span class="o">-</span><span class="n">stride</span><span class="p">)]</span>
</span><span id="L-3563"><a href="#L-3563"><span class="linenos">3563</span></a>    
</span><span id="L-3564"><a href="#L-3564"><span class="linenos">3564</span></a>    <span class="k">return</span> <span class="n">values</span>
</span><span id="L-3565"><a href="#L-3565"><span class="linenos">3565</span></a>
</span><span id="L-3566"><a href="#L-3566"><span class="linenos">3566</span></a>
</span><span id="L-3567"><a href="#L-3567"><span class="linenos">3567</span></a><span class="c1"># cuda implementation</span>
</span><span id="L-3568"><a href="#L-3568"><span class="linenos">3568</span></a><span class="k">def</span><span class="w"> </span><span class="nf">segcumsum_cuda</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">max_seg_size</span><span class="p">,</span> <span class="n">in_place</span><span class="p">):</span>
</span><span id="L-3569"><a href="#L-3569"><span class="linenos">3569</span></a>    <span class="c1"># Maximal number of CUDA threads to use per block.</span>
</span><span id="L-3570"><a href="#L-3570"><span class="linenos">3570</span></a>    <span class="c1"># Note: This is automatically capped by the maximal number supported by the architecture.</span>
</span><span id="L-3571"><a href="#L-3571"><span class="linenos">3571</span></a>    <span class="c1"># Set to an arbitrarily large number (e.g. 1e6) to determine automatically.</span>
</span><span id="L-3572"><a href="#L-3572"><span class="linenos">3572</span></a>    <span class="n">max_num_threads_per_block</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">)</span>
</span><span id="L-3573"><a href="#L-3573"><span class="linenos">3573</span></a>
</span><span id="L-3574"><a href="#L-3574"><span class="linenos">3574</span></a>    <span class="k">global</span> <span class="n">_lib_handle</span>
</span><span id="L-3575"><a href="#L-3575"><span class="linenos">3575</span></a>    <span class="n">libfsw_embedding</span> <span class="o">=</span> <span class="n">_lib_handle</span>
</span><span id="L-3576"><a href="#L-3576"><span class="linenos">3576</span></a>
</span><span id="L-3577"><a href="#L-3577"><span class="linenos">3577</span></a>    <span class="k">assert</span> <span class="n">libfsw_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;libfsw_embedding library is not loaded&#39;</span>
</span><span id="L-3578"><a href="#L-3578"><span class="linenos">3578</span></a>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">libfsw_embedding</span><span class="p">,</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">)</span> <span class="c1"># to silence PyCharm warning</span>
</span><span id="L-3579"><a href="#L-3579"><span class="linenos">3579</span></a>
</span><span id="L-3580"><a href="#L-3580"><span class="linenos">3580</span></a>    <span class="k">assert</span> <span class="n">values</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="s1">&#39;the tensor &#39;&#39;values&#39;&#39; must be on a CUDA device&#39;</span>
</span><span id="L-3581"><a href="#L-3581"><span class="linenos">3581</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="s1">&#39;the tensor &#39;&#39;segment_ids&#39;&#39; must be on a CUDA device&#39;</span>
</span><span id="L-3582"><a href="#L-3582"><span class="linenos">3582</span></a>    <span class="k">assert</span> <span class="n">segment_ids</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="s1">&#39;segment_ids must have int64 dtype&#39;</span>
</span><span id="L-3583"><a href="#L-3583"><span class="linenos">3583</span></a>    
</span><span id="L-3584"><a href="#L-3584"><span class="linenos">3584</span></a>    <span class="c1"># Process input data types</span>
</span><span id="L-3585"><a href="#L-3585"><span class="linenos">3585</span></a>    <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
</span><span id="L-3586"><a href="#L-3586"><span class="linenos">3586</span></a>        <span class="n">dtype_num</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3587"><a href="#L-3587"><span class="linenos">3587</span></a>        <span class="n">c_num_type</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_float</span>
</span><span id="L-3588"><a href="#L-3588"><span class="linenos">3588</span></a>    <span class="k">elif</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
</span><span id="L-3589"><a href="#L-3589"><span class="linenos">3589</span></a>        <span class="n">dtype_num</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="L-3590"><a href="#L-3590"><span class="linenos">3590</span></a>        <span class="n">c_num_type</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_double</span>
</span><span id="L-3591"><a href="#L-3591"><span class="linenos">3591</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3592"><a href="#L-3592"><span class="linenos">3592</span></a>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unsupported input_tensor dtype &#39;&#39;</span><span class="si">%s</span><span class="s2">&#39;&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
</span><span id="L-3593"><a href="#L-3593"><span class="linenos">3593</span></a>
</span><span id="L-3594"><a href="#L-3594"><span class="linenos">3594</span></a>    <span class="n">n</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span><span id="L-3595"><a href="#L-3595"><span class="linenos">3595</span></a>
</span><span id="L-3596"><a href="#L-3596"><span class="linenos">3596</span></a>    <span class="c1"># TODO: Consider calculating this number at initialization. Calling the function get_max_threads_per_block takes ~0.7 seconds.</span>
</span><span id="L-3597"><a href="#L-3597"><span class="linenos">3597</span></a>    <span class="c1"># Determine the maximal number of threads per block supported in the current CUDA device</span>
</span><span id="L-3598"><a href="#L-3598"><span class="linenos">3598</span></a>    <span class="c1"># cuda_max_threads_per_block = get_max_threads_per_block(values.device.index)</span>
</span><span id="L-3599"><a href="#L-3599"><span class="linenos">3599</span></a>    <span class="n">cuda_max_threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
</span><span id="L-3600"><a href="#L-3600"><span class="linenos">3600</span></a>
</span><span id="L-3601"><a href="#L-3601"><span class="linenos">3601</span></a>    <span class="c1"># Take the smallest multiple of 32 greater or equal to the input size, but no less than 64</span>
</span><span id="L-3602"><a href="#L-3602"><span class="linenos">3602</span></a>    <span class="n">num_threads_2</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">31</span><span class="p">)</span><span class="o">//</span><span class="mi">32</span><span class="p">)</span>
</span><span id="L-3603"><a href="#L-3603"><span class="linenos">3603</span></a>
</span><span id="L-3604"><a href="#L-3604"><span class="linenos">3604</span></a>    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_threads_2</span><span class="p">,</span> <span class="n">max_num_threads_per_block</span><span class="p">,</span> <span class="n">cuda_max_threads_per_block</span><span class="p">)</span>
</span><span id="L-3605"><a href="#L-3605"><span class="linenos">3605</span></a>    <span class="n">shared_memory_size</span> <span class="o">=</span> <span class="n">threads_per_block</span> <span class="o">*</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">sizeof</span><span class="p">(</span><span class="n">c_num_type</span><span class="p">)</span>
</span><span id="L-3606"><a href="#L-3606"><span class="linenos">3606</span></a>
</span><span id="L-3607"><a href="#L-3607"><span class="linenos">3607</span></a>    <span class="k">assert</span> <span class="n">threads_per_block</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;threads_per_block must be greater than 1&#39;</span>
</span><span id="L-3608"><a href="#L-3608"><span class="linenos">3608</span></a>
</span><span id="L-3609"><a href="#L-3609"><span class="linenos">3609</span></a>    <span class="c1"># Construct block hierarchy</span>
</span><span id="L-3610"><a href="#L-3610"><span class="linenos">3610</span></a>    <span class="n">tensor_sizes</span> <span class="o">=</span> <span class="p">[</span> <span class="n">n</span><span class="p">,</span> <span class="p">]</span>
</span><span id="L-3611"><a href="#L-3611"><span class="linenos">3611</span></a>    <span class="n">num_blocks</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-3612"><a href="#L-3612"><span class="linenos">3612</span></a>    <span class="n">max_seg_sizes</span> <span class="o">=</span> <span class="p">[</span> <span class="n">max_seg_size</span><span class="p">,</span> <span class="p">]</span>
</span><span id="L-3613"><a href="#L-3613"><span class="linenos">3613</span></a>    
</span><span id="L-3614"><a href="#L-3614"><span class="linenos">3614</span></a>    <span class="c1"># Stop dividing when the whole tensor fits in one block</span>
</span><span id="L-3615"><a href="#L-3615"><span class="linenos">3615</span></a>    <span class="k">while</span> <span class="n">tensor_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threads_per_block</span><span class="p">:</span>
</span><span id="L-3616"><a href="#L-3616"><span class="linenos">3616</span></a>        <span class="n">tensor_size_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensor_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>
</span><span id="L-3617"><a href="#L-3617"><span class="linenos">3617</span></a>        <span class="n">tensor_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_size_new</span><span class="p">)</span>
</span><span id="L-3618"><a href="#L-3618"><span class="linenos">3618</span></a>
</span><span id="L-3619"><a href="#L-3619"><span class="linenos">3619</span></a>        <span class="n">num_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_size_new</span><span class="p">)</span>
</span><span id="L-3620"><a href="#L-3620"><span class="linenos">3620</span></a>
</span><span id="L-3621"><a href="#L-3621"><span class="linenos">3621</span></a>        <span class="n">max_seg_size_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_seg_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>
</span><span id="L-3622"><a href="#L-3622"><span class="linenos">3622</span></a>        <span class="n">max_seg_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_seg_size_new</span><span class="p">)</span>
</span><span id="L-3623"><a href="#L-3623"><span class="linenos">3623</span></a>
</span><span id="L-3624"><a href="#L-3624"><span class="linenos">3624</span></a>    <span class="n">num_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-3625"><a href="#L-3625"><span class="linenos">3625</span></a>
</span><span id="L-3626"><a href="#L-3626"><span class="linenos">3626</span></a>    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-3627"><a href="#L-3627"><span class="linenos">3627</span></a>    <span class="n">id_tensors</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-3628"><a href="#L-3628"><span class="linenos">3628</span></a>
</span><span id="L-3629"><a href="#L-3629"><span class="linenos">3629</span></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">):</span>
</span><span id="L-3630"><a href="#L-3630"><span class="linenos">3630</span></a>        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3631"><a href="#L-3631"><span class="linenos">3631</span></a>            <span class="k">if</span> <span class="n">in_place</span><span class="p">:</span>
</span><span id="L-3632"><a href="#L-3632"><span class="linenos">3632</span></a>                <span class="n">output_tensor_new</span> <span class="o">=</span> <span class="n">values</span>
</span><span id="L-3633"><a href="#L-3633"><span class="linenos">3633</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-3634"><a href="#L-3634"><span class="linenos">3634</span></a>                <span class="n">output_tensor_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
</span><span id="L-3635"><a href="#L-3635"><span class="linenos">3635</span></a>
</span><span id="L-3636"><a href="#L-3636"><span class="linenos">3636</span></a>            <span class="n">id_tensor_new</span> <span class="o">=</span> <span class="n">segment_ids</span>
</span><span id="L-3637"><a href="#L-3637"><span class="linenos">3637</span></a>
</span><span id="L-3638"><a href="#L-3638"><span class="linenos">3638</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3639"><a href="#L-3639"><span class="linenos">3639</span></a>            <span class="n">output_tensor_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">s</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
</span><span id="L-3640"><a href="#L-3640"><span class="linenos">3640</span></a>            <span class="n">id_tensor_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">s</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">segment_ids</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">segment_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
</span><span id="L-3641"><a href="#L-3641"><span class="linenos">3641</span></a>
</span><span id="L-3642"><a href="#L-3642"><span class="linenos">3642</span></a>        <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_tensor_new</span><span class="p">)</span>
</span><span id="L-3643"><a href="#L-3643"><span class="linenos">3643</span></a>        <span class="n">id_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">id_tensor_new</span><span class="p">)</span>
</span><span id="L-3644"><a href="#L-3644"><span class="linenos">3644</span></a>
</span><span id="L-3645"><a href="#L-3645"><span class="linenos">3645</span></a>    <span class="c1"># Define the kernel signatures</span>
</span><span id="L-3646"><a href="#L-3646"><span class="linenos">3646</span></a>    <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">segcumsum_wrapper</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-3647"><a href="#L-3647"><span class="linenos">3647</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># dtype_num</span>
</span><span id="L-3648"><a href="#L-3648"><span class="linenos">3648</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># values input/output pointer</span>
</span><span id="L-3649"><a href="#L-3649"><span class="linenos">3649</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># segment_ids pointer</span>
</span><span id="L-3650"><a href="#L-3650"><span class="linenos">3650</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># size</span>
</span><span id="L-3651"><a href="#L-3651"><span class="linenos">3651</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># max_seg_size</span>
</span><span id="L-3652"><a href="#L-3652"><span class="linenos">3652</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># block sums output pointer</span>
</span><span id="L-3653"><a href="#L-3653"><span class="linenos">3653</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># block last ids output pointer</span>
</span><span id="L-3654"><a href="#L-3654"><span class="linenos">3654</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_bool</span><span class="p">,</span>    <span class="c1"># return_next_level boolean</span>
</span><span id="L-3655"><a href="#L-3655"><span class="linenos">3655</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># blocks</span>
</span><span id="L-3656"><a href="#L-3656"><span class="linenos">3656</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># threads_per_block</span>
</span><span id="L-3657"><a href="#L-3657"><span class="linenos">3657</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_size_t</span>   <span class="c1"># shared_memory_size</span>
</span><span id="L-3658"><a href="#L-3658"><span class="linenos">3658</span></a>    <span class="p">]</span>
</span><span id="L-3659"><a href="#L-3659"><span class="linenos">3659</span></a>    <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">segcumsum_wrapper</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3660"><a href="#L-3660"><span class="linenos">3660</span></a>
</span><span id="L-3661"><a href="#L-3661"><span class="linenos">3661</span></a>    <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">add_block_sums_wrapper</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-3662"><a href="#L-3662"><span class="linenos">3662</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># dtype_num</span>
</span><span id="L-3663"><a href="#L-3663"><span class="linenos">3663</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># output pointer</span>
</span><span id="L-3664"><a href="#L-3664"><span class="linenos">3664</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># block_sums pointer</span>
</span><span id="L-3665"><a href="#L-3665"><span class="linenos">3665</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># segment_ids pointer</span>
</span><span id="L-3666"><a href="#L-3666"><span class="linenos">3666</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">,</span>  <span class="c1"># block_last_id pointer</span>
</span><span id="L-3667"><a href="#L-3667"><span class="linenos">3667</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># size</span>
</span><span id="L-3668"><a href="#L-3668"><span class="linenos">3668</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>     <span class="c1"># blocks</span>
</span><span id="L-3669"><a href="#L-3669"><span class="linenos">3669</span></a>        <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span>      <span class="c1"># threads_per_block</span>
</span><span id="L-3670"><a href="#L-3670"><span class="linenos">3670</span></a>    <span class="p">]</span>
</span><span id="L-3671"><a href="#L-3671"><span class="linenos">3671</span></a>    <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">add_block_sums_wrapper</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-3672"><a href="#L-3672"><span class="linenos">3672</span></a>
</span><span id="L-3673"><a href="#L-3673"><span class="linenos">3673</span></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">):</span>
</span><span id="L-3674"><a href="#L-3674"><span class="linenos">3674</span></a>        <span class="n">return_next_level</span> <span class="o">=</span> <span class="p">(</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3675"><a href="#L-3675"><span class="linenos">3675</span></a>
</span><span id="L-3676"><a href="#L-3676"><span class="linenos">3676</span></a>        <span class="c1"># Launch the segcumsum_wrapper</span>
</span><span id="L-3677"><a href="#L-3677"><span class="linenos">3677</span></a>        <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">segcumsum_wrapper</span><span class="p">(</span>
</span><span id="L-3678"><a href="#L-3678"><span class="linenos">3678</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">dtype_num</span><span class="p">),</span>
</span><span id="L-3679"><a href="#L-3679"><span class="linenos">3679</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3680"><a href="#L-3680"><span class="linenos">3680</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">id_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3681"><a href="#L-3681"><span class="linenos">3681</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>            
</span><span id="L-3682"><a href="#L-3682"><span class="linenos">3682</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">max_seg_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
</span><span id="L-3683"><a href="#L-3683"><span class="linenos">3683</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="n">return_next_level</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-3684"><a href="#L-3684"><span class="linenos">3684</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">id_tensors</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="n">return_next_level</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="L-3685"><a href="#L-3685"><span class="linenos">3685</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_bool</span><span class="p">(</span> <span class="n">return_next_level</span>  <span class="p">),</span>
</span><span id="L-3686"><a href="#L-3686"><span class="linenos">3686</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
</span><span id="L-3687"><a href="#L-3687"><span class="linenos">3687</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">threads_per_block</span><span class="p">),</span>
</span><span id="L-3688"><a href="#L-3688"><span class="linenos">3688</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_size_t</span><span class="p">(</span><span class="n">shared_memory_size</span><span class="p">)</span>
</span><span id="L-3689"><a href="#L-3689"><span class="linenos">3689</span></a>        <span class="p">)</span>
</span><span id="L-3690"><a href="#L-3690"><span class="linenos">3690</span></a>
</span><span id="L-3691"><a href="#L-3691"><span class="linenos">3691</span></a>
</span><span id="L-3692"><a href="#L-3692"><span class="linenos">3692</span></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
</span><span id="L-3693"><a href="#L-3693"><span class="linenos">3693</span></a>
</span><span id="L-3694"><a href="#L-3694"><span class="linenos">3694</span></a>        <span class="c1"># Launch the add_block_sums_wrapper</span>
</span><span id="L-3695"><a href="#L-3695"><span class="linenos">3695</span></a>        <span class="n">libfsw_embedding</span><span class="o">.</span><span class="n">add_block_sums_wrapper</span><span class="p">(</span>
</span><span id="L-3696"><a href="#L-3696"><span class="linenos">3696</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">dtype_num</span><span class="p">),</span>
</span><span id="L-3697"><a href="#L-3697"><span class="linenos">3697</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3698"><a href="#L-3698"><span class="linenos">3698</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3699"><a href="#L-3699"><span class="linenos">3699</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">id_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3700"><a href="#L-3700"><span class="linenos">3700</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">id_tensors</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()),</span>
</span><span id="L-3701"><a href="#L-3701"><span class="linenos">3701</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
</span><span id="L-3702"><a href="#L-3702"><span class="linenos">3702</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
</span><span id="L-3703"><a href="#L-3703"><span class="linenos">3703</span></a>            <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">(</span><span class="n">threads_per_block</span><span class="p">)</span>
</span><span id="L-3704"><a href="#L-3704"><span class="linenos">3704</span></a>        <span class="p">)</span>
</span><span id="L-3705"><a href="#L-3705"><span class="linenos">3705</span></a>
</span><span id="L-3706"><a href="#L-3706"><span class="linenos">3706</span></a>    <span class="k">return</span> <span class="n">output_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-3707"><a href="#L-3707"><span class="linenos">3707</span></a>
</span><span id="L-3708"><a href="#L-3708"><span class="linenos">3708</span></a>
</span><span id="L-3709"><a href="#L-3709"><span class="linenos">3709</span></a><span class="c1"># This is a slow alternative of segcumsum() to verify the correctness of the results</span>
</span><span id="L-3710"><a href="#L-3710"><span class="linenos">3710</span></a><span class="k">def</span><span class="w"> </span><span class="nf">segcumsum_slow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">):</span>
</span><span id="L-3711"><a href="#L-3711"><span class="linenos">3711</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-3712"><a href="#L-3712"><span class="linenos">3712</span></a>
</span><span id="L-3713"><a href="#L-3713"><span class="linenos">3713</span></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
</span><span id="L-3714"><a href="#L-3714"><span class="linenos">3714</span></a>        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-3715"><a href="#L-3715"><span class="linenos">3715</span></a>            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="L-3716"><a href="#L-3716"><span class="linenos">3716</span></a>        <span class="k">elif</span> <span class="n">segment_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">segment_ids</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
</span><span id="L-3717"><a href="#L-3717"><span class="linenos">3717</span></a>            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="L-3718"><a href="#L-3718"><span class="linenos">3718</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3719"><a href="#L-3719"><span class="linenos">3719</span></a>            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="L-3720"><a href="#L-3720"><span class="linenos">3720</span></a>    
</span><span id="L-3721"><a href="#L-3721"><span class="linenos">3721</span></a>    <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3722"><a href="#L-3722"><span class="linenos">3722</span></a>
</span><span id="L-3723"><a href="#L-3723"><span class="linenos">3723</span></a>
</span><span id="L-3724"><a href="#L-3724"><span class="linenos">3724</span></a><span class="c1"># Returns the maximal number of threads per block supported by the CUDA device with the given index</span>
</span><span id="L-3725"><a href="#L-3725"><span class="linenos">3725</span></a><span class="c1"># Note: This function may take ~0.7 seconds to run</span>
</span><span id="L-3726"><a href="#L-3726"><span class="linenos">3726</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_max_threads_per_block</span><span class="p">(</span><span class="n">device_index</span><span class="p">):</span>
</span><span id="L-3727"><a href="#L-3727"><span class="linenos">3727</span></a>    <span class="k">global</span> <span class="n">_lib_handle</span>
</span><span id="L-3728"><a href="#L-3728"><span class="linenos">3728</span></a>    <span class="k">assert</span> <span class="n">_lib_handle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-3729"><a href="#L-3729"><span class="linenos">3729</span></a>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_lib_handle</span><span class="p">,</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">)</span> <span class="c1"># to silence PyCharm warning</span>
</span><span id="L-3730"><a href="#L-3730"><span class="linenos">3730</span></a>    <span class="n">_lib_handle</span><span class="o">.</span><span class="n">get_max_threads_per_block</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span> <span class="p">]</span>
</span><span id="L-3731"><a href="#L-3731"><span class="linenos">3731</span></a>    <span class="n">_lib_handle</span><span class="o">.</span><span class="n">get_max_threads_per_block</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span>
</span><span id="L-3732"><a href="#L-3732"><span class="linenos">3732</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">_lib_handle</span><span class="o">.</span><span class="n">get_max_threads_per_block</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">(</span><span class="n">device_index</span><span class="p">))</span>
</span><span id="L-3733"><a href="#L-3733"><span class="linenos">3733</span></a>    <span class="k">return</span> <span class="n">out</span>
</span><span id="L-3734"><a href="#L-3734"><span class="linenos">3734</span></a>
</span><span id="L-3735"><a href="#L-3735"><span class="linenos">3735</span></a>
</span><span id="L-3736"><a href="#L-3736"><span class="linenos">3736</span></a>
</span><span id="L-3737"><a href="#L-3737"><span class="linenos">3737</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3738"><a href="#L-3738"><span class="linenos">3738</span></a><span class="c1">##                                      Mutual Coherence Minimization                                      ##</span>
</span><span id="L-3739"><a href="#L-3739"><span class="linenos">3739</span></a><span class="c1">#############################################################################################################</span>
</span><span id="L-3740"><a href="#L-3740"><span class="linenos">3740</span></a>
</span><span id="L-3741"><a href="#L-3741"><span class="linenos">3741</span></a>
</span><span id="L-3742"><a href="#L-3742"><span class="linenos">3742</span></a><span class="k">def</span><span class="w"> </span><span class="nf">minimize_mutual_coherence</span><span class="p">(</span><span class="n">X_init</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span><span id="L-3743"><a href="#L-3743"><span class="linenos">3743</span></a>    <span class="n">step_size_init</span> <span class="o">=</span> <span class="mi">2000</span>
</span><span id="L-3744"><a href="#L-3744"><span class="linenos">3744</span></a>    <span class="n">nIter_max</span> <span class="o">=</span> <span class="mi">1000</span>
</span><span id="L-3745"><a href="#L-3745"><span class="linenos">3745</span></a>    <span class="n">improvement_thresh</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># Use 1e-6 for more thoroughness</span>
</span><span id="L-3746"><a href="#L-3746"><span class="linenos">3746</span></a>
</span><span id="L-3747"><a href="#L-3747"><span class="linenos">3747</span></a>    <span class="n">p_vals</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">2000</span><span class="p">,</span><span class="mi">5000</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">2e4</span><span class="p">,</span> <span class="mf">5e4</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e7</span><span class="p">,</span> <span class="mf">1e8</span><span class="p">,</span> <span class="mf">1e9</span><span class="p">,</span> <span class="mf">1e10</span><span class="p">,</span> <span class="mf">1e11</span><span class="p">,</span> <span class="mf">1e12</span><span class="p">,</span> <span class="mf">1e13</span><span class="p">]</span>
</span><span id="L-3748"><a href="#L-3748"><span class="linenos">3748</span></a>
</span><span id="L-3749"><a href="#L-3749"><span class="linenos">3749</span></a>    <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_init</span>
</span><span id="L-3750"><a href="#L-3750"><span class="linenos">3750</span></a>    
</span><span id="L-3751"><a href="#L-3751"><span class="linenos">3751</span></a>    <span class="c1">#n = X_init.shape[0]</span>
</span><span id="L-3752"><a href="#L-3752"><span class="linenos">3752</span></a>    <span class="n">X_curr</span> <span class="o">=</span> <span class="n">X_init</span>
</span><span id="L-3753"><a href="#L-3753"><span class="linenos">3753</span></a>    <span class="n">X_curr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">X_curr</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-3754"><a href="#L-3754"><span class="linenos">3754</span></a>    <span class="c1">#mu_init = calc_mu_from_G(calc_G(X_curr))</span>
</span><span id="L-3755"><a href="#L-3755"><span class="linenos">3755</span></a>    
</span><span id="L-3756"><a href="#L-3756"><span class="linenos">3756</span></a>    <span class="k">for</span> <span class="n">ip</span><span class="p">,</span> <span class="n">p_curr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_vals</span><span class="p">):</span>
</span><span id="L-3757"><a href="#L-3757"><span class="linenos">3757</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">=== Optimizing with p = </span><span class="si">%g</span><span class="s1"> (</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">) ===&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">p_curr</span><span class="p">,</span> <span class="n">ip</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_vals</span><span class="p">))</span> <span class="p">)</span>
</span><span id="L-3758"><a href="#L-3758"><span class="linenos">3758</span></a>        <span class="n">X_curr</span><span class="p">,</span> <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">minimize_mutual_coherence_p</span><span class="p">(</span><span class="n">X_curr</span><span class="p">,</span> <span class="n">p_curr</span><span class="p">,</span> <span class="n">step_size_init</span><span class="o">=</span><span class="n">step_size_curr</span><span class="p">,</span> <span class="n">improvement_thresh</span><span class="o">=</span><span class="n">improvement_thresh</span><span class="p">,</span> <span class="n">nIter_max</span><span class="o">=</span><span class="n">nIter_max</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-3759"><a href="#L-3759"><span class="linenos">3759</span></a>
</span><span id="L-3760"><a href="#L-3760"><span class="linenos">3760</span></a>        <span class="n">mu_curr</span> <span class="o">=</span> <span class="n">calc_mu_from_G</span><span class="p">(</span><span class="n">calc_G</span><span class="p">(</span><span class="n">X_curr</span><span class="p">))</span>
</span><span id="L-3761"><a href="#L-3761"><span class="linenos">3761</span></a>        <span class="c1">#qprintln(&#39;Relative improvement vs. init: %g&#39; % ((mu_init-mu_curr)/(1-mu_init)))</span>
</span><span id="L-3762"><a href="#L-3762"><span class="linenos">3762</span></a>
</span><span id="L-3763"><a href="#L-3763"><span class="linenos">3763</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Incoherence: </span><span class="si">%g</span><span class="s1">  Min. pairwise dist: </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span> <span class="mf">1.0</span><span class="o">-</span><span class="n">mu_curr</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">mu_curr</span><span class="o">.</span><span class="n">item</span><span class="p">())))</span> <span class="p">)</span> <span class="p">)</span>
</span><span id="L-3764"><a href="#L-3764"><span class="linenos">3764</span></a>    <span class="k">return</span> <span class="n">X_curr</span>
</span><span id="L-3765"><a href="#L-3765"><span class="linenos">3765</span></a>
</span><span id="L-3766"><a href="#L-3766"><span class="linenos">3766</span></a>
</span><span id="L-3767"><a href="#L-3767"><span class="linenos">3767</span></a>
</span><span id="L-3768"><a href="#L-3768"><span class="linenos">3768</span></a><span class="k">def</span><span class="w"> </span><span class="nf">minimize_mutual_coherence_p</span><span class="p">(</span><span class="n">X_init</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">step_size_init</span><span class="p">,</span> <span class="n">improvement_thresh</span><span class="p">,</span> <span class="n">nIter_max</span><span class="p">,</span> <span class="n">report</span><span class="p">):</span>
</span><span id="L-3769"><a href="#L-3769"><span class="linenos">3769</span></a>    <span class="c1"># Note: X_init must be normalized to unit rows</span>
</span><span id="L-3770"><a href="#L-3770"><span class="linenos">3770</span></a>
</span><span id="L-3771"><a href="#L-3771"><span class="linenos">3771</span></a>    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span><span id="L-3772"><a href="#L-3772"><span class="linenos">3772</span></a>
</span><span id="L-3773"><a href="#L-3773"><span class="linenos">3773</span></a>    <span class="c1"># Parameters</span>
</span><span id="L-3774"><a href="#L-3774"><span class="linenos">3774</span></a>    <span class="n">step_size_min</span> <span class="o">=</span> <span class="mf">1e-5</span>
</span><span id="L-3775"><a href="#L-3775"><span class="linenos">3775</span></a>    <span class="n">step_size_max</span> <span class="o">=</span> <span class="mf">1e10</span>
</span><span id="L-3776"><a href="#L-3776"><span class="linenos">3776</span></a>    <span class="n">max_num_low_improvements</span> <span class="o">=</span> <span class="mi">5</span>
</span><span id="L-3777"><a href="#L-3777"><span class="linenos">3777</span></a>    <span class="n">step_decrease_factor</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="L-3778"><a href="#L-3778"><span class="linenos">3778</span></a>
</span><span id="L-3779"><a href="#L-3779"><span class="linenos">3779</span></a>    <span class="k">assert</span> <span class="n">p</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;p must be greater or equal to 2&quot;</span>
</span><span id="L-3780"><a href="#L-3780"><span class="linenos">3780</span></a>    <span class="k">assert</span> <span class="n">step_size_init</span> <span class="o">&gt;=</span> <span class="n">step_size_min</span><span class="p">,</span> <span class="s2">&quot;Initial step size below minium&quot;</span>
</span><span id="L-3781"><a href="#L-3781"><span class="linenos">3781</span></a>    <span class="k">assert</span> <span class="n">step_size_init</span> <span class="o">&lt;=</span> <span class="n">step_size_max</span><span class="p">,</span> <span class="s2">&quot;Initial step size above maximum&quot;</span>
</span><span id="L-3782"><a href="#L-3782"><span class="linenos">3782</span></a>
</span><span id="L-3783"><a href="#L-3783"><span class="linenos">3783</span></a>    <span class="c1"># Initialization</span>
</span><span id="L-3784"><a href="#L-3784"><span class="linenos">3784</span></a>    <span class="n">n</span> <span class="o">=</span> <span class="n">X_init</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-3785"><a href="#L-3785"><span class="linenos">3785</span></a>
</span><span id="L-3786"><a href="#L-3786"><span class="linenos">3786</span></a>    <span class="n">onevec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X_init</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X_init</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-3787"><a href="#L-3787"><span class="linenos">3787</span></a>
</span><span id="L-3788"><a href="#L-3788"><span class="linenos">3788</span></a>    <span class="n">mu_init</span> <span class="o">=</span> <span class="n">calc_mu_from_G</span><span class="p">(</span><span class="n">calc_G</span><span class="p">(</span><span class="n">X_init</span><span class="p">))</span>
</span><span id="L-3789"><a href="#L-3789"><span class="linenos">3789</span></a>
</span><span id="L-3790"><a href="#L-3790"><span class="linenos">3790</span></a>    <span class="c1">## Initialize first iteration</span>
</span><span id="L-3791"><a href="#L-3791"><span class="linenos">3791</span></a>    <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_init</span>
</span><span id="L-3792"><a href="#L-3792"><span class="linenos">3792</span></a>
</span><span id="L-3793"><a href="#L-3793"><span class="linenos">3793</span></a>    <span class="c1"># The first step size is chosen as follows: Start from step_size_init.</span>
</span><span id="L-3794"><a href="#L-3794"><span class="linenos">3794</span></a>    <span class="c1"># If at the first iteration this step size yields an objective decrease,</span>
</span><span id="L-3795"><a href="#L-3795"><span class="linenos">3795</span></a>    <span class="c1"># iteratively increase the step size until the objective increases.</span>
</span><span id="L-3796"><a href="#L-3796"><span class="linenos">3796</span></a>    <span class="c1"># Then choose the best among the tested step sizes and use it. This</span>
</span><span id="L-3797"><a href="#L-3797"><span class="linenos">3797</span></a>    <span class="c1"># prevents getting stuck with too low a step size throughout the</span>
</span><span id="L-3798"><a href="#L-3798"><span class="linenos">3798</span></a>    <span class="c1"># optimization process.</span>
</span><span id="L-3799"><a href="#L-3799"><span class="linenos">3799</span></a>    <span class="n">step_size_init_best</span> <span class="o">=</span> <span class="n">step_size_init</span>
</span><span id="L-3800"><a href="#L-3800"><span class="linenos">3800</span></a>    <span class="n">obj_best_at_step_init_seek</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
</span><span id="L-3801"><a href="#L-3801"><span class="linenos">3801</span></a>    <span class="n">finished_step_size_init</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="L-3802"><a href="#L-3802"><span class="linenos">3802</span></a>
</span><span id="L-3803"><a href="#L-3803"><span class="linenos">3803</span></a>    <span class="n">low_improvement_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3804"><a href="#L-3804"><span class="linenos">3804</span></a>
</span><span id="L-3805"><a href="#L-3805"><span class="linenos">3805</span></a>    <span class="c1"># These four have to be maintained together and consistent with each</span>
</span><span id="L-3806"><a href="#L-3806"><span class="linenos">3806</span></a>    <span class="c1"># other.</span>
</span><span id="L-3807"><a href="#L-3807"><span class="linenos">3807</span></a>    <span class="n">X_curr</span> <span class="o">=</span> <span class="n">X_init</span>
</span><span id="L-3808"><a href="#L-3808"><span class="linenos">3808</span></a>    <span class="n">G_curr</span> <span class="o">=</span> <span class="n">calc_G</span><span class="p">(</span><span class="n">X_curr</span><span class="p">)</span> <span class="c1"># The Gram matrix of X with its main diagonal annihilated.</span>
</span><span id="L-3809"><a href="#L-3809"><span class="linenos">3809</span></a>    <span class="n">mu_curr</span><span class="p">,</span> <span class="n">obj_curr</span> <span class="o">=</span> <span class="n">eval_G</span><span class="p">(</span><span class="n">G_curr</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</span><span id="L-3810"><a href="#L-3810"><span class="linenos">3810</span></a>
</span><span id="L-3811"><a href="#L-3811"><span class="linenos">3811</span></a>    <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;#</span><span class="si">%.2d</span><span class="s1">:  Objective: </span><span class="si">%g</span><span class="s1">  Surrogate incoherence: </span><span class="si">%g</span><span class="s1">  Step size: </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">obj_curr</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mf">1.0</span><span class="o">-</span><span class="n">obj_curr</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">step_size_curr</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3812"><a href="#L-3812"><span class="linenos">3812</span></a>
</span><span id="L-3813"><a href="#L-3813"><span class="linenos">3813</span></a>    <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)),</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">p</span> <span class="p">)</span>
</span><span id="L-3814"><a href="#L-3814"><span class="linenos">3814</span></a>
</span><span id="L-3815"><a href="#L-3815"><span class="linenos">3815</span></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nIter_max</span><span class="o">+</span><span class="mi">2</span><span class="p">):</span>
</span><span id="L-3816"><a href="#L-3816"><span class="linenos">3816</span></a>        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">nIter_max</span><span class="p">:</span>
</span><span id="L-3817"><a href="#L-3817"><span class="linenos">3817</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Reached maximal number of iterations. Breaking.&#39;</span><span class="p">)</span>
</span><span id="L-3818"><a href="#L-3818"><span class="linenos">3818</span></a>            <span class="k">break</span>
</span><span id="L-3819"><a href="#L-3819"><span class="linenos">3819</span></a>
</span><span id="L-3820"><a href="#L-3820"><span class="linenos">3820</span></a>        <span class="c1"># Calculate gradient at current solution</span>
</span><span id="L-3821"><a href="#L-3821"><span class="linenos">3821</span></a>        <span class="c1"># For numerical safety, G is normalized so that its</span>
</span><span id="L-3822"><a href="#L-3822"><span class="linenos">3822</span></a>        <span class="c1"># largest-magnitude entry equals 1.</span>
</span><span id="L-3823"><a href="#L-3823"><span class="linenos">3823</span></a>        <span class="n">G_normalized</span> <span class="o">=</span> <span class="n">G_curr</span> <span class="o">/</span> <span class="n">mu_curr</span>
</span><span id="L-3824"><a href="#L-3824"><span class="linenos">3824</span></a>        <span class="n">sum_offdiags_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">G_normalized</span><span class="p">),</span> <span class="n">p</span><span class="p">))</span>
</span><span id="L-3825"><a href="#L-3825"><span class="linenos">3825</span></a>        
</span><span id="L-3826"><a href="#L-3826"><span class="linenos">3826</span></a>        <span class="n">grad_curr</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">sum_offdiags_norm</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span> <span class="n">G_normalized</span> <span class="p">),</span> <span class="n">p</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">G_normalized</span><span class="p">)</span> <span class="p">)</span> <span class="o">@</span> <span class="n">X_curr</span> <span class="o">-</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">G_normalized</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">mu_curr</span><span class="o">*</span><span class="n">onevec</span><span class="p">)</span> <span class="p">)</span> <span class="o">*</span> <span class="n">X_curr</span> <span class="p">)</span>
</span><span id="L-3827"><a href="#L-3827"><span class="linenos">3827</span></a>
</span><span id="L-3828"><a href="#L-3828"><span class="linenos">3828</span></a>        <span class="c1"># Calculate and evaluate new candidate solution</span>
</span><span id="L-3829"><a href="#L-3829"><span class="linenos">3829</span></a>        <span class="n">X_new</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">X_curr</span> <span class="o">-</span> <span class="n">step_size_curr</span> <span class="o">*</span> <span class="n">grad_curr</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="L-3830"><a href="#L-3830"><span class="linenos">3830</span></a>        <span class="n">G_new</span> <span class="o">=</span> <span class="n">calc_G</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</span><span id="L-3831"><a href="#L-3831"><span class="linenos">3831</span></a>        <span class="n">mu_new</span><span class="p">,</span> <span class="n">obj_new</span> <span class="o">=</span> <span class="n">eval_G</span><span class="p">(</span><span class="n">G_new</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</span><span id="L-3832"><a href="#L-3832"><span class="linenos">3832</span></a>
</span><span id="L-3833"><a href="#L-3833"><span class="linenos">3833</span></a>        <span class="c1"># If the objective does not improve</span>
</span><span id="L-3834"><a href="#L-3834"><span class="linenos">3834</span></a>        <span class="k">if</span> <span class="n">obj_new</span> <span class="o">&gt;=</span> <span class="n">obj_curr</span><span class="p">:</span>
</span><span id="L-3835"><a href="#L-3835"><span class="linenos">3835</span></a>            <span class="k">if</span> <span class="n">finished_step_size_init</span><span class="p">:</span>
</span><span id="L-3836"><a href="#L-3836"><span class="linenos">3836</span></a>                <span class="c1"># Decrease step size                </span>
</span><span id="L-3837"><a href="#L-3837"><span class="linenos">3837</span></a>                <span class="k">if</span> <span class="n">step_size_curr</span> <span class="o">*</span> <span class="n">step_decrease_factor</span> <span class="o">&lt;</span> <span class="n">step_size_min</span><span class="p">:</span>
</span><span id="L-3838"><a href="#L-3838"><span class="linenos">3838</span></a>                    <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;#</span><span class="si">%.2d</span><span class="s1">:  Objective does not improve at minimal step size. Breaking.&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="p">)</span>
</span><span id="L-3839"><a href="#L-3839"><span class="linenos">3839</span></a>                    <span class="k">break</span>
</span><span id="L-3840"><a href="#L-3840"><span class="linenos">3840</span></a>
</span><span id="L-3841"><a href="#L-3841"><span class="linenos">3841</span></a>                <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_curr</span> <span class="o">*</span> <span class="n">step_decrease_factor</span>
</span><span id="L-3842"><a href="#L-3842"><span class="linenos">3842</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-3843"><a href="#L-3843"><span class="linenos">3843</span></a>                <span class="c1"># If we&#39;re still seeking the first step size, stop and pick</span>
</span><span id="L-3844"><a href="#L-3844"><span class="linenos">3844</span></a>                <span class="c1"># the best step size so far.</span>
</span><span id="L-3845"><a href="#L-3845"><span class="linenos">3845</span></a>                <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_init_best</span>
</span><span id="L-3846"><a href="#L-3846"><span class="linenos">3846</span></a>                <span class="n">finished_step_size_init</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-3847"><a href="#L-3847"><span class="linenos">3847</span></a>            
</span><span id="L-3848"><a href="#L-3848"><span class="linenos">3848</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;#</span><span class="si">%.2d</span><span class="s1">:  Decreaseing step size: </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">step_size_curr</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3849"><a href="#L-3849"><span class="linenos">3849</span></a>            <span class="k">continue</span>
</span><span id="L-3850"><a href="#L-3850"><span class="linenos">3850</span></a>
</span><span id="L-3851"><a href="#L-3851"><span class="linenos">3851</span></a>        <span class="c1"># --&gt; If we&#39;re here, the objective has improved.</span>
</span><span id="L-3852"><a href="#L-3852"><span class="linenos">3852</span></a>        
</span><span id="L-3853"><a href="#L-3853"><span class="linenos">3853</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">finished_step_size_init</span><span class="p">:</span>
</span><span id="L-3854"><a href="#L-3854"><span class="linenos">3854</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">obj_new</span> <span class="o">&lt;</span> <span class="n">obj_best_at_step_init_seek</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">step_size_curr</span> <span class="o">/</span> <span class="n">step_decrease_factor</span> <span class="o">&lt;=</span> <span class="n">step_size_max</span><span class="p">):</span>
</span><span id="L-3855"><a href="#L-3855"><span class="linenos">3855</span></a>                <span class="n">obj_best_at_step_init_seek</span> <span class="o">=</span> <span class="n">obj_new</span>
</span><span id="L-3856"><a href="#L-3856"><span class="linenos">3856</span></a>                <span class="n">step_size_init_best</span> <span class="o">=</span> <span class="n">step_size_curr</span>
</span><span id="L-3857"><a href="#L-3857"><span class="linenos">3857</span></a>                <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_curr</span> <span class="o">/</span> <span class="n">step_decrease_factor</span>
</span><span id="L-3858"><a href="#L-3858"><span class="linenos">3858</span></a>
</span><span id="L-3859"><a href="#L-3859"><span class="linenos">3859</span></a>                <span class="c1"># Save the new best solution for later backtracking</span>
</span><span id="L-3860"><a href="#L-3860"><span class="linenos">3860</span></a>                <span class="n">X_stepinit</span> <span class="o">=</span> <span class="n">X_new</span>
</span><span id="L-3861"><a href="#L-3861"><span class="linenos">3861</span></a>                <span class="n">G_stepinit</span> <span class="o">=</span> <span class="n">G_new</span>
</span><span id="L-3862"><a href="#L-3862"><span class="linenos">3862</span></a>                <span class="n">obj_stepinit</span> <span class="o">=</span> <span class="n">obj_new</span>
</span><span id="L-3863"><a href="#L-3863"><span class="linenos">3863</span></a>                <span class="n">mu_stepinit</span> <span class="o">=</span> <span class="n">mu_new</span>
</span><span id="L-3864"><a href="#L-3864"><span class="linenos">3864</span></a>
</span><span id="L-3865"><a href="#L-3865"><span class="linenos">3865</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;#</span><span class="si">%.2d</span><span class="s1">:  Trying larger step size: </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">step_size_curr</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3866"><a href="#L-3866"><span class="linenos">3866</span></a>                <span class="k">continue</span>
</span><span id="L-3867"><a href="#L-3867"><span class="linenos">3867</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="L-3868"><a href="#L-3868"><span class="linenos">3868</span></a>                <span class="c1"># If we&#39;re seeking the best first step, on the first time</span>
</span><span id="L-3869"><a href="#L-3869"><span class="linenos">3869</span></a>                <span class="c1"># that increasing the step does not improve the objective,</span>
</span><span id="L-3870"><a href="#L-3870"><span class="linenos">3870</span></a>                <span class="c1"># stop increasing and backtrack to the best solution so</span>
</span><span id="L-3871"><a href="#L-3871"><span class="linenos">3871</span></a>                <span class="c1"># far.</span>
</span><span id="L-3872"><a href="#L-3872"><span class="linenos">3872</span></a>                <span class="n">step_size_curr</span> <span class="o">=</span> <span class="n">step_size_init_best</span>
</span><span id="L-3873"><a href="#L-3873"><span class="linenos">3873</span></a>                <span class="n">finished_step_size_init</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-3874"><a href="#L-3874"><span class="linenos">3874</span></a>
</span><span id="L-3875"><a href="#L-3875"><span class="linenos">3875</span></a>                <span class="c1"># Backtrack to best candidate solution</span>
</span><span id="L-3876"><a href="#L-3876"><span class="linenos">3876</span></a>                <span class="n">X_new</span> <span class="o">=</span> <span class="n">X_stepinit</span>
</span><span id="L-3877"><a href="#L-3877"><span class="linenos">3877</span></a>                <span class="n">G_new</span> <span class="o">=</span> <span class="n">G_stepinit</span>
</span><span id="L-3878"><a href="#L-3878"><span class="linenos">3878</span></a>                <span class="n">obj_new</span> <span class="o">=</span> <span class="n">obj_stepinit</span>
</span><span id="L-3879"><a href="#L-3879"><span class="linenos">3879</span></a>                <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_stepinit</span>
</span><span id="L-3880"><a href="#L-3880"><span class="linenos">3880</span></a>
</span><span id="L-3881"><a href="#L-3881"><span class="linenos">3881</span></a>        <span class="c1"># --&gt; If we&#39;re here, we accept the candidate solution.</span>
</span><span id="L-3882"><a href="#L-3882"><span class="linenos">3882</span></a>
</span><span id="L-3883"><a href="#L-3883"><span class="linenos">3883</span></a>        <span class="c1"># We divide by 1-obj_curr rather than obj_curr because this measure</span>
</span><span id="L-3884"><a href="#L-3884"><span class="linenos">3884</span></a>        <span class="c1"># is more informative at values near 1.</span>
</span><span id="L-3885"><a href="#L-3885"><span class="linenos">3885</span></a>        <span class="n">improvement_curr</span> <span class="o">=</span> <span class="p">(</span><span class="n">obj_curr</span><span class="o">-</span><span class="n">obj_new</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">obj_curr</span><span class="p">)</span>
</span><span id="L-3886"><a href="#L-3886"><span class="linenos">3886</span></a>
</span><span id="L-3887"><a href="#L-3887"><span class="linenos">3887</span></a>        <span class="c1"># Accept candidate solution        </span>
</span><span id="L-3888"><a href="#L-3888"><span class="linenos">3888</span></a>        <span class="n">X_curr</span> <span class="o">=</span> <span class="n">X_new</span>
</span><span id="L-3889"><a href="#L-3889"><span class="linenos">3889</span></a>        <span class="n">G_curr</span> <span class="o">=</span> <span class="n">G_new</span>
</span><span id="L-3890"><a href="#L-3890"><span class="linenos">3890</span></a>        <span class="n">obj_curr</span> <span class="o">=</span> <span class="n">obj_new</span>
</span><span id="L-3891"><a href="#L-3891"><span class="linenos">3891</span></a>        <span class="n">mu_curr</span> <span class="o">=</span> <span class="n">mu_new</span>
</span><span id="L-3892"><a href="#L-3892"><span class="linenos">3892</span></a>        
</span><span id="L-3893"><a href="#L-3893"><span class="linenos">3893</span></a>        <span class="n">qprint</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;#</span><span class="si">%.2d</span><span class="s1">:  Objective: </span><span class="si">%g</span><span class="s1">  Surrogate incoherence: </span><span class="si">%g</span><span class="s1">  Step size: </span><span class="si">%g</span><span class="s1">  Improvement: </span><span class="si">%g</span><span class="s1">  &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">obj_curr</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mf">1.0</span><span class="o">-</span><span class="n">obj_curr</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">step_size_curr</span><span class="p">,</span> <span class="n">improvement_curr</span><span class="p">)</span> <span class="p">)</span>
</span><span id="L-3894"><a href="#L-3894"><span class="linenos">3894</span></a>
</span><span id="L-3895"><a href="#L-3895"><span class="linenos">3895</span></a>        <span class="k">if</span> <span class="n">improvement_curr</span> <span class="o">&lt;=</span> <span class="n">improvement_thresh</span><span class="p">:</span>
</span><span id="L-3896"><a href="#L-3896"><span class="linenos">3896</span></a>            <span class="n">low_improvement_counter</span> <span class="o">=</span>  <span class="n">low_improvement_counter</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="L-3897"><a href="#L-3897"><span class="linenos">3897</span></a>            <span class="n">qprint</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Low improvement strike </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">low_improvement_counter</span> <span class="p">)</span>
</span><span id="L-3898"><a href="#L-3898"><span class="linenos">3898</span></a>
</span><span id="L-3899"><a href="#L-3899"><span class="linenos">3899</span></a>            <span class="k">if</span> <span class="n">low_improvement_counter</span> <span class="o">&gt;=</span> <span class="n">max_num_low_improvements</span><span class="p">:</span>
</span><span id="L-3900"><a href="#L-3900"><span class="linenos">3900</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-3901"><a href="#L-3901"><span class="linenos">3901</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-3902"><a href="#L-3902"><span class="linenos">3902</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Reached maximal number of consecutive iterations with low improvement. Breaking.&#39;</span><span class="p">)</span>
</span><span id="L-3903"><a href="#L-3903"><span class="linenos">3903</span></a>                <span class="k">break</span>
</span><span id="L-3904"><a href="#L-3904"><span class="linenos">3904</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-3905"><a href="#L-3905"><span class="linenos">3905</span></a>            <span class="n">low_improvement_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3906"><a href="#L-3906"><span class="linenos">3906</span></a>
</span><span id="L-3907"><a href="#L-3907"><span class="linenos">3907</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="L-3908"><a href="#L-3908"><span class="linenos">3908</span></a>
</span><span id="L-3909"><a href="#L-3909"><span class="linenos">3909</span></a>    <span class="c1">#qprintln(report, &#39;\nIncoherence: %g&#39; % (1-mu_curr) )</span>
</span><span id="L-3910"><a href="#L-3910"><span class="linenos">3910</span></a>
</span><span id="L-3911"><a href="#L-3911"><span class="linenos">3911</span></a>    <span class="k">if</span> <span class="n">mu_curr</span> <span class="o">&lt;</span> <span class="n">mu_init</span><span class="p">:</span>
</span><span id="L-3912"><a href="#L-3912"><span class="linenos">3912</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Relative improvement with p=</span><span class="si">%g</span><span class="s1">: </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">mu_init</span><span class="o">-</span><span class="n">mu_curr</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">mu_init</span><span class="p">))</span> <span class="p">)</span>
</span><span id="L-3913"><a href="#L-3913"><span class="linenos">3913</span></a>        <span class="n">X_out</span> <span class="o">=</span> <span class="n">X_curr</span>
</span><span id="L-3914"><a href="#L-3914"><span class="linenos">3914</span></a>        <span class="n">step_size_out</span> <span class="o">=</span> <span class="n">step_size_curr</span>
</span><span id="L-3915"><a href="#L-3915"><span class="linenos">3915</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="L-3916"><a href="#L-3916"><span class="linenos">3916</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Incoherence did not improve with p=</span><span class="si">%g</span><span class="s1">. Reverting to previous solution.&#39;</span> <span class="o">%</span> <span class="n">p</span> <span class="p">)</span>
</span><span id="L-3917"><a href="#L-3917"><span class="linenos">3917</span></a>        <span class="n">X_out</span> <span class="o">=</span> <span class="n">X_init</span>
</span><span id="L-3918"><a href="#L-3918"><span class="linenos">3918</span></a>        <span class="n">step_size_out</span> <span class="o">=</span> <span class="n">step_size_init</span>
</span><span id="L-3919"><a href="#L-3919"><span class="linenos">3919</span></a>
</span><span id="L-3920"><a href="#L-3920"><span class="linenos">3920</span></a>    <span class="k">return</span> <span class="n">X_out</span><span class="p">,</span> <span class="n">step_size_out</span>
</span><span id="L-3921"><a href="#L-3921"><span class="linenos">3921</span></a>
</span><span id="L-3922"><a href="#L-3922"><span class="linenos">3922</span></a>
</span><span id="L-3923"><a href="#L-3923"><span class="linenos">3923</span></a><span class="k">def</span><span class="w"> </span><span class="nf">calc_G</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>    
</span><span id="L-3924"><a href="#L-3924"><span class="linenos">3924</span></a>    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-3925"><a href="#L-3925"><span class="linenos">3925</span></a>    <span class="n">G</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@X</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-3926"><a href="#L-3926"><span class="linenos">3926</span></a>    <span class="n">G</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="L-3927"><a href="#L-3927"><span class="linenos">3927</span></a>    <span class="k">return</span> <span class="n">G</span>
</span><span id="L-3928"><a href="#L-3928"><span class="linenos">3928</span></a>
</span><span id="L-3929"><a href="#L-3929"><span class="linenos">3929</span></a>
</span><span id="L-3930"><a href="#L-3930"><span class="linenos">3930</span></a><span class="k">def</span><span class="w"> </span><span class="nf">calc_mu_from_G</span><span class="p">(</span><span class="n">G</span><span class="p">):</span>
</span><span id="L-3931"><a href="#L-3931"><span class="linenos">3931</span></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>  <span class="p">)</span>
</span><span id="L-3932"><a href="#L-3932"><span class="linenos">3932</span></a>
</span><span id="L-3933"><a href="#L-3933"><span class="linenos">3933</span></a>
</span><span id="L-3934"><a href="#L-3934"><span class="linenos">3934</span></a><span class="k">def</span><span class="w"> </span><span class="nf">eval_G</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span><span id="L-3935"><a href="#L-3935"><span class="linenos">3935</span></a>    <span class="n">n</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="L-3936"><a href="#L-3936"><span class="linenos">3936</span></a>    <span class="n">mu</span> <span class="o">=</span> <span class="n">calc_mu_from_G</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
</span><span id="L-3937"><a href="#L-3937"><span class="linenos">3937</span></a>    <span class="n">rho</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mf">1.0</span><span class="p">))</span>
</span><span id="L-3938"><a href="#L-3938"><span class="linenos">3938</span></a>    <span class="n">objective</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">G</span><span class="o">/</span><span class="n">mu</span><span class="p">),</span> <span class="n">p</span> <span class="p">)</span> <span class="p">),</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">p</span> <span class="p">)</span>
</span><span id="L-3939"><a href="#L-3939"><span class="linenos">3939</span></a>
</span><span id="L-3940"><a href="#L-3940"><span class="linenos">3940</span></a>    <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">objective</span>
</span></pre></div>


            </section>
                <section id="FSWEmbedding">
                            <input id="FSWEmbedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FSWEmbedding</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="FSWEmbedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding-270"><a href="#FSWEmbedding-270"><span class="linenos"> 270</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FSWEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="FSWEmbedding-271"><a href="#FSWEmbedding-271"><span class="linenos"> 271</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-272"><a href="#FSWEmbedding-272"><span class="linenos"> 272</span></a><span class="sd">    Fourier Sliced-Wasserstein (FSW) embedding module.</span>
</span><span id="FSWEmbedding-273"><a href="#FSWEmbedding-273"><span class="linenos"> 273</span></a>
</span><span id="FSWEmbedding-274"><a href="#FSWEmbedding-274"><span class="linenos"> 274</span></a><span class="sd">    Maps input multisets (or, more generally, discrete measures) in</span>
</span><span id="FSWEmbedding-275"><a href="#FSWEmbedding-275"><span class="linenos"> 275</span></a><span class="sd">    $\mathbb{R}^{d_\text{in}}$ to fixed-length vectors in</span>
</span><span id="FSWEmbedding-276"><a href="#FSWEmbedding-276"><span class="linenos"> 276</span></a><span class="sd">    $\mathbb{R}^{d_\text{out}}$ via the Fourier Sliced-Wasserstein</span>
</span><span id="FSWEmbedding-277"><a href="#FSWEmbedding-277"><span class="linenos"> 277</span></a><span class="sd">    embedding as described in [Amir &amp; Dym, ICLR 2025].</span>
</span><span id="FSWEmbedding-278"><a href="#FSWEmbedding-278"><span class="linenos"> 278</span></a>
</span><span id="FSWEmbedding-279"><a href="#FSWEmbedding-279"><span class="linenos"> 279</span></a><span class="sd">    Features</span>
</span><span id="FSWEmbedding-280"><a href="#FSWEmbedding-280"><span class="linenos"> 280</span></a><span class="sd">    --------</span>
</span><span id="FSWEmbedding-281"><a href="#FSWEmbedding-281"><span class="linenos"> 281</span></a><span class="sd">    • **Batched inputs**: eupports arbitrary number of batch dimensions.</span>
</span><span id="FSWEmbedding-282"><a href="#FSWEmbedding-282"><span class="linenos"> 282</span></a><span class="sd">    • **Graph mode**: efficient message-aggregation, including sparse adjacency support.</span>
</span><span id="FSWEmbedding-283"><a href="#FSWEmbedding-283"><span class="linenos"> 283</span></a><span class="sd">    • **Differentiability**: Full autograd/gradient support.</span>
</span><span id="FSWEmbedding-284"><a href="#FSWEmbedding-284"><span class="linenos"> 284</span></a>
</span><span id="FSWEmbedding-285"><a href="#FSWEmbedding-285"><span class="linenos"> 285</span></a><span class="sd">    See Also</span>
</span><span id="FSWEmbedding-286"><a href="#FSWEmbedding-286"><span class="linenos"> 286</span></a><span class="sd">    --------</span>
</span><span id="FSWEmbedding-287"><a href="#FSWEmbedding-287"><span class="linenos"> 287</span></a><span class="sd">    `FSWEmbedding.__init__` : Constructor parameters.</span>
</span><span id="FSWEmbedding-288"><a href="#FSWEmbedding-288"><span class="linenos"> 288</span></a><span class="sd">    `FSWEmbedding.forward` : Input/output tensor shapes and options.</span>
</span><span id="FSWEmbedding-289"><a href="#FSWEmbedding-289"><span class="linenos"> 289</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-290"><a href="#FSWEmbedding-290"><span class="linenos"> 290</span></a>
</span><span id="FSWEmbedding-291"><a href="#FSWEmbedding-291"><span class="linenos"> 291</span></a>
</span><span id="FSWEmbedding-292"><a href="#FSWEmbedding-292"><span class="linenos"> 292</span></a>
</span><span id="FSWEmbedding-293"><a href="#FSWEmbedding-293"><span class="linenos"> 293</span></a>
</span><span id="FSWEmbedding-294"><a href="#FSWEmbedding-294"><span class="linenos"> 294</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding-295"><a href="#FSWEmbedding-295"><span class="linenos"> 295</span></a>                 <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding-296"><a href="#FSWEmbedding-296"><span class="linenos"> 296</span></a>                 <span class="n">d_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-297"><a href="#FSWEmbedding-297"><span class="linenos"> 297</span></a>                 <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-298"><a href="#FSWEmbedding-298"><span class="linenos"> 298</span></a>                 <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-299"><a href="#FSWEmbedding-299"><span class="linenos"> 299</span></a>                 <span class="n">flatten_cartesian_axes</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-300"><a href="#FSWEmbedding-300"><span class="linenos"> 300</span></a>                 <span class="n">d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="FSWEmbedding-301"><a href="#FSWEmbedding-301"><span class="linenos"> 301</span></a>                 <span class="n">encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-302"><a href="#FSWEmbedding-302"><span class="linenos"> 302</span></a>                 <span class="n">total_mass_encoding_transformation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingTransformation</span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding-303"><a href="#FSWEmbedding-303"><span class="linenos"> 303</span></a>                 <span class="n">total_mass_encoding_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingMethod</span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding-304"><a href="#FSWEmbedding-304"><span class="linenos"> 304</span></a>                 <span class="n">total_mass_encoding_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="FSWEmbedding-305"><a href="#FSWEmbedding-305"><span class="linenos"> 305</span></a>                 <span class="n">total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="FSWEmbedding-306"><a href="#FSWEmbedding-306"><span class="linenos"> 306</span></a>                 <span class="n">learnable_slices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-307"><a href="#FSWEmbedding-307"><span class="linenos"> 307</span></a>                 <span class="n">learnable_frequencies</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-308"><a href="#FSWEmbedding-308"><span class="linenos"> 308</span></a>                 <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding-309"><a href="#FSWEmbedding-309"><span class="linenos"> 309</span></a>                 <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-310"><a href="#FSWEmbedding-310"><span class="linenos"> 310</span></a>                 <span class="n">enable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FSWEmbedding-311"><a href="#FSWEmbedding-311"><span class="linenos"> 311</span></a>                 <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-312"><a href="#FSWEmbedding-312"><span class="linenos"> 312</span></a>                 <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-313"><a href="#FSWEmbedding-313"><span class="linenos"> 313</span></a>                 <span class="n">use_custom_cuda_extension_if_available</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-314"><a href="#FSWEmbedding-314"><span class="linenos"> 314</span></a>                 <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-315"><a href="#FSWEmbedding-315"><span class="linenos"> 315</span></a>                 <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-316"><a href="#FSWEmbedding-316"><span class="linenos"> 316</span></a>                 <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="FSWEmbedding-317"><a href="#FSWEmbedding-317"><span class="linenos"> 317</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-318"><a href="#FSWEmbedding-318"><span class="linenos"> 318</span></a><span class="sd">        Initialize an FSWEmbedding module.</span>
</span><span id="FSWEmbedding-319"><a href="#FSWEmbedding-319"><span class="linenos"> 319</span></a>
</span><span id="FSWEmbedding-320"><a href="#FSWEmbedding-320"><span class="linenos"> 320</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding-321"><a href="#FSWEmbedding-321"><span class="linenos"> 321</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding-322"><a href="#FSWEmbedding-322"><span class="linenos"> 322</span></a><span class="sd">        d_in : int</span>
</span><span id="FSWEmbedding-323"><a href="#FSWEmbedding-323"><span class="linenos"> 323</span></a><span class="sd">            The dimension of input multiset elements or, more generally, measure support points.  </span>
</span><span id="FSWEmbedding-324"><a href="#FSWEmbedding-324"><span class="linenos"> 324</span></a><span class="sd">            Coresponds to $d$ in $\mathcal{S}_{\leq N}\left(\mathbb{R}^d\right)$, $\mathcal{P}_{\leq N}\left(\mathbb{R}^d\right)$, or $\mathcal{M}_{\leq N}\left(\mathbb{R}^d\right)$ in our paper. </span>
</span><span id="FSWEmbedding-325"><a href="#FSWEmbedding-325"><span class="linenos"> 325</span></a><span class="sd">        d_out : int; optional</span>
</span><span id="FSWEmbedding-326"><a href="#FSWEmbedding-326"><span class="linenos"> 326</span></a><span class="sd">            Desired embedding dimension.  </span>
</span><span id="FSWEmbedding-327"><a href="#FSWEmbedding-327"><span class="linenos"> 327</span></a><span class="sd">            If not set, both `num_slices` and `num_frequencies` must be explicitly provided.</span>
</span><span id="FSWEmbedding-328"><a href="#FSWEmbedding-328"><span class="linenos"> 328</span></a><span class="sd">        num_slices : int; optional</span>
</span><span id="FSWEmbedding-329"><a href="#FSWEmbedding-329"><span class="linenos"> 329</span></a><span class="sd">            Number of slices.  </span>
</span><span id="FSWEmbedding-330"><a href="#FSWEmbedding-330"><span class="linenos"> 330</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="FSWEmbedding-331"><a href="#FSWEmbedding-331"><span class="linenos"> 331</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="FSWEmbedding-332"><a href="#FSWEmbedding-332"><span class="linenos"> 332</span></a><span class="sd">        num_frequencies : int; optional</span>
</span><span id="FSWEmbedding-333"><a href="#FSWEmbedding-333"><span class="linenos"> 333</span></a><span class="sd">            Number of frequencies per slice.  </span>
</span><span id="FSWEmbedding-334"><a href="#FSWEmbedding-334"><span class="linenos"> 334</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="FSWEmbedding-335"><a href="#FSWEmbedding-335"><span class="linenos"> 335</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="FSWEmbedding-336"><a href="#FSWEmbedding-336"><span class="linenos"> 336</span></a><span class="sd">        flatten_cartesian_axes : bool; default=False</span>
</span><span id="FSWEmbedding-337"><a href="#FSWEmbedding-337"><span class="linenos"> 337</span></a><span class="sd">            If True, flattens the slice and frequency dimensions into a single output axis.  </span>
</span><span id="FSWEmbedding-338"><a href="#FSWEmbedding-338"><span class="linenos"> 338</span></a><span class="sd">            Only relevant if `num_slices` and `num_frequencies` are provided.</span>
</span><span id="FSWEmbedding-339"><a href="#FSWEmbedding-339"><span class="linenos"> 339</span></a><span class="sd">        d_edge : int; default=0</span>
</span><span id="FSWEmbedding-340"><a href="#FSWEmbedding-340"><span class="linenos"> 340</span></a><span class="sd">            Dimension of edge feature vectors. Used only for graph inputs.  </span>
</span><span id="FSWEmbedding-341"><a href="#FSWEmbedding-341"><span class="linenos"> 341</span></a><span class="sd">            See the `graph_mode` argument of `FSWEmbedding.forward` for details.</span>
</span><span id="FSWEmbedding-342"><a href="#FSWEmbedding-342"><span class="linenos"> 342</span></a><span class="sd">        encode_total_mass : bool; default=False</span>
</span><span id="FSWEmbedding-343"><a href="#FSWEmbedding-343"><span class="linenos"> 343</span></a><span class="sd">            Whether to incorporate the input multiset size (or, more generally, the *total mass* of the input measure)</span>
</span><span id="FSWEmbedding-344"><a href="#FSWEmbedding-344"><span class="linenos"> 344</span></a><span class="sd">            into the embedding output.</span>
</span><span id="FSWEmbedding-345"><a href="#FSWEmbedding-345"><span class="linenos"> 345</span></a><span class="sd">        total_mass_encoding_transformation : {&#39;identity&#39;, &#39;sqrt&#39;, &#39;log&#39;} or TotalMassEncodingFunction; default=&#39;identity&#39;</span>
</span><span id="FSWEmbedding-346"><a href="#FSWEmbedding-346"><span class="linenos"> 346</span></a><span class="sd">            Transformation applied to the total mass *before* embedding.  </span>
</span><span id="FSWEmbedding-347"><a href="#FSWEmbedding-347"><span class="linenos"> 347</span></a><span class="sd">            See also: `TotalMassEncodingFunction`</span>
</span><span id="FSWEmbedding-348"><a href="#FSWEmbedding-348"><span class="linenos"> 348</span></a><span class="sd">        total_mass_encoding_method : {&#39;decoupled&#39;, &#39;scaled&#39;, &#39;homogeneous&#39;, &#39;homogeneous_scaled&#39;, &#39;homogeneous_legacy&#39;} or TotalMassEncodingMethod; default=&#39;decoupled&#39;</span>
</span><span id="FSWEmbedding-349"><a href="#FSWEmbedding-349"><span class="linenos"> 349</span></a><span class="sd">            Strategy for combining the total mass with the core embedding.  </span>
</span><span id="FSWEmbedding-350"><a href="#FSWEmbedding-350"><span class="linenos"> 350</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding-351"><a href="#FSWEmbedding-351"><span class="linenos"> 351</span></a><span class="sd">        total_mass_encoding_scale : float; default=1.0</span>
</span><span id="FSWEmbedding-352"><a href="#FSWEmbedding-352"><span class="linenos"> 352</span></a><span class="sd">            The encoded total mass is multiplied by this scaling factor.  </span>
</span><span id="FSWEmbedding-353"><a href="#FSWEmbedding-353"><span class="linenos"> 353</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding-354"><a href="#FSWEmbedding-354"><span class="linenos"> 354</span></a><span class="sd">        total_mass_padding_thresh : float or int; default=1.0</span>
</span><span id="FSWEmbedding-355"><a href="#FSWEmbedding-355"><span class="linenos"> 355</span></a><span class="sd">            Inputs with total mass below this threshold are padded with the zero vector to reach it; see</span>
</span><span id="FSWEmbedding-356"><a href="#FSWEmbedding-356"><span class="linenos"> 356</span></a><span class="sd">            in [Amir and Dym, ICLR 2025], Appendix A.1.  </span>
</span><span id="FSWEmbedding-357"><a href="#FSWEmbedding-357"><span class="linenos"> 357</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding-358"><a href="#FSWEmbedding-358"><span class="linenos"> 358</span></a><span class="sd">        learnable_slices : bool; default=False</span>
</span><span id="FSWEmbedding-359"><a href="#FSWEmbedding-359"><span class="linenos"> 359</span></a><span class="sd">            If True, slice vectors are learnable parameters.  </span>
</span><span id="FSWEmbedding-360"><a href="#FSWEmbedding-360"><span class="linenos"> 360</span></a><span class="sd">        learnable_frequencies : bool; default=False</span>
</span><span id="FSWEmbedding-361"><a href="#FSWEmbedding-361"><span class="linenos"> 361</span></a><span class="sd">            If True, frequency values are learnable parameters.</span>
</span><span id="FSWEmbedding-362"><a href="#FSWEmbedding-362"><span class="linenos"> 362</span></a><span class="sd">        frequency_init : float, str, tuple of float, or FrequencyInitMethod; default=&#39;random&#39;</span>
</span><span id="FSWEmbedding-363"><a href="#FSWEmbedding-363"><span class="linenos"> 363</span></a><span class="sd">            Initialization scheme for frequencies:</span>
</span><span id="FSWEmbedding-364"><a href="#FSWEmbedding-364"><span class="linenos"> 364</span></a><span class="sd">              - A float: sets all frequencies to the same value.</span>
</span><span id="FSWEmbedding-365"><a href="#FSWEmbedding-365"><span class="linenos"> 365</span></a><span class="sd">              - A tuple `(low, high)` of floats: sets evenly spaced values in that interval.</span>
</span><span id="FSWEmbedding-366"><a href="#FSWEmbedding-366"><span class="linenos"> 366</span></a><span class="sd">              - &#39;random&#39;: frequencies are drawn independently from the distribution $\mathcal{D_{\xi}}$, defined in</span>
</span><span id="FSWEmbedding-367"><a href="#FSWEmbedding-367"><span class="linenos"> 367</span></a><span class="sd">                          [Amir and Dym, ICLR 2025], Section 3.</span>
</span><span id="FSWEmbedding-368"><a href="#FSWEmbedding-368"><span class="linenos"> 368</span></a><span class="sd">              - &#39;even&#39;: frequencies are spaced evenly according to their distribution $\mathcal{D_{\xi}}$, with spaces</span>
</span><span id="FSWEmbedding-369"><a href="#FSWEmbedding-369"><span class="linenos"> 369</span></a><span class="sd">                        inversely proportional to the density.  </span>
</span><span id="FSWEmbedding-370"><a href="#FSWEmbedding-370"><span class="linenos"> 370</span></a><span class="sd">            See also: `FrequencyInitMethod`</span>
</span><span id="FSWEmbedding-371"><a href="#FSWEmbedding-371"><span class="linenos"> 371</span></a><span class="sd">        minimize_slice_coherence : bool; default=False</span>
</span><span id="FSWEmbedding-372"><a href="#FSWEmbedding-372"><span class="linenos"> 372</span></a><span class="sd">            If True, minimizes the *mutual coherence* between slices for a more uniform spread on the unit sphere.  </span>
</span><span id="FSWEmbedding-373"><a href="#FSWEmbedding-373"><span class="linenos"> 373</span></a><span class="sd">            If False, slice vectors are drawn uniformly at random from the unit sphere.</span>
</span><span id="FSWEmbedding-374"><a href="#FSWEmbedding-374"><span class="linenos"> 374</span></a><span class="sd">        enable_bias : bool; default=True</span>
</span><span id="FSWEmbedding-375"><a href="#FSWEmbedding-375"><span class="linenos"> 375</span></a><span class="sd">            If True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized</span>
</span><span id="FSWEmbedding-376"><a href="#FSWEmbedding-376"><span class="linenos"> 376</span></a><span class="sd">            to zero.  </span>
</span><span id="FSWEmbedding-377"><a href="#FSWEmbedding-377"><span class="linenos"> 377</span></a><span class="sd">        device : torch.device, int, str, or None, optional</span>
</span><span id="FSWEmbedding-378"><a href="#FSWEmbedding-378"><span class="linenos"> 378</span></a><span class="sd">            The torch device on which to allocate tensors (e.g., &#39;cpu&#39;, &#39;cuda&#39;, or an index).  </span>
</span><span id="FSWEmbedding-379"><a href="#FSWEmbedding-379"><span class="linenos"> 379</span></a><span class="sd">            If not provided, the default device defined in Torch is used.</span>
</span><span id="FSWEmbedding-380"><a href="#FSWEmbedding-380"><span class="linenos"> 380</span></a><span class="sd">        dtype : torch.dtype, optional</span>
</span><span id="FSWEmbedding-381"><a href="#FSWEmbedding-381"><span class="linenos"> 381</span></a><span class="sd">            Data type of input and output tensors (e.g., torch.float32).</span>
</span><span id="FSWEmbedding-382"><a href="#FSWEmbedding-382"><span class="linenos"> 382</span></a><span class="sd">            If not provided, the default dtype defined in Torch is used.</span>
</span><span id="FSWEmbedding-383"><a href="#FSWEmbedding-383"><span class="linenos"> 383</span></a><span class="sd">        use_custom_cuda_extension_if_available : bool or None, optional</span>
</span><span id="FSWEmbedding-384"><a href="#FSWEmbedding-384"><span class="linenos"> 384</span></a><span class="sd">            Whether to use the custom CUDA kernel if present.</span>
</span><span id="FSWEmbedding-385"><a href="#FSWEmbedding-385"><span class="linenos"> 385</span></a><span class="sd">            Default: Linux: True, all other systems: False</span>
</span><span id="FSWEmbedding-386"><a href="#FSWEmbedding-386"><span class="linenos"> 386</span></a><span class="sd">        fail_if_cuda_extension_load_fails : bool; default=False</span>
</span><span id="FSWEmbedding-387"><a href="#FSWEmbedding-387"><span class="linenos"> 387</span></a><span class="sd">            Whether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</span>
</span><span id="FSWEmbedding-388"><a href="#FSWEmbedding-388"><span class="linenos"> 388</span></a><span class="sd">        report : bool; default=False</span>
</span><span id="FSWEmbedding-389"><a href="#FSWEmbedding-389"><span class="linenos"> 389</span></a><span class="sd">            If True, prints a report with diagnostic information during initialization and forward computation.</span>
</span><span id="FSWEmbedding-390"><a href="#FSWEmbedding-390"><span class="linenos"> 390</span></a><span class="sd">        report_on_coherence_minimization : bool; default=False</span>
</span><span id="FSWEmbedding-391"><a href="#FSWEmbedding-391"><span class="linenos"> 391</span></a><span class="sd">            If True, prints special diagnostics during slice coherence minimization.</span>
</span><span id="FSWEmbedding-392"><a href="#FSWEmbedding-392"><span class="linenos"> 392</span></a>
</span><span id="FSWEmbedding-393"><a href="#FSWEmbedding-393"><span class="linenos"> 393</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-394"><a href="#FSWEmbedding-394"><span class="linenos"> 394</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-395"><a href="#FSWEmbedding-395"><span class="linenos"> 395</span></a><span class="sd">        If Cartesian mode is activated and `encode_total_mass` is True, `flatten_cartesian_axes` must be True.</span>
</span><span id="FSWEmbedding-396"><a href="#FSWEmbedding-396"><span class="linenos"> 396</span></a>
</span><span id="FSWEmbedding-397"><a href="#FSWEmbedding-397"><span class="linenos"> 397</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-398"><a href="#FSWEmbedding-398"><span class="linenos"> 398</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-399"><a href="#FSWEmbedding-399"><span class="linenos"> 399</span></a><span class="sd">        FrequencyInitMethod :</span>
</span><span id="FSWEmbedding-400"><a href="#FSWEmbedding-400"><span class="linenos"> 400</span></a><span class="sd">            Enum for selecting frequency initialization strategies.</span>
</span><span id="FSWEmbedding-401"><a href="#FSWEmbedding-401"><span class="linenos"> 401</span></a><span class="sd">        TotalMassEncodingTransformation :</span>
</span><span id="FSWEmbedding-402"><a href="#FSWEmbedding-402"><span class="linenos"> 402</span></a><span class="sd">            Enum for total mass transformations.</span>
</span><span id="FSWEmbedding-403"><a href="#FSWEmbedding-403"><span class="linenos"> 403</span></a><span class="sd">        TotalMassEncodingMethod :</span>
</span><span id="FSWEmbedding-404"><a href="#FSWEmbedding-404"><span class="linenos"> 404</span></a><span class="sd">            Enum for strategies to incorporate total mass into the embedding.</span>
</span><span id="FSWEmbedding-405"><a href="#FSWEmbedding-405"><span class="linenos"> 405</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-406"><a href="#FSWEmbedding-406"><span class="linenos"> 406</span></a>
</span><span id="FSWEmbedding-407"><a href="#FSWEmbedding-407"><span class="linenos"> 407</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FSWEmbedding-408"><a href="#FSWEmbedding-408"><span class="linenos"> 408</span></a>
</span><span id="FSWEmbedding-409"><a href="#FSWEmbedding-409"><span class="linenos"> 409</span></a>        <span class="c1"># Process sizes</span>
</span><span id="FSWEmbedding-410"><a href="#FSWEmbedding-410"><span class="linenos"> 410</span></a>        <span class="k">assert</span> <span class="n">d_in</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_in must be nonnegative&#39;</span>
</span><span id="FSWEmbedding-411"><a href="#FSWEmbedding-411"><span class="linenos"> 411</span></a>        <span class="k">assert</span> <span class="n">d_edge</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_edge must be nonnegative&#39;</span>
</span><span id="FSWEmbedding-412"><a href="#FSWEmbedding-412"><span class="linenos"> 412</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;d_out must be nonnegative or None&#39;</span>
</span><span id="FSWEmbedding-413"><a href="#FSWEmbedding-413"><span class="linenos"> 413</span></a>
</span><span id="FSWEmbedding-414"><a href="#FSWEmbedding-414"><span class="linenos"> 414</span></a>        <span class="k">if</span> <span class="n">d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-415"><a href="#FSWEmbedding-415"><span class="linenos"> 415</span></a>            <span class="c1"># If the output should be empty, we force encode_total_mass to be False</span>
</span><span id="FSWEmbedding-416"><a href="#FSWEmbedding-416"><span class="linenos"> 416</span></a>            <span class="n">encode_total_mass</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-417"><a href="#FSWEmbedding-417"><span class="linenos"> 417</span></a>
</span><span id="FSWEmbedding-418"><a href="#FSWEmbedding-418"><span class="linenos"> 418</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_in</span>
</span><span id="FSWEmbedding-419"><a href="#FSWEmbedding-419"><span class="linenos"> 419</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_edge</span>
</span><span id="FSWEmbedding-420"><a href="#FSWEmbedding-420"><span class="linenos"> 420</span></a>
</span><span id="FSWEmbedding-421"><a href="#FSWEmbedding-421"><span class="linenos"> 421</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">encode_total_mass</span>
</span><span id="FSWEmbedding-422"><a href="#FSWEmbedding-422"><span class="linenos"> 422</span></a>
</span><span id="FSWEmbedding-423"><a href="#FSWEmbedding-423"><span class="linenos"> 423</span></a>        <span class="n">total_mass_padding_thresh</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="FSWEmbedding-424"><a href="#FSWEmbedding-424"><span class="linenos"> 424</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be inf&#39;</span>
</span><span id="FSWEmbedding-425"><a href="#FSWEmbedding-425"><span class="linenos"> 425</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be NaN&#39;</span>
</span><span id="FSWEmbedding-426"><a href="#FSWEmbedding-426"><span class="linenos"> 426</span></a>        <span class="k">assert</span> <span class="n">total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="FSWEmbedding-427"><a href="#FSWEmbedding-427"><span class="linenos"> 427</span></a>
</span><span id="FSWEmbedding-428"><a href="#FSWEmbedding-428"><span class="linenos"> 428</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="FSWEmbedding-429"><a href="#FSWEmbedding-429"><span class="linenos"> 429</span></a>        <span class="k">del</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="FSWEmbedding-430"><a href="#FSWEmbedding-430"><span class="linenos"> 430</span></a>
</span><span id="FSWEmbedding-431"><a href="#FSWEmbedding-431"><span class="linenos"> 431</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="o">=</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_method</span><span class="p">)</span>
</span><span id="FSWEmbedding-432"><a href="#FSWEmbedding-432"><span class="linenos"> 432</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_method</span>
</span><span id="FSWEmbedding-433"><a href="#FSWEmbedding-433"><span class="linenos"> 433</span></a>
</span><span id="FSWEmbedding-434"><a href="#FSWEmbedding-434"><span class="linenos"> 434</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span> <span class="o">=</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="FSWEmbedding-435"><a href="#FSWEmbedding-435"><span class="linenos"> 435</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="FSWEmbedding-436"><a href="#FSWEmbedding-436"><span class="linenos"> 436</span></a>
</span><span id="FSWEmbedding-437"><a href="#FSWEmbedding-437"><span class="linenos"> 437</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span> <span class="o">=</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_transformation</span><span class="p">)</span>
</span><span id="FSWEmbedding-438"><a href="#FSWEmbedding-438"><span class="linenos"> 438</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_transformation</span>
</span><span id="FSWEmbedding-439"><a href="#FSWEmbedding-439"><span class="linenos"> 439</span></a>
</span><span id="FSWEmbedding-440"><a href="#FSWEmbedding-440"><span class="linenos"> 440</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-441"><a href="#FSWEmbedding-441"><span class="linenos"> 441</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span><span id="FSWEmbedding-442"><a href="#FSWEmbedding-442"><span class="linenos"> 442</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-443"><a href="#FSWEmbedding-443"><span class="linenos"> 443</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^(</span><span class="si">%d</span><span class="s1">+</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">)</span>
</span><span id="FSWEmbedding-444"><a href="#FSWEmbedding-444"><span class="linenos"> 444</span></a>
</span><span id="FSWEmbedding-445"><a href="#FSWEmbedding-445"><span class="linenos"> 445</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-446"><a href="#FSWEmbedding-446"><span class="linenos"> 446</span></a>
</span><span id="FSWEmbedding-447"><a href="#FSWEmbedding-447"><span class="linenos"> 447</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding-448"><a href="#FSWEmbedding-448"><span class="linenos"> 448</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding-449"><a href="#FSWEmbedding-449"><span class="linenos"> 449</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding-450"><a href="#FSWEmbedding-450"><span class="linenos"> 450</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">d_out</span>
</span><span id="FSWEmbedding-451"><a href="#FSWEmbedding-451"><span class="linenos"> 451</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding-452"><a href="#FSWEmbedding-452"><span class="linenos"> 452</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding-453"><a href="#FSWEmbedding-453"><span class="linenos"> 453</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span><span id="FSWEmbedding-454"><a href="#FSWEmbedding-454"><span class="linenos"> 454</span></a>
</span><span id="FSWEmbedding-455"><a href="#FSWEmbedding-455"><span class="linenos"> 455</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding-456"><a href="#FSWEmbedding-456"><span class="linenos"> 456</span></a>            <span class="k">assert</span> <span class="n">flatten_cartesian_axes</span>  <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">encode_total_mass</span><span class="p">),</span> <span class="s1">&#39;Cartesian mode with flatten_cartesian_axes =False is not supported when encode_total_mass=True&#39;</span>
</span><span id="FSWEmbedding-457"><a href="#FSWEmbedding-457"><span class="linenos"> 457</span></a>
</span><span id="FSWEmbedding-458"><a href="#FSWEmbedding-458"><span class="linenos"> 458</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="FSWEmbedding-459"><a href="#FSWEmbedding-459"><span class="linenos"> 459</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="n">flatten_cartesian_axes</span>
</span><span id="FSWEmbedding-460"><a href="#FSWEmbedding-460"><span class="linenos"> 460</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">num_slices</span>
</span><span id="FSWEmbedding-461"><a href="#FSWEmbedding-461"><span class="linenos"> 461</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">num_frequencies</span>
</span><span id="FSWEmbedding-462"><a href="#FSWEmbedding-462"><span class="linenos"> 462</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">num_slices</span> <span class="o">*</span> <span class="n">num_frequencies</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding-463"><a href="#FSWEmbedding-463"><span class="linenos"> 463</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="k">else</span> <span class="p">(</span><span class="s1">&#39;R^(</span><span class="si">%d</span><span class="se">\u00d7</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="FSWEmbedding-464"><a href="#FSWEmbedding-464"><span class="linenos"> 464</span></a>
</span><span id="FSWEmbedding-465"><a href="#FSWEmbedding-465"><span class="linenos"> 465</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-466"><a href="#FSWEmbedding-466"><span class="linenos"> 466</span></a>            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Expected exactly one of (d_out != None) or (num_slices != None and num_frequencies != None)&quot;</span>
</span><span id="FSWEmbedding-467"><a href="#FSWEmbedding-467"><span class="linenos"> 467</span></a>
</span><span id="FSWEmbedding-468"><a href="#FSWEmbedding-468"><span class="linenos"> 468</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_out must be nonnegative&#39;</span>
</span><span id="FSWEmbedding-469"><a href="#FSWEmbedding-469"><span class="linenos"> 469</span></a>
</span><span id="FSWEmbedding-470"><a href="#FSWEmbedding-470"><span class="linenos"> 470</span></a>        <span class="c1">#d_out = self.d_out</span>
</span><span id="FSWEmbedding-471"><a href="#FSWEmbedding-471"><span class="linenos"> 471</span></a>        <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span><span id="FSWEmbedding-472"><a href="#FSWEmbedding-472"><span class="linenos"> 472</span></a>        <span class="n">num_frequencies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span><span id="FSWEmbedding-473"><a href="#FSWEmbedding-473"><span class="linenos"> 473</span></a>
</span><span id="FSWEmbedding-474"><a href="#FSWEmbedding-474"><span class="linenos"> 474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">minimize_slice_coherence</span>
</span><span id="FSWEmbedding-475"><a href="#FSWEmbedding-475"><span class="linenos"> 475</span></a>
</span><span id="FSWEmbedding-476"><a href="#FSWEmbedding-476"><span class="linenos"> 476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="o">=</span> <span class="n">learnable_slices</span>
</span><span id="FSWEmbedding-477"><a href="#FSWEmbedding-477"><span class="linenos"> 477</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span> <span class="o">=</span> <span class="n">learnable_frequencies</span>
</span><span id="FSWEmbedding-478"><a href="#FSWEmbedding-478"><span class="linenos"> 478</span></a>
</span><span id="FSWEmbedding-479"><a href="#FSWEmbedding-479"><span class="linenos"> 479</span></a>        <span class="c1"># Note: frequency_init is checked for correctness downstream at generate_embedding_parameters()</span>
</span><span id="FSWEmbedding-480"><a href="#FSWEmbedding-480"><span class="linenos"> 480</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">frequency_init</span>
</span><span id="FSWEmbedding-481"><a href="#FSWEmbedding-481"><span class="linenos"> 481</span></a>
</span><span id="FSWEmbedding-482"><a href="#FSWEmbedding-482"><span class="linenos"> 482</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span> <span class="o">=</span> <span class="n">enable_bias</span>
</span><span id="FSWEmbedding-483"><a href="#FSWEmbedding-483"><span class="linenos"> 483</span></a>
</span><span id="FSWEmbedding-484"><a href="#FSWEmbedding-484"><span class="linenos"> 484</span></a>        <span class="c1"># _device_new and _dtype_new are only defined here on __init__ and passed on to reset_parameters(), which then deletes them</span>
</span><span id="FSWEmbedding-485"><a href="#FSWEmbedding-485"><span class="linenos"> 485</span></a>        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-486"><a href="#FSWEmbedding-486"><span class="linenos"> 486</span></a>            <span class="c1"># Use get_default_device if available (PyTorch 2.3+)</span>
</span><span id="FSWEmbedding-487"><a href="#FSWEmbedding-487"><span class="linenos"> 487</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_device&quot;</span><span class="p">):</span>
</span><span id="FSWEmbedding-488"><a href="#FSWEmbedding-488"><span class="linenos"> 488</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_device</span><span class="p">()</span>
</span><span id="FSWEmbedding-489"><a href="#FSWEmbedding-489"><span class="linenos"> 489</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-490"><a href="#FSWEmbedding-490"><span class="linenos"> 490</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="FSWEmbedding-491"><a href="#FSWEmbedding-491"><span class="linenos"> 491</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">device</span>
</span><span id="FSWEmbedding-492"><a href="#FSWEmbedding-492"><span class="linenos"> 492</span></a>
</span><span id="FSWEmbedding-493"><a href="#FSWEmbedding-493"><span class="linenos"> 493</span></a>        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-494"><a href="#FSWEmbedding-494"><span class="linenos"> 494</span></a>            <span class="c1"># Use get_default_dtype if available</span>
</span><span id="FSWEmbedding-495"><a href="#FSWEmbedding-495"><span class="linenos"> 495</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_dtype&quot;</span><span class="p">):</span>
</span><span id="FSWEmbedding-496"><a href="#FSWEmbedding-496"><span class="linenos"> 496</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>
</span><span id="FSWEmbedding-497"><a href="#FSWEmbedding-497"><span class="linenos"> 497</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-498"><a href="#FSWEmbedding-498"><span class="linenos"> 498</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="FSWEmbedding-499"><a href="#FSWEmbedding-499"><span class="linenos"> 499</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="FSWEmbedding-500"><a href="#FSWEmbedding-500"><span class="linenos"> 500</span></a>
</span><span id="FSWEmbedding-501"><a href="#FSWEmbedding-501"><span class="linenos"> 501</span></a>        <span class="k">assert</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">),</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">dtype</span>
</span><span id="FSWEmbedding-502"><a href="#FSWEmbedding-502"><span class="linenos"> 502</span></a>
</span><span id="FSWEmbedding-503"><a href="#FSWEmbedding-503"><span class="linenos"> 503</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span> <span class="o">=</span> <span class="n">device</span>
</span><span id="FSWEmbedding-504"><a href="#FSWEmbedding-504"><span class="linenos"> 504</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span> <span class="o">=</span> <span class="n">dtype</span>
</span><span id="FSWEmbedding-505"><a href="#FSWEmbedding-505"><span class="linenos"> 505</span></a>
</span><span id="FSWEmbedding-506"><a href="#FSWEmbedding-506"><span class="linenos"> 506</span></a>        <span class="k">if</span> <span class="n">use_custom_cuda_extension_if_available</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-507"><a href="#FSWEmbedding-507"><span class="linenos"> 507</span></a>            <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;Windows&#39;</span><span class="p">,</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">}:</span>
</span><span id="FSWEmbedding-508"><a href="#FSWEmbedding-508"><span class="linenos"> 508</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding-509"><a href="#FSWEmbedding-509"><span class="linenos"> 509</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-510"><a href="#FSWEmbedding-510"><span class="linenos"> 510</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="FSWEmbedding-511"><a href="#FSWEmbedding-511"><span class="linenos"> 511</span></a>
</span><span id="FSWEmbedding-512"><a href="#FSWEmbedding-512"><span class="linenos"> 512</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="FSWEmbedding-513"><a href="#FSWEmbedding-513"><span class="linenos"> 513</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="FSWEmbedding-514"><a href="#FSWEmbedding-514"><span class="linenos"> 514</span></a>
</span><span id="FSWEmbedding-515"><a href="#FSWEmbedding-515"><span class="linenos"> 515</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">report</span>
</span><span id="FSWEmbedding-516"><a href="#FSWEmbedding-516"><span class="linenos"> 516</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="FSWEmbedding-517"><a href="#FSWEmbedding-517"><span class="linenos"> 517</span></a>
</span><span id="FSWEmbedding-518"><a href="#FSWEmbedding-518"><span class="linenos"> 518</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding-519"><a href="#FSWEmbedding-519"><span class="linenos"> 519</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Fourier Sliced-Wasserstein Embedding&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-520"><a href="#FSWEmbedding-520"><span class="linenos"> 520</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;version </span><span class="si">%s</span><span class="s1">, </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="n">version_date</span><span class="p">))</span>
</span><span id="FSWEmbedding-521"><a href="#FSWEmbedding-521"><span class="linenos"> 521</span></a>
</span><span id="FSWEmbedding-522"><a href="#FSWEmbedding-522"><span class="linenos"> 522</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding-523"><a href="#FSWEmbedding-523"><span class="linenos"> 523</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Based on our paper titled &quot;Fourier Sliced-Wasserstrin Embedding for Multisets and Measures&quot;, ICLR 2025&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-524"><a href="#FSWEmbedding-524"><span class="linenos"> 524</span></a>
</span><span id="FSWEmbedding-525"><a href="#FSWEmbedding-525"><span class="linenos"> 525</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding-526"><a href="#FSWEmbedding-526"><span class="linenos"> 526</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Constructing embedding for sets in </span><span class="si">%s</span><span class="s1"> into </span><span class="si">%s</span><span class="s1">  &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_space_name</span><span class="p">,</span> <span class="n">output_space_name</span><span class="p">))</span>
</span><span id="FSWEmbedding-527"><a href="#FSWEmbedding-527"><span class="linenos"> 527</span></a>
</span><span id="FSWEmbedding-528"><a href="#FSWEmbedding-528"><span class="linenos"> 528</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding-529"><a href="#FSWEmbedding-529"><span class="linenos"> 529</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%d</span><span class="s1"> frequencies, collapsed to one </span><span class="si">%d</span><span class="s1"> dimensional axis; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">num_slices</span><span class="o">*</span><span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-530"><a href="#FSWEmbedding-530"><span class="linenos"> 530</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-531"><a href="#FSWEmbedding-531"><span class="linenos"> 531</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> frequencies; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-532"><a href="#FSWEmbedding-532"><span class="linenos"> 532</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-533"><a href="#FSWEmbedding-533"><span class="linenos"> 533</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> (slice, frequency) pairs; &#39;</span> <span class="o">%</span> <span class="n">num_slices</span>
</span><span id="FSWEmbedding-534"><a href="#FSWEmbedding-534"><span class="linenos"> 534</span></a>
</span><span id="FSWEmbedding-535"><a href="#FSWEmbedding-535"><span class="linenos"> 535</span></a>        <span class="n">qprint</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">slice_freq_str</span><span class="p">)</span>
</span><span id="FSWEmbedding-536"><a href="#FSWEmbedding-536"><span class="linenos"> 536</span></a>
</span><span id="FSWEmbedding-537"><a href="#FSWEmbedding-537"><span class="linenos"> 537</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding-538"><a href="#FSWEmbedding-538"><span class="linenos"> 538</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-539"><a href="#FSWEmbedding-539"><span class="linenos"> 539</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, frequences and biases&#39;</span>
</span><span id="FSWEmbedding-540"><a href="#FSWEmbedding-540"><span class="linenos"> 540</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-541"><a href="#FSWEmbedding-541"><span class="linenos"> 541</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and frequences, no bias&#39;</span>
</span><span id="FSWEmbedding-542"><a href="#FSWEmbedding-542"><span class="linenos"> 542</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="FSWEmbedding-543"><a href="#FSWEmbedding-543"><span class="linenos"> 543</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-544"><a href="#FSWEmbedding-544"><span class="linenos"> 544</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and biases, fixed frequencies&#39;</span>
</span><span id="FSWEmbedding-545"><a href="#FSWEmbedding-545"><span class="linenos"> 545</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-546"><a href="#FSWEmbedding-546"><span class="linenos"> 546</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, fixed frequences, no bias&#39;</span>
</span><span id="FSWEmbedding-547"><a href="#FSWEmbedding-547"><span class="linenos"> 547</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding-548"><a href="#FSWEmbedding-548"><span class="linenos"> 548</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-549"><a href="#FSWEmbedding-549"><span class="linenos"> 549</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="FSWEmbedding-550"><a href="#FSWEmbedding-550"><span class="linenos"> 550</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-551"><a href="#FSWEmbedding-551"><span class="linenos"> 551</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, no biases&#39;</span>
</span><span id="FSWEmbedding-552"><a href="#FSWEmbedding-552"><span class="linenos"> 552</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-553"><a href="#FSWEmbedding-553"><span class="linenos"> 553</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-554"><a href="#FSWEmbedding-554"><span class="linenos"> 554</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="FSWEmbedding-555"><a href="#FSWEmbedding-555"><span class="linenos"> 555</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-556"><a href="#FSWEmbedding-556"><span class="linenos"> 556</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, no bias&#39;</span>
</span><span id="FSWEmbedding-557"><a href="#FSWEmbedding-557"><span class="linenos"> 557</span></a>
</span><span id="FSWEmbedding-558"><a href="#FSWEmbedding-558"><span class="linenos"> 558</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">learnable_str</span><span class="p">)</span>
</span><span id="FSWEmbedding-559"><a href="#FSWEmbedding-559"><span class="linenos"> 559</span></a>
</span><span id="FSWEmbedding-560"><a href="#FSWEmbedding-560"><span class="linenos"> 560</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;device: </span><span class="si">%s</span><span class="s1">    dtype: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span><span class="p">))</span>
</span><span id="FSWEmbedding-561"><a href="#FSWEmbedding-561"><span class="linenos"> 561</span></a>
</span><span id="FSWEmbedding-562"><a href="#FSWEmbedding-562"><span class="linenos"> 562</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-563"><a href="#FSWEmbedding-563"><span class="linenos"> 563</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-564"><a href="#FSWEmbedding-564"><span class="linenos"> 564</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-565"><a href="#FSWEmbedding-565"><span class="linenos"> 565</span></a>
</span><span id="FSWEmbedding-566"><a href="#FSWEmbedding-566"><span class="linenos"> 566</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span><span id="FSWEmbedding-567"><a href="#FSWEmbedding-567"><span class="linenos"> 567</span></a>
</span><span id="FSWEmbedding-568"><a href="#FSWEmbedding-568"><span class="linenos"> 568</span></a>    <span class="nd">@classmethod</span>
</span><span id="FSWEmbedding-569"><a href="#FSWEmbedding-569"><span class="linenos"> 569</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;FSWEmbedding&quot;</span><span class="p">:</span>
</span><span id="FSWEmbedding-570"><a href="#FSWEmbedding-570"><span class="linenos"> 570</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-571"><a href="#FSWEmbedding-571"><span class="linenos"> 571</span></a><span class="sd">        Construct an FSWEmbedding instance from a configuration dictionary.</span>
</span><span id="FSWEmbedding-572"><a href="#FSWEmbedding-572"><span class="linenos"> 572</span></a>
</span><span id="FSWEmbedding-573"><a href="#FSWEmbedding-573"><span class="linenos"> 573</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding-574"><a href="#FSWEmbedding-574"><span class="linenos"> 574</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding-575"><a href="#FSWEmbedding-575"><span class="linenos"> 575</span></a><span class="sd">        config : dict</span>
</span><span id="FSWEmbedding-576"><a href="#FSWEmbedding-576"><span class="linenos"> 576</span></a><span class="sd">            Dictionary of keyword arguments matching the `__init__` parameters.</span>
</span><span id="FSWEmbedding-577"><a href="#FSWEmbedding-577"><span class="linenos"> 577</span></a>
</span><span id="FSWEmbedding-578"><a href="#FSWEmbedding-578"><span class="linenos"> 578</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-579"><a href="#FSWEmbedding-579"><span class="linenos"> 579</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-580"><a href="#FSWEmbedding-580"><span class="linenos"> 580</span></a><span class="sd">        FSWEmbedding</span>
</span><span id="FSWEmbedding-581"><a href="#FSWEmbedding-581"><span class="linenos"> 581</span></a><span class="sd">            A new instance initialized with the provided configuration.</span>
</span><span id="FSWEmbedding-582"><a href="#FSWEmbedding-582"><span class="linenos"> 582</span></a>
</span><span id="FSWEmbedding-583"><a href="#FSWEmbedding-583"><span class="linenos"> 583</span></a><span class="sd">        Raises</span>
</span><span id="FSWEmbedding-584"><a href="#FSWEmbedding-584"><span class="linenos"> 584</span></a><span class="sd">        ------</span>
</span><span id="FSWEmbedding-585"><a href="#FSWEmbedding-585"><span class="linenos"> 585</span></a><span class="sd">        TypeError</span>
</span><span id="FSWEmbedding-586"><a href="#FSWEmbedding-586"><span class="linenos"> 586</span></a><span class="sd">            If any keys in the dictionary are not valid constructor arguments.</span>
</span><span id="FSWEmbedding-587"><a href="#FSWEmbedding-587"><span class="linenos"> 587</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-588"><a href="#FSWEmbedding-588"><span class="linenos"> 588</span></a>        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
</span><span id="FSWEmbedding-589"><a href="#FSWEmbedding-589"><span class="linenos"> 589</span></a>        <span class="n">valid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s1">&#39;self&#39;</span><span class="p">}</span>
</span><span id="FSWEmbedding-590"><a href="#FSWEmbedding-590"><span class="linenos"> 590</span></a>        <span class="n">invalid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="o">-</span> <span class="n">valid_keys</span>
</span><span id="FSWEmbedding-591"><a href="#FSWEmbedding-591"><span class="linenos"> 591</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding-592"><a href="#FSWEmbedding-592"><span class="linenos"> 592</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config key: &#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding-593"><a href="#FSWEmbedding-593"><span class="linenos"> 593</span></a>        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding-594"><a href="#FSWEmbedding-594"><span class="linenos"> 594</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config keys: </span><span class="si">{</span><span class="n">invalid_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding-595"><a href="#FSWEmbedding-595"><span class="linenos"> 595</span></a>        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</span><span id="FSWEmbedding-596"><a href="#FSWEmbedding-596"><span class="linenos"> 596</span></a>
</span><span id="FSWEmbedding-597"><a href="#FSWEmbedding-597"><span class="linenos"> 597</span></a>    <span class="c1"># Resets the model parameters (slice vectors and frequencies) and updates the model settings.</span>
</span><span id="FSWEmbedding-598"><a href="#FSWEmbedding-598"><span class="linenos"> 598</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding-599"><a href="#FSWEmbedding-599"><span class="linenos"> 599</span></a>                         <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-600"><a href="#FSWEmbedding-600"><span class="linenos"> 600</span></a>                         <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-601"><a href="#FSWEmbedding-601"><span class="linenos"> 601</span></a>                         <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-602"><a href="#FSWEmbedding-602"><span class="linenos"> 602</span></a>                         <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding-603"><a href="#FSWEmbedding-603"><span class="linenos"> 603</span></a>
</span><span id="FSWEmbedding-604"><a href="#FSWEmbedding-604"><span class="linenos"> 604</span></a>        <span class="c1"># Apply user updates for these parameters</span>
</span><span id="FSWEmbedding-605"><a href="#FSWEmbedding-605"><span class="linenos"> 605</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">)</span>
</span><span id="FSWEmbedding-606"><a href="#FSWEmbedding-606"><span class="linenos"> 606</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">)</span>
</span><span id="FSWEmbedding-607"><a href="#FSWEmbedding-607"><span class="linenos"> 607</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="FSWEmbedding-608"><a href="#FSWEmbedding-608"><span class="linenos"> 608</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report_on_coherence_minimization</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="FSWEmbedding-609"><a href="#FSWEmbedding-609"><span class="linenos"> 609</span></a>
</span><span id="FSWEmbedding-610"><a href="#FSWEmbedding-610"><span class="linenos"> 610</span></a>        <span class="c1"># To make sure we don&#39;t use these values inside the function; if any of then is None, we must use its self. counterpart.</span>
</span><span id="FSWEmbedding-611"><a href="#FSWEmbedding-611"><span class="linenos"> 611</span></a>        <span class="k">del</span> <span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="n">frequency_init</span><span class="p">,</span> <span class="n">report</span><span class="p">,</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="FSWEmbedding-612"><a href="#FSWEmbedding-612"><span class="linenos"> 612</span></a>
</span><span id="FSWEmbedding-613"><a href="#FSWEmbedding-613"><span class="linenos"> 613</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="FSWEmbedding-614"><a href="#FSWEmbedding-614"><span class="linenos"> 614</span></a>
</span><span id="FSWEmbedding-615"><a href="#FSWEmbedding-615"><span class="linenos"> 615</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding-616"><a href="#FSWEmbedding-616"><span class="linenos"> 616</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Generating embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-617"><a href="#FSWEmbedding-617"><span class="linenos"> 617</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-618"><a href="#FSWEmbedding-618"><span class="linenos"> 618</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Resetting embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-619"><a href="#FSWEmbedding-619"><span class="linenos"> 619</span></a>
</span><span id="FSWEmbedding-620"><a href="#FSWEmbedding-620"><span class="linenos"> 620</span></a>
</span><span id="FSWEmbedding-621"><a href="#FSWEmbedding-621"><span class="linenos"> 621</span></a>        <span class="c1"># If we&#39;re running for the first time, get the device and dtype that were set in the __init__ method;</span>
</span><span id="FSWEmbedding-622"><a href="#FSWEmbedding-622"><span class="linenos"> 622</span></a>        <span class="c1"># otherwise use the current device and dtype.</span>
</span><span id="FSWEmbedding-623"><a href="#FSWEmbedding-623"><span class="linenos"> 623</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding-624"><a href="#FSWEmbedding-624"><span class="linenos"> 624</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span>
</span><span id="FSWEmbedding-625"><a href="#FSWEmbedding-625"><span class="linenos"> 625</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-626"><a href="#FSWEmbedding-626"><span class="linenos"> 626</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-627"><a href="#FSWEmbedding-627"><span class="linenos"> 627</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FSWEmbedding-628"><a href="#FSWEmbedding-628"><span class="linenos"> 628</span></a>
</span><span id="FSWEmbedding-629"><a href="#FSWEmbedding-629"><span class="linenos"> 629</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding-630"><a href="#FSWEmbedding-630"><span class="linenos"> 630</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span>
</span><span id="FSWEmbedding-631"><a href="#FSWEmbedding-631"><span class="linenos"> 631</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-632"><a href="#FSWEmbedding-632"><span class="linenos"> 632</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-633"><a href="#FSWEmbedding-633"><span class="linenos"> 633</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="FSWEmbedding-634"><a href="#FSWEmbedding-634"><span class="linenos"> 634</span></a>
</span><span id="FSWEmbedding-635"><a href="#FSWEmbedding-635"><span class="linenos"> 635</span></a>
</span><span id="FSWEmbedding-636"><a href="#FSWEmbedding-636"><span class="linenos"> 636</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-637"><a href="#FSWEmbedding-637"><span class="linenos"> 637</span></a>
</span><span id="FSWEmbedding-638"><a href="#FSWEmbedding-638"><span class="linenos"> 638</span></a>        <span class="c1"># Generate slice vectors and frequencies</span>
</span><span id="FSWEmbedding-639"><a href="#FSWEmbedding-639"><span class="linenos"> 639</span></a>        <span class="c1"># We always generate (and optimize) them in float64 and then convert to the desired dtype.</span>
</span><span id="FSWEmbedding-640"><a href="#FSWEmbedding-640"><span class="linenos"> 640</span></a>        <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_generate_embedding_parameters</span><span class="p">(</span><span class="n">d_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span>
</span><span id="FSWEmbedding-641"><a href="#FSWEmbedding-641"><span class="linenos"> 641</span></a>                                                                                       <span class="n">num_slices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">,</span>
</span><span id="FSWEmbedding-642"><a href="#FSWEmbedding-642"><span class="linenos"> 642</span></a>                                                                                       <span class="n">cartesian_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span>
</span><span id="FSWEmbedding-643"><a href="#FSWEmbedding-643"><span class="linenos"> 643</span></a>                                                                                       <span class="n">flatten_cartesian_axes</span> <span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span><span class="p">,</span>
</span><span id="FSWEmbedding-644"><a href="#FSWEmbedding-644"><span class="linenos"> 644</span></a>                                                                                       <span class="n">total_mass_encoding_dim</span><span class="o">=</span><span class="n">total_mass_encoding_dim</span><span class="p">,</span>
</span><span id="FSWEmbedding-645"><a href="#FSWEmbedding-645"><span class="linenos"> 645</span></a>                                                                                       <span class="n">frequency_init</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">,</span>
</span><span id="FSWEmbedding-646"><a href="#FSWEmbedding-646"><span class="linenos"> 646</span></a>                                                                                       <span class="n">minimize_slice_coherence</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">,</span>
</span><span id="FSWEmbedding-647"><a href="#FSWEmbedding-647"><span class="linenos"> 647</span></a>                                                                                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
</span><span id="FSWEmbedding-648"><a href="#FSWEmbedding-648"><span class="linenos"> 648</span></a>                                                                                       <span class="n">report</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span>
</span><span id="FSWEmbedding-649"><a href="#FSWEmbedding-649"><span class="linenos"> 649</span></a>                                                                                       <span class="n">report_on_coherence_minimization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="FSWEmbedding-650"><a href="#FSWEmbedding-650"><span class="linenos"> 650</span></a>
</span><span id="FSWEmbedding-651"><a href="#FSWEmbedding-651"><span class="linenos"> 651</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-652"><a href="#FSWEmbedding-652"><span class="linenos"> 652</span></a>        <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-653"><a href="#FSWEmbedding-653"><span class="linenos"> 653</span></a>
</span><span id="FSWEmbedding-654"><a href="#FSWEmbedding-654"><span class="linenos"> 654</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding-655"><a href="#FSWEmbedding-655"><span class="linenos"> 655</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-656"><a href="#FSWEmbedding-656"><span class="linenos"> 656</span></a>
</span><span id="FSWEmbedding-657"><a href="#FSWEmbedding-657"><span class="linenos"> 657</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-658"><a href="#FSWEmbedding-658"><span class="linenos"> 658</span></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-659"><a href="#FSWEmbedding-659"><span class="linenos"> 659</span></a>
</span><span id="FSWEmbedding-660"><a href="#FSWEmbedding-660"><span class="linenos"> 660</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding-661"><a href="#FSWEmbedding-661"><span class="linenos"> 661</span></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="FSWEmbedding-662"><a href="#FSWEmbedding-662"><span class="linenos"> 662</span></a>
</span><span id="FSWEmbedding-663"><a href="#FSWEmbedding-663"><span class="linenos"> 663</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding-664"><a href="#FSWEmbedding-664"><span class="linenos"> 664</span></a>
</span><span id="FSWEmbedding-665"><a href="#FSWEmbedding-665"><span class="linenos"> 665</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-666"><a href="#FSWEmbedding-666"><span class="linenos"> 666</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-667"><a href="#FSWEmbedding-667"><span class="linenos"> 667</span></a>
</span><span id="FSWEmbedding-668"><a href="#FSWEmbedding-668"><span class="linenos"> 668</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="FSWEmbedding-669"><a href="#FSWEmbedding-669"><span class="linenos"> 669</span></a>
</span><span id="FSWEmbedding-670"><a href="#FSWEmbedding-670"><span class="linenos"> 670</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="FSWEmbedding-671"><a href="#FSWEmbedding-671"><span class="linenos"> 671</span></a>
</span><span id="FSWEmbedding-672"><a href="#FSWEmbedding-672"><span class="linenos"> 672</span></a>
</span><span id="FSWEmbedding-673"><a href="#FSWEmbedding-673"><span class="linenos"> 673</span></a>
</span><span id="FSWEmbedding-674"><a href="#FSWEmbedding-674"><span class="linenos"> 674</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="FSWEmbedding-675"><a href="#FSWEmbedding-675"><span class="linenos"> 675</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the module to the specified device or dtype.</span>
</span><span id="FSWEmbedding-676"><a href="#FSWEmbedding-676"><span class="linenos"> 676</span></a>
</span><span id="FSWEmbedding-677"><a href="#FSWEmbedding-677"><span class="linenos"> 677</span></a><span class="sd">        Example:</span>
</span><span id="FSWEmbedding-678"><a href="#FSWEmbedding-678"><span class="linenos"> 678</span></a>
</span><span id="FSWEmbedding-679"><a href="#FSWEmbedding-679"><span class="linenos"> 679</span></a><span class="sd">            module.to(torch.float32)</span>
</span><span id="FSWEmbedding-680"><a href="#FSWEmbedding-680"><span class="linenos"> 680</span></a><span class="sd">            module.to(device=&#39;cuda&#39;)</span>
</span><span id="FSWEmbedding-681"><a href="#FSWEmbedding-681"><span class="linenos"> 681</span></a>
</span><span id="FSWEmbedding-682"><a href="#FSWEmbedding-682"><span class="linenos"> 682</span></a><span class="sd">        See also: torch.nn.Module.to()</span>
</span><span id="FSWEmbedding-683"><a href="#FSWEmbedding-683"><span class="linenos"> 683</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-684"><a href="#FSWEmbedding-684"><span class="linenos"> 684</span></a>        <span class="k">if</span> <span class="s1">&#39;dtype&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
</span><span id="FSWEmbedding-685"><a href="#FSWEmbedding-685"><span class="linenos"> 685</span></a>            <span class="n">arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
</span><span id="FSWEmbedding-686"><a href="#FSWEmbedding-686"><span class="linenos"> 686</span></a>
</span><span id="FSWEmbedding-687"><a href="#FSWEmbedding-687"><span class="linenos"> 687</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="s1">&#39;invalid input type </span><span class="si">%s</span><span class="s1"> at argument &#39;&#39;dtype&#39;&#39;&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
</span><span id="FSWEmbedding-688"><a href="#FSWEmbedding-688"><span class="linenos"> 688</span></a>            <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="FSWEmbedding-689"><a href="#FSWEmbedding-689"><span class="linenos"> 689</span></a>
</span><span id="FSWEmbedding-690"><a href="#FSWEmbedding-690"><span class="linenos"> 690</span></a>        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
</span><span id="FSWEmbedding-691"><a href="#FSWEmbedding-691"><span class="linenos"> 691</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
</span><span id="FSWEmbedding-692"><a href="#FSWEmbedding-692"><span class="linenos"> 692</span></a>                <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="FSWEmbedding-693"><a href="#FSWEmbedding-693"><span class="linenos"> 693</span></a>
</span><span id="FSWEmbedding-694"><a href="#FSWEmbedding-694"><span class="linenos"> 694</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="FSWEmbedding-695"><a href="#FSWEmbedding-695"><span class="linenos"> 695</span></a>
</span><span id="FSWEmbedding-696"><a href="#FSWEmbedding-696"><span class="linenos"> 696</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="FSWEmbedding-697"><a href="#FSWEmbedding-697"><span class="linenos"> 697</span></a>
</span><span id="FSWEmbedding-698"><a href="#FSWEmbedding-698"><span class="linenos"> 698</span></a>
</span><span id="FSWEmbedding-699"><a href="#FSWEmbedding-699"><span class="linenos"> 699</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-700"><a href="#FSWEmbedding-700"><span class="linenos"> 700</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding-701"><a href="#FSWEmbedding-701"><span class="linenos"> 701</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of slices used in the embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-702"><a href="#FSWEmbedding-702"><span class="linenos"> 702</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span><span id="FSWEmbedding-703"><a href="#FSWEmbedding-703"><span class="linenos"> 703</span></a>
</span><span id="FSWEmbedding-704"><a href="#FSWEmbedding-704"><span class="linenos"> 704</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-705"><a href="#FSWEmbedding-705"><span class="linenos"> 705</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding-706"><a href="#FSWEmbedding-706"><span class="linenos"> 706</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-707"><a href="#FSWEmbedding-707"><span class="linenos"> 707</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span><span id="FSWEmbedding-708"><a href="#FSWEmbedding-708"><span class="linenos"> 708</span></a>
</span><span id="FSWEmbedding-709"><a href="#FSWEmbedding-709"><span class="linenos"> 709</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-710"><a href="#FSWEmbedding-710"><span class="linenos"> 710</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">cartesian_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-711"><a href="#FSWEmbedding-711"><span class="linenos"> 711</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices</span>
</span><span id="FSWEmbedding-712"><a href="#FSWEmbedding-712"><span class="linenos"> 712</span></a><span class="sd">        and frequencies.</span>
</span><span id="FSWEmbedding-713"><a href="#FSWEmbedding-713"><span class="linenos"> 713</span></a><span class="sd">        In Cartesian mode, the embeding dimension is `d_out` = `num_slices × num_frequencies`.</span>
</span><span id="FSWEmbedding-714"><a href="#FSWEmbedding-714"><span class="linenos"> 714</span></a><span class="sd">        Cartesian mode is activated by providing `num_slices` and `num_frequencies` to `FSWEmbedding.__init__`bool</span>
</span><span id="FSWEmbedding-715"><a href="#FSWEmbedding-715"><span class="linenos"> 715</span></a><span class="sd">        See also: `flatten_cartesian_axes`&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-716"><a href="#FSWEmbedding-716"><span class="linenos"> 716</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span>
</span><span id="FSWEmbedding-717"><a href="#FSWEmbedding-717"><span class="linenos"> 717</span></a>
</span><span id="FSWEmbedding-718"><a href="#FSWEmbedding-718"><span class="linenos"> 718</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-719"><a href="#FSWEmbedding-719"><span class="linenos"> 719</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">flatten_cartesian_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-720"><a href="#FSWEmbedding-720"><span class="linenos"> 720</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.</span>
</span><span id="FSWEmbedding-721"><a href="#FSWEmbedding-721"><span class="linenos"> 721</span></a><span class="sd">        If True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (`num_slices`, `num_frequencies`).</span>
</span><span id="FSWEmbedding-722"><a href="#FSWEmbedding-722"><span class="linenos"> 722</span></a><span class="sd">        If False, the otput is shaped `num_slices` × `num_frequencies`.</span>
</span><span id="FSWEmbedding-723"><a href="#FSWEmbedding-723"><span class="linenos"> 723</span></a><span class="sd">        This setting is only relevant if `cartesian_mode` is True.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-724"><a href="#FSWEmbedding-724"><span class="linenos"> 724</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>
</span><span id="FSWEmbedding-725"><a href="#FSWEmbedding-725"><span class="linenos"> 725</span></a>
</span><span id="FSWEmbedding-726"><a href="#FSWEmbedding-726"><span class="linenos"> 726</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-727"><a href="#FSWEmbedding-727"><span class="linenos"> 727</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-728"><a href="#FSWEmbedding-728"><span class="linenos"> 728</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether slice directions are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-729"><a href="#FSWEmbedding-729"><span class="linenos"> 729</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span>
</span><span id="FSWEmbedding-730"><a href="#FSWEmbedding-730"><span class="linenos"> 730</span></a>
</span><span id="FSWEmbedding-731"><a href="#FSWEmbedding-731"><span class="linenos"> 731</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-732"><a href="#FSWEmbedding-732"><span class="linenos"> 732</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-733"><a href="#FSWEmbedding-733"><span class="linenos"> 733</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether frequency values are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-734"><a href="#FSWEmbedding-734"><span class="linenos"> 734</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span>
</span><span id="FSWEmbedding-735"><a href="#FSWEmbedding-735"><span class="linenos"> 735</span></a>
</span><span id="FSWEmbedding-736"><a href="#FSWEmbedding-736"><span class="linenos"> 736</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-737"><a href="#FSWEmbedding-737"><span class="linenos"> 737</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">enable_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-738"><a href="#FSWEmbedding-738"><span class="linenos"> 738</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether a learnable bias vector is added to the output embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-739"><a href="#FSWEmbedding-739"><span class="linenos"> 739</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span>
</span><span id="FSWEmbedding-740"><a href="#FSWEmbedding-740"><span class="linenos"> 740</span></a>
</span><span id="FSWEmbedding-741"><a href="#FSWEmbedding-741"><span class="linenos"> 741</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-742"><a href="#FSWEmbedding-742"><span class="linenos"> 742</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">encode_total_mass</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding-743"><a href="#FSWEmbedding-743"><span class="linenos"> 743</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the total mass of the input measure is encoded into the embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-744"><a href="#FSWEmbedding-744"><span class="linenos"> 744</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span>
</span><span id="FSWEmbedding-745"><a href="#FSWEmbedding-745"><span class="linenos"> 745</span></a>
</span><span id="FSWEmbedding-746"><a href="#FSWEmbedding-746"><span class="linenos"> 746</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-747"><a href="#FSWEmbedding-747"><span class="linenos"> 747</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_transformation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingTransformation</span><span class="p">:</span>
</span><span id="FSWEmbedding-748"><a href="#FSWEmbedding-748"><span class="linenos"> 748</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Function applied to the total mass before it is stored.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-749"><a href="#FSWEmbedding-749"><span class="linenos"> 749</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span>
</span><span id="FSWEmbedding-750"><a href="#FSWEmbedding-750"><span class="linenos"> 750</span></a>
</span><span id="FSWEmbedding-751"><a href="#FSWEmbedding-751"><span class="linenos"> 751</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-752"><a href="#FSWEmbedding-752"><span class="linenos"> 752</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingMethod</span><span class="p">:</span>
</span><span id="FSWEmbedding-753"><a href="#FSWEmbedding-753"><span class="linenos"> 753</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Strategy used to incorporate total mass into the final embedding vector.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-754"><a href="#FSWEmbedding-754"><span class="linenos"> 754</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span>
</span><span id="FSWEmbedding-755"><a href="#FSWEmbedding-755"><span class="linenos"> 755</span></a>
</span><span id="FSWEmbedding-756"><a href="#FSWEmbedding-756"><span class="linenos"> 756</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-757"><a href="#FSWEmbedding-757"><span class="linenos"> 757</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="FSWEmbedding-758"><a href="#FSWEmbedding-758"><span class="linenos"> 758</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The encoded total mass is scaled by this factor.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-759"><a href="#FSWEmbedding-759"><span class="linenos"> 759</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span><span id="FSWEmbedding-760"><a href="#FSWEmbedding-760"><span class="linenos"> 760</span></a>
</span><span id="FSWEmbedding-761"><a href="#FSWEmbedding-761"><span class="linenos"> 761</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-762"><a href="#FSWEmbedding-762"><span class="linenos"> 762</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_padding_thresh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="FSWEmbedding-763"><a href="#FSWEmbedding-763"><span class="linenos"> 763</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Minimum total mass threshold; inputs below this value are padded to reach it.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-764"><a href="#FSWEmbedding-764"><span class="linenos"> 764</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span>
</span><span id="FSWEmbedding-765"><a href="#FSWEmbedding-765"><span class="linenos"> 765</span></a>
</span><span id="FSWEmbedding-766"><a href="#FSWEmbedding-766"><span class="linenos"> 766</span></a>
</span><span id="FSWEmbedding-767"><a href="#FSWEmbedding-767"><span class="linenos"> 767</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-768"><a href="#FSWEmbedding-768"><span class="linenos"> 768</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_in</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding-769"><a href="#FSWEmbedding-769"><span class="linenos"> 769</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Ambient dimension of the input elements.</span>
</span><span id="FSWEmbedding-770"><a href="#FSWEmbedding-770"><span class="linenos"> 770</span></a>
</span><span id="FSWEmbedding-771"><a href="#FSWEmbedding-771"><span class="linenos"> 771</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-772"><a href="#FSWEmbedding-772"><span class="linenos"> 772</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-773"><a href="#FSWEmbedding-773"><span class="linenos"> 773</span></a><span class="sd">        int</span>
</span><span id="FSWEmbedding-774"><a href="#FSWEmbedding-774"><span class="linenos"> 774</span></a><span class="sd">            The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</span>
</span><span id="FSWEmbedding-775"><a href="#FSWEmbedding-775"><span class="linenos"> 775</span></a>
</span><span id="FSWEmbedding-776"><a href="#FSWEmbedding-776"><span class="linenos"> 776</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-777"><a href="#FSWEmbedding-777"><span class="linenos"> 777</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-778"><a href="#FSWEmbedding-778"><span class="linenos"> 778</span></a><span class="sd">        This value is set at initialization and determines the expected feature dimension of input points.</span>
</span><span id="FSWEmbedding-779"><a href="#FSWEmbedding-779"><span class="linenos"> 779</span></a>
</span><span id="FSWEmbedding-780"><a href="#FSWEmbedding-780"><span class="linenos"> 780</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-781"><a href="#FSWEmbedding-781"><span class="linenos"> 781</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-782"><a href="#FSWEmbedding-782"><span class="linenos"> 782</span></a><span class="sd">        __init__ : The `d_in` argument specifies this value at initialization.</span>
</span><span id="FSWEmbedding-783"><a href="#FSWEmbedding-783"><span class="linenos"> 783</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-784"><a href="#FSWEmbedding-784"><span class="linenos"> 784</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span><span id="FSWEmbedding-785"><a href="#FSWEmbedding-785"><span class="linenos"> 785</span></a>
</span><span id="FSWEmbedding-786"><a href="#FSWEmbedding-786"><span class="linenos"> 786</span></a>
</span><span id="FSWEmbedding-787"><a href="#FSWEmbedding-787"><span class="linenos"> 787</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-788"><a href="#FSWEmbedding-788"><span class="linenos"> 788</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_out</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding-789"><a href="#FSWEmbedding-789"><span class="linenos"> 789</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Dimensionality of the embedding output.</span>
</span><span id="FSWEmbedding-790"><a href="#FSWEmbedding-790"><span class="linenos"> 790</span></a>
</span><span id="FSWEmbedding-791"><a href="#FSWEmbedding-791"><span class="linenos"> 791</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-792"><a href="#FSWEmbedding-792"><span class="linenos"> 792</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-793"><a href="#FSWEmbedding-793"><span class="linenos"> 793</span></a><span class="sd">        int</span>
</span><span id="FSWEmbedding-794"><a href="#FSWEmbedding-794"><span class="linenos"> 794</span></a><span class="sd">            The dimension of the vector produced by the embedding for each multiset or distribution.</span>
</span><span id="FSWEmbedding-795"><a href="#FSWEmbedding-795"><span class="linenos"> 795</span></a>
</span><span id="FSWEmbedding-796"><a href="#FSWEmbedding-796"><span class="linenos"> 796</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-797"><a href="#FSWEmbedding-797"><span class="linenos"> 797</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-798"><a href="#FSWEmbedding-798"><span class="linenos"> 798</span></a><span class="sd">        This value is set at initialization and governs the size of the embedding output.</span>
</span><span id="FSWEmbedding-799"><a href="#FSWEmbedding-799"><span class="linenos"> 799</span></a>
</span><span id="FSWEmbedding-800"><a href="#FSWEmbedding-800"><span class="linenos"> 800</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-801"><a href="#FSWEmbedding-801"><span class="linenos"> 801</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-802"><a href="#FSWEmbedding-802"><span class="linenos"> 802</span></a><span class="sd">        __init__ : The `d_out` argument specifies this value at initialization.</span>
</span><span id="FSWEmbedding-803"><a href="#FSWEmbedding-803"><span class="linenos"> 803</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-804"><a href="#FSWEmbedding-804"><span class="linenos"> 804</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span><span id="FSWEmbedding-805"><a href="#FSWEmbedding-805"><span class="linenos"> 805</span></a>
</span><span id="FSWEmbedding-806"><a href="#FSWEmbedding-806"><span class="linenos"> 806</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-807"><a href="#FSWEmbedding-807"><span class="linenos"> 807</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FSWEmbedding-808"><a href="#FSWEmbedding-808"><span class="linenos"> 808</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.device: The device on which the module&#39;s parameters and buffers are stored.</span>
</span><span id="FSWEmbedding-809"><a href="#FSWEmbedding-809"><span class="linenos"> 809</span></a>
</span><span id="FSWEmbedding-810"><a href="#FSWEmbedding-810"><span class="linenos"> 810</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-811"><a href="#FSWEmbedding-811"><span class="linenos"> 811</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-812"><a href="#FSWEmbedding-812"><span class="linenos"> 812</span></a><span class="sd">        torch.device</span>
</span><span id="FSWEmbedding-813"><a href="#FSWEmbedding-813"><span class="linenos"> 813</span></a><span class="sd">            The PyTorch device (`&#39;cpu&#39;`, `&#39;cuda&#39;`, etc.) where the embedding computations will take place.</span>
</span><span id="FSWEmbedding-814"><a href="#FSWEmbedding-814"><span class="linenos"> 814</span></a>
</span><span id="FSWEmbedding-815"><a href="#FSWEmbedding-815"><span class="linenos"> 815</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-816"><a href="#FSWEmbedding-816"><span class="linenos"> 816</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-817"><a href="#FSWEmbedding-817"><span class="linenos"> 817</span></a><span class="sd">        This behaves like the `device` property in standard PyTorch modules.</span>
</span><span id="FSWEmbedding-818"><a href="#FSWEmbedding-818"><span class="linenos"> 818</span></a>
</span><span id="FSWEmbedding-819"><a href="#FSWEmbedding-819"><span class="linenos"> 819</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-820"><a href="#FSWEmbedding-820"><span class="linenos"> 820</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-821"><a href="#FSWEmbedding-821"><span class="linenos"> 821</span></a><span class="sd">        __init__ : The `device` can be specified at initialization.</span>
</span><span id="FSWEmbedding-822"><a href="#FSWEmbedding-822"><span class="linenos"> 822</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-823"><a href="#FSWEmbedding-823"><span class="linenos"> 823</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">device</span>
</span><span id="FSWEmbedding-824"><a href="#FSWEmbedding-824"><span class="linenos"> 824</span></a>
</span><span id="FSWEmbedding-825"><a href="#FSWEmbedding-825"><span class="linenos"> 825</span></a>
</span><span id="FSWEmbedding-826"><a href="#FSWEmbedding-826"><span class="linenos"> 826</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding-827"><a href="#FSWEmbedding-827"><span class="linenos"> 827</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FSWEmbedding-828"><a href="#FSWEmbedding-828"><span class="linenos"> 828</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.dtype: The default data type used by the module.</span>
</span><span id="FSWEmbedding-829"><a href="#FSWEmbedding-829"><span class="linenos"> 829</span></a>
</span><span id="FSWEmbedding-830"><a href="#FSWEmbedding-830"><span class="linenos"> 830</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-831"><a href="#FSWEmbedding-831"><span class="linenos"> 831</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-832"><a href="#FSWEmbedding-832"><span class="linenos"> 832</span></a><span class="sd">        torch.dtype</span>
</span><span id="FSWEmbedding-833"><a href="#FSWEmbedding-833"><span class="linenos"> 833</span></a><span class="sd">            The data type (`torch.float32`, `torch.float64`, etc.) of the module’s parameters and buffers.</span>
</span><span id="FSWEmbedding-834"><a href="#FSWEmbedding-834"><span class="linenos"> 834</span></a>
</span><span id="FSWEmbedding-835"><a href="#FSWEmbedding-835"><span class="linenos"> 835</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-836"><a href="#FSWEmbedding-836"><span class="linenos"> 836</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-837"><a href="#FSWEmbedding-837"><span class="linenos"> 837</span></a><span class="sd">        This behaves like the `dtype` property in standard PyTorch modules.</span>
</span><span id="FSWEmbedding-838"><a href="#FSWEmbedding-838"><span class="linenos"> 838</span></a>
</span><span id="FSWEmbedding-839"><a href="#FSWEmbedding-839"><span class="linenos"> 839</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-840"><a href="#FSWEmbedding-840"><span class="linenos"> 840</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-841"><a href="#FSWEmbedding-841"><span class="linenos"> 841</span></a><span class="sd">        __init__ : The `dtype` can be specified at initialization.</span>
</span><span id="FSWEmbedding-842"><a href="#FSWEmbedding-842"><span class="linenos"> 842</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-843"><a href="#FSWEmbedding-843"><span class="linenos"> 843</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="FSWEmbedding-844"><a href="#FSWEmbedding-844"><span class="linenos"> 844</span></a>
</span><span id="FSWEmbedding-845"><a href="#FSWEmbedding-845"><span class="linenos"> 845</span></a>
</span><span id="FSWEmbedding-846"><a href="#FSWEmbedding-846"><span class="linenos"> 846</span></a>    <span class="nd">@staticmethod</span>
</span><span id="FSWEmbedding-847"><a href="#FSWEmbedding-847"><span class="linenos"> 847</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_embedding_parameters</span><span class="p">(</span><span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding-848"><a href="#FSWEmbedding-848"><span class="linenos"> 848</span></a>                                       <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding-849"><a href="#FSWEmbedding-849"><span class="linenos"> 849</span></a>                                       <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding-850"><a href="#FSWEmbedding-850"><span class="linenos"> 850</span></a>                                       <span class="n">cartesian_mode</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FSWEmbedding-851"><a href="#FSWEmbedding-851"><span class="linenos"> 851</span></a>                                       <span class="n">flatten_cartesian_axes</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FSWEmbedding-852"><a href="#FSWEmbedding-852"><span class="linenos"> 852</span></a>                                       <span class="n">total_mass_encoding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding-853"><a href="#FSWEmbedding-853"><span class="linenos"> 853</span></a>                                       <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span><span class="p">,</span>
</span><span id="FSWEmbedding-854"><a href="#FSWEmbedding-854"><span class="linenos"> 854</span></a>                                       <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FSWEmbedding-855"><a href="#FSWEmbedding-855"><span class="linenos"> 855</span></a>                                       <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-856"><a href="#FSWEmbedding-856"><span class="linenos"> 856</span></a>                                       <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="FSWEmbedding-857"><a href="#FSWEmbedding-857"><span class="linenos"> 857</span></a>                                       <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
</span><span id="FSWEmbedding-858"><a href="#FSWEmbedding-858"><span class="linenos"> 858</span></a>        <span class="n">dtype_init</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>
</span><span id="FSWEmbedding-859"><a href="#FSWEmbedding-859"><span class="linenos"> 859</span></a>
</span><span id="FSWEmbedding-860"><a href="#FSWEmbedding-860"><span class="linenos"> 860</span></a>        <span class="c1"># Axis number for the ambient space R^d_in</span>
</span><span id="FSWEmbedding-861"><a href="#FSWEmbedding-861"><span class="linenos"> 861</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-862"><a href="#FSWEmbedding-862"><span class="linenos"> 862</span></a>
</span><span id="FSWEmbedding-863"><a href="#FSWEmbedding-863"><span class="linenos"> 863</span></a>        <span class="c1">### A. Generate slice vectors</span>
</span><span id="FSWEmbedding-864"><a href="#FSWEmbedding-864"><span class="linenos"> 864</span></a>
</span><span id="FSWEmbedding-865"><a href="#FSWEmbedding-865"><span class="linenos"> 865</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-866"><a href="#FSWEmbedding-866"><span class="linenos"> 866</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">ambspace_axis</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="FSWEmbedding-867"><a href="#FSWEmbedding-867"><span class="linenos"> 867</span></a>
</span><span id="FSWEmbedding-868"><a href="#FSWEmbedding-868"><span class="linenos"> 868</span></a>        <span class="k">if</span> <span class="n">minimize_slice_coherence</span><span class="p">:</span>
</span><span id="FSWEmbedding-869"><a href="#FSWEmbedding-869"><span class="linenos"> 869</span></a>            <span class="k">if</span> <span class="p">(</span><span class="n">num_slices</span> <span class="o">&gt;</span> <span class="n">d_in</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">True</span><span class="p">:</span>
</span><span id="FSWEmbedding-870"><a href="#FSWEmbedding-870"><span class="linenos"> 870</span></a>                <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">minimize_mutual_coherence</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="n">report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="FSWEmbedding-871"><a href="#FSWEmbedding-871"><span class="linenos"> 871</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> slice vectors in R^</span><span class="si">%d</span><span class="s1"> with minimized mutual coherence&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">))</span>
</span><span id="FSWEmbedding-872"><a href="#FSWEmbedding-872"><span class="linenos"> 872</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-873"><a href="#FSWEmbedding-873"><span class="linenos"> 873</span></a>                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;num_slices: &#39;</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">,</span> <span class="s1">&#39;d_in: &#39;</span><span class="p">,</span> <span class="n">d_in</span><span class="p">)</span>
</span><span id="FSWEmbedding-874"><a href="#FSWEmbedding-874"><span class="linenos"> 874</span></a>                <span class="c1"># Here we need to compute a set of num_slices orthogonal vectors in R^d_in.</span>
</span><span id="FSWEmbedding-875"><a href="#FSWEmbedding-875"><span class="linenos"> 875</span></a>                <span class="c1"># Below are two methods to do so: SVD and QR decomposition</span>
</span><span id="FSWEmbedding-876"><a href="#FSWEmbedding-876"><span class="linenos"> 876</span></a>                <span class="c1"># In some cases with little available memory, SVD seems more resilient, whereas QR sometimes crashes.</span>
</span><span id="FSWEmbedding-877"><a href="#FSWEmbedding-877"><span class="linenos"> 877</span></a>                <span class="n">use_svd</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="FSWEmbedding-878"><a href="#FSWEmbedding-878"><span class="linenos"> 878</span></a>
</span><span id="FSWEmbedding-879"><a href="#FSWEmbedding-879"><span class="linenos"> 879</span></a>                <span class="k">if</span> <span class="n">use_svd</span><span class="p">:</span>
</span><span id="FSWEmbedding-880"><a href="#FSWEmbedding-880"><span class="linenos"> 880</span></a>                    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FSWEmbedding-881"><a href="#FSWEmbedding-881"><span class="linenos"> 881</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">Vh</span>
</span><span id="FSWEmbedding-882"><a href="#FSWEmbedding-882"><span class="linenos"> 882</span></a>                    <span class="k">del</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span>
</span><span id="FSWEmbedding-883"><a href="#FSWEmbedding-883"><span class="linenos"> 883</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-884"><a href="#FSWEmbedding-884"><span class="linenos"> 884</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-885"><a href="#FSWEmbedding-885"><span class="linenos"> 885</span></a>                    <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;reduced&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-886"><a href="#FSWEmbedding-886"><span class="linenos"> 886</span></a>                    <span class="k">del</span> <span class="n">R</span>
</span><span id="FSWEmbedding-887"><a href="#FSWEmbedding-887"><span class="linenos"> 887</span></a>                    <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-888"><a href="#FSWEmbedding-888"><span class="linenos"> 888</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> perpendicular slice vectors in R^</span><span class="si">%d</span><span class="s1"> using QR decomposition&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">d_in</span><span class="p">))</span>
</span><span id="FSWEmbedding-889"><a href="#FSWEmbedding-889"><span class="linenos"> 889</span></a>
</span><span id="FSWEmbedding-890"><a href="#FSWEmbedding-890"><span class="linenos"> 890</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-891"><a href="#FSWEmbedding-891"><span class="linenos"> 891</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Generated </span><span class="si">%d</span><span class="s1"> random slice vectors&#39;</span> <span class="o">%</span> <span class="n">num_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding-892"><a href="#FSWEmbedding-892"><span class="linenos"> 892</span></a>
</span><span id="FSWEmbedding-893"><a href="#FSWEmbedding-893"><span class="linenos"> 893</span></a>        <span class="c1"># Detect nans, infs and zero vectors in slice_vectors</span>
</span><span id="FSWEmbedding-894"><a href="#FSWEmbedding-894"><span class="linenos"> 894</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found infs in slice_vectors&quot;</span>
</span><span id="FSWEmbedding-895"><a href="#FSWEmbedding-895"><span class="linenos"> 895</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found nans in slice_vectors&quot;</span>
</span><span id="FSWEmbedding-896"><a href="#FSWEmbedding-896"><span class="linenos"> 896</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">slice_vectors</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Found zero vectors in slice_vectors&#39;</span>
</span><span id="FSWEmbedding-897"><a href="#FSWEmbedding-897"><span class="linenos"> 897</span></a>
</span><span id="FSWEmbedding-898"><a href="#FSWEmbedding-898"><span class="linenos"> 898</span></a>
</span><span id="FSWEmbedding-899"><a href="#FSWEmbedding-899"><span class="linenos"> 899</span></a>
</span><span id="FSWEmbedding-900"><a href="#FSWEmbedding-900"><span class="linenos"> 900</span></a>        <span class="c1">### B. Generate frequencies</span>
</span><span id="FSWEmbedding-901"><a href="#FSWEmbedding-901"><span class="linenos"> 901</span></a>        <span class="n">freqs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,)</span> <span class="c1"># Note: Changing this to (self.num_frequencies, 1) yields incorrect results in self.forward()</span>
</span><span id="FSWEmbedding-902"><a href="#FSWEmbedding-902"><span class="linenos"> 902</span></a>
</span><span id="FSWEmbedding-903"><a href="#FSWEmbedding-903"><span class="linenos"> 903</span></a>        <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-904"><a href="#FSWEmbedding-904"><span class="linenos"> 904</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-905"><a href="#FSWEmbedding-905"><span class="linenos"> 905</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized 0 frequencies&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-906"><a href="#FSWEmbedding-906"><span class="linenos"> 906</span></a>
</span><span id="FSWEmbedding-907"><a href="#FSWEmbedding-907"><span class="linenos"> 907</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
</span><span id="FSWEmbedding-908"><a href="#FSWEmbedding-908"><span class="linenos"> 908</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">),</span> <span class="s1">&#39;frequency_init cannot be infinite&#39;</span>
</span><span id="FSWEmbedding-909"><a href="#FSWEmbedding-909"><span class="linenos"> 909</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">),</span> <span class="s1">&#39;frequency_init cannot be NaN&#39;</span>
</span><span id="FSWEmbedding-910"><a href="#FSWEmbedding-910"><span class="linenos"> 910</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequency_init</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-911"><a href="#FSWEmbedding-911"><span class="linenos"> 911</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> frequencies to </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">frequency_init</span><span class="p">))</span>
</span><span id="FSWEmbedding-912"><a href="#FSWEmbedding-912"><span class="linenos"> 912</span></a>
</span><span id="FSWEmbedding-913"><a href="#FSWEmbedding-913"><span class="linenos"> 913</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span><span id="FSWEmbedding-914"><a href="#FSWEmbedding-914"><span class="linenos"> 914</span></a>            <span class="c1"># Here frequency_init should have been type-enforced to be a tuple of two real numbers.</span>
</span><span id="FSWEmbedding-915"><a href="#FSWEmbedding-915"><span class="linenos"> 915</span></a>            <span class="c1"># However, it does not prevent the tuple from containing more numbers.</span>
</span><span id="FSWEmbedding-916"><a href="#FSWEmbedding-916"><span class="linenos"> 916</span></a>            <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;When frequency_init is a tuple, it must be of length 2&#39;</span>
</span><span id="FSWEmbedding-917"><a href="#FSWEmbedding-917"><span class="linenos"> 917</span></a>
</span><span id="FSWEmbedding-918"><a href="#FSWEmbedding-918"><span class="linenos"> 918</span></a>            <span class="n">a</span> <span class="o">=</span> <span class="n">frequency_init</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="FSWEmbedding-919"><a href="#FSWEmbedding-919"><span class="linenos"> 919</span></a>            <span class="n">b</span> <span class="o">=</span> <span class="n">frequency_init</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="FSWEmbedding-920"><a href="#FSWEmbedding-920"><span class="linenos"> 920</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="s1">&#39;Received infinite value in frequency_init tuple&#39;</span>
</span><span id="FSWEmbedding-921"><a href="#FSWEmbedding-921"><span class="linenos"> 921</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="s1">&#39;Received NaN value in frequency_init tuple&#39;</span>
</span><span id="FSWEmbedding-922"><a href="#FSWEmbedding-922"><span class="linenos"> 922</span></a>            <span class="k">assert</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;When frequency_init is a tuple, it is required to satisfy frequency_init[0] &lt;= frequency_init[1]&#39;</span>
</span><span id="FSWEmbedding-923"><a href="#FSWEmbedding-923"><span class="linenos"> 923</span></a>
</span><span id="FSWEmbedding-924"><a href="#FSWEmbedding-924"><span class="linenos"> 924</span></a>            <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding-925"><a href="#FSWEmbedding-925"><span class="linenos"> 925</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-926"><a href="#FSWEmbedding-926"><span class="linenos"> 926</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-927"><a href="#FSWEmbedding-927"><span class="linenos"> 927</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-928"><a href="#FSWEmbedding-928"><span class="linenos"> 928</span></a>
</span><span id="FSWEmbedding-929"><a href="#FSWEmbedding-929"><span class="linenos"> 929</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> equispaced frequencies in the interval [</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>
</span><span id="FSWEmbedding-930"><a href="#FSWEmbedding-930"><span class="linenos"> 930</span></a>
</span><span id="FSWEmbedding-931"><a href="#FSWEmbedding-931"><span class="linenos"> 931</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-932"><a href="#FSWEmbedding-932"><span class="linenos"> 932</span></a>            <span class="n">frequency_init</span> <span class="o">=</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">)</span>
</span><span id="FSWEmbedding-933"><a href="#FSWEmbedding-933"><span class="linenos"> 933</span></a>
</span><span id="FSWEmbedding-934"><a href="#FSWEmbedding-934"><span class="linenos"> 934</span></a>            <span class="k">if</span> <span class="n">frequency_init</span> <span class="o">==</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">RANDOM</span><span class="p">:</span>
</span><span id="FSWEmbedding-935"><a href="#FSWEmbedding-935"><span class="linenos"> 935</span></a>                <span class="n">frequencies</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">freqs_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-936"><a href="#FSWEmbedding-936"><span class="linenos"> 936</span></a>                <span class="n">frequencies</span><span class="p">,</span> <span class="n">junk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FSWEmbedding-937"><a href="#FSWEmbedding-937"><span class="linenos"> 937</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">frequencies</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;Unexpected behavior of torch.rand(): Returned a value of 1, whereas values are supposed to be in [0,1)&quot;</span>
</span><span id="FSWEmbedding-938"><a href="#FSWEmbedding-938"><span class="linenos"> 938</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">frequencies</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;Unexpected behavior of torch.rand(): Returned a value &gt; 1, whereas values are supposed to be in [0,1)&quot;</span>
</span><span id="FSWEmbedding-939"><a href="#FSWEmbedding-939"><span class="linenos"> 939</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-940"><a href="#FSWEmbedding-940"><span class="linenos"> 940</span></a>
</span><span id="FSWEmbedding-941"><a href="#FSWEmbedding-941"><span class="linenos"> 941</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> random frequencies i.i.d. with density f(x) = 1/(1+x)^2, x</span><span class="se">\u2265</span><span class="s1">0&#39;</span> <span class="o">%</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-942"><a href="#FSWEmbedding-942"><span class="linenos"> 942</span></a>
</span><span id="FSWEmbedding-943"><a href="#FSWEmbedding-943"><span class="linenos"> 943</span></a>            <span class="k">elif</span> <span class="n">frequency_init</span> <span class="o">==</span> <span class="n">FrequencyInitMethod</span><span class="o">.</span><span class="n">EVEN</span><span class="p">:</span>
</span><span id="FSWEmbedding-944"><a href="#FSWEmbedding-944"><span class="linenos"> 944</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="p">(</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">freqs_shape</span><span class="p">)</span> <span class="p">)</span> <span class="o">/</span> <span class="n">num_frequencies</span>
</span><span id="FSWEmbedding-945"><a href="#FSWEmbedding-945"><span class="linenos"> 945</span></a>                <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-946"><a href="#FSWEmbedding-946"><span class="linenos"> 946</span></a>                <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;- Initialized </span><span class="si">%d</span><span class="s1"> frequencies spread evenly in [</span><span class="si">%g</span><span class="s1">, </span><span class="si">%g</span><span class="s1">] according to probability density&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">frequencies</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</span><span id="FSWEmbedding-947"><a href="#FSWEmbedding-947"><span class="linenos"> 947</span></a>
</span><span id="FSWEmbedding-948"><a href="#FSWEmbedding-948"><span class="linenos"> 948</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-949"><a href="#FSWEmbedding-949"><span class="linenos"> 949</span></a>                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Invalid value for argument frequency_init; expected number, tuple (a,b) of numbers denoting an interval, </span><span class="se">\&#39;</span><span class="s1">random</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">spread</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding-950"><a href="#FSWEmbedding-950"><span class="linenos"> 950</span></a>
</span><span id="FSWEmbedding-951"><a href="#FSWEmbedding-951"><span class="linenos"> 951</span></a>        <span class="c1"># Detect nan and inf entries in frequencies</span>
</span><span id="FSWEmbedding-952"><a href="#FSWEmbedding-952"><span class="linenos"> 952</span></a>        <span class="k">if</span> <span class="n">num_frequencies</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-953"><a href="#FSWEmbedding-953"><span class="linenos"> 953</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found infs in frequencies&quot;</span>
</span><span id="FSWEmbedding-954"><a href="#FSWEmbedding-954"><span class="linenos"> 954</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;Found nans in frequencies&quot;</span>
</span><span id="FSWEmbedding-955"><a href="#FSWEmbedding-955"><span class="linenos"> 955</span></a>
</span><span id="FSWEmbedding-956"><a href="#FSWEmbedding-956"><span class="linenos"> 956</span></a>        <span class="c1"># C. Generate bias vector. Always initialized to zero.</span>
</span><span id="FSWEmbedding-957"><a href="#FSWEmbedding-957"><span class="linenos"> 957</span></a>        <span class="k">if</span> <span class="n">cartesian_mode</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding-958"><a href="#FSWEmbedding-958"><span class="linenos"> 958</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-959"><a href="#FSWEmbedding-959"><span class="linenos"> 959</span></a>        <span class="k">elif</span> <span class="n">cartesian_mode</span> <span class="ow">and</span> <span class="n">flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding-960"><a href="#FSWEmbedding-960"><span class="linenos"> 960</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span><span class="o">*</span><span class="n">num_frequencies</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span><span class="p">,)</span>
</span><span id="FSWEmbedding-961"><a href="#FSWEmbedding-961"><span class="linenos"> 961</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-962"><a href="#FSWEmbedding-962"><span class="linenos"> 962</span></a>            <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_slices</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span><span class="p">,)</span>
</span><span id="FSWEmbedding-963"><a href="#FSWEmbedding-963"><span class="linenos"> 963</span></a>
</span><span id="FSWEmbedding-964"><a href="#FSWEmbedding-964"><span class="linenos"> 964</span></a>        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-965"><a href="#FSWEmbedding-965"><span class="linenos"> 965</span></a>
</span><span id="FSWEmbedding-966"><a href="#FSWEmbedding-966"><span class="linenos"> 966</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding-967"><a href="#FSWEmbedding-967"><span class="linenos"> 967</span></a>
</span><span id="FSWEmbedding-968"><a href="#FSWEmbedding-968"><span class="linenos"> 968</span></a>        <span class="k">return</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">bias</span>
</span><span id="FSWEmbedding-969"><a href="#FSWEmbedding-969"><span class="linenos"> 969</span></a>
</span><span id="FSWEmbedding-970"><a href="#FSWEmbedding-970"><span class="linenos"> 970</span></a>
</span><span id="FSWEmbedding-971"><a href="#FSWEmbedding-971"><span class="linenos"> 971</span></a>
</span><span id="FSWEmbedding-972"><a href="#FSWEmbedding-972"><span class="linenos"> 972</span></a>    <span class="c1"># Spreads the frequencies on an interval centered at &#39;center&#39; with the given radius, in an equispaced manner.</span>
</span><span id="FSWEmbedding-973"><a href="#FSWEmbedding-973"><span class="linenos"> 973</span></a>    <span class="c1"># This might be useful when using the embedding for graph message passing with learnable_slices=True, as the magnitude of the</span>
</span><span id="FSWEmbedding-974"><a href="#FSWEmbedding-974"><span class="linenos"> 974</span></a>    <span class="c1"># slice vectors already determines the effective frequency, and having a very high max-frequency-to-low-frequency ratio</span>
</span><span id="FSWEmbedding-975"><a href="#FSWEmbedding-975"><span class="linenos"> 975</span></a>    <span class="c1"># may impede the optimization due to ill conditioning.</span>
</span><span id="FSWEmbedding-976"><a href="#FSWEmbedding-976"><span class="linenos"> 976</span></a>
</span><span id="FSWEmbedding-977"><a href="#FSWEmbedding-977"><span class="linenos"> 977</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_spread_freqs_at_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">center</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="FSWEmbedding-978"><a href="#FSWEmbedding-978"><span class="linenos"> 978</span></a>        <span class="k">assert</span> <span class="n">radius</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-979"><a href="#FSWEmbedding-979"><span class="linenos"> 979</span></a>
</span><span id="FSWEmbedding-980"><a href="#FSWEmbedding-980"><span class="linenos"> 980</span></a>        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">radius</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="FSWEmbedding-981"><a href="#FSWEmbedding-981"><span class="linenos"> 981</span></a>            <span class="n">freqs_new</span> <span class="o">=</span> <span class="n">center</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-982"><a href="#FSWEmbedding-982"><span class="linenos"> 982</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-983"><a href="#FSWEmbedding-983"><span class="linenos"> 983</span></a>            <span class="n">spread</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">-</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-984"><a href="#FSWEmbedding-984"><span class="linenos"> 984</span></a>            <span class="n">spread</span> <span class="o">=</span> <span class="n">spread</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-985"><a href="#FSWEmbedding-985"><span class="linenos"> 985</span></a>            <span class="n">freqs_new</span> <span class="o">=</span> <span class="n">center</span> <span class="o">+</span> <span class="n">radius</span> <span class="o">*</span> <span class="n">spread</span>
</span><span id="FSWEmbedding-986"><a href="#FSWEmbedding-986"><span class="linenos"> 986</span></a>
</span><span id="FSWEmbedding-987"><a href="#FSWEmbedding-987"><span class="linenos"> 987</span></a>        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</span><span id="FSWEmbedding-988"><a href="#FSWEmbedding-988"><span class="linenos"> 988</span></a>        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;frequencies&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">freqs_new</span>
</span><span id="FSWEmbedding-989"><a href="#FSWEmbedding-989"><span class="linenos"> 989</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</span><span id="FSWEmbedding-990"><a href="#FSWEmbedding-990"><span class="linenos"> 990</span></a>
</span><span id="FSWEmbedding-991"><a href="#FSWEmbedding-991"><span class="linenos"> 991</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span><span id="FSWEmbedding-992"><a href="#FSWEmbedding-992"><span class="linenos"> 992</span></a>
</span><span id="FSWEmbedding-993"><a href="#FSWEmbedding-993"><span class="linenos"> 993</span></a>
</span><span id="FSWEmbedding-994"><a href="#FSWEmbedding-994"><span class="linenos"> 994</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding-995"><a href="#FSWEmbedding-995"><span class="linenos"> 995</span></a>                <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="FSWEmbedding-996"><a href="#FSWEmbedding-996"><span class="linenos"> 996</span></a>                <span class="n">W</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="s1">&#39;unit&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding-997"><a href="#FSWEmbedding-997"><span class="linenos"> 997</span></a>                <span class="n">X_edge</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding-998"><a href="#FSWEmbedding-998"><span class="linenos"> 998</span></a>                <span class="n">graph_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-999"><a href="#FSWEmbedding-999"><span class="linenos"> 999</span></a>                <span class="n">max_parallel_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding-1000"><a href="#FSWEmbedding-1000"><span class="linenos">1000</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-1001"><a href="#FSWEmbedding-1001"><span class="linenos">1001</span></a><span class="sd">        Compute the FSW embedding of an input multiset, measure, or graph.</span>
</span><span id="FSWEmbedding-1002"><a href="#FSWEmbedding-1002"><span class="linenos">1002</span></a>
</span><span id="FSWEmbedding-1003"><a href="#FSWEmbedding-1003"><span class="linenos">1003</span></a><span class="sd">        This method maps input sets of vectors (optionally weighted) to vectors in ℝ^{d_out}</span>
</span><span id="FSWEmbedding-1004"><a href="#FSWEmbedding-1004"><span class="linenos">1004</span></a><span class="sd">        using the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and</span>
</span><span id="FSWEmbedding-1005"><a href="#FSWEmbedding-1005"><span class="linenos">1005</span></a><span class="sd">        graph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</span>
</span><span id="FSWEmbedding-1006"><a href="#FSWEmbedding-1006"><span class="linenos">1006</span></a>
</span><span id="FSWEmbedding-1007"><a href="#FSWEmbedding-1007"><span class="linenos">1007</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding-1008"><a href="#FSWEmbedding-1008"><span class="linenos">1008</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding-1009"><a href="#FSWEmbedding-1009"><span class="linenos">1009</span></a><span class="sd">        X : torch.Tensor</span>
</span><span id="FSWEmbedding-1010"><a href="#FSWEmbedding-1010"><span class="linenos">1010</span></a><span class="sd">            Input tensor of shape `(n, d_in)` or `(..., n, d_in)` for batched input.</span>
</span><span id="FSWEmbedding-1011"><a href="#FSWEmbedding-1011"><span class="linenos">1011</span></a><span class="sd">        W : torch.Tensor or {&#39;unit&#39;, &#39;uniform&#39;}, default=&#39;unit&#39;</span>
</span><span id="FSWEmbedding-1012"><a href="#FSWEmbedding-1012"><span class="linenos">1012</span></a><span class="sd">            Weights tensor of shape `(n,)` or `(..., n)` corresponding to point importance.</span>
</span><span id="FSWEmbedding-1013"><a href="#FSWEmbedding-1013"><span class="linenos">1013</span></a><span class="sd">            If set to `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights of `1/n` are assumed.</span>
</span><span id="FSWEmbedding-1014"><a href="#FSWEmbedding-1014"><span class="linenos">1014</span></a><span class="sd">        X_edge : torch.Tensor, optional</span>
</span><span id="FSWEmbedding-1015"><a href="#FSWEmbedding-1015"><span class="linenos">1015</span></a><span class="sd">            Optional edge feature tensor. Required if `d_edge &gt; 0` was set at initialization.</span>
</span><span id="FSWEmbedding-1016"><a href="#FSWEmbedding-1016"><span class="linenos">1016</span></a><span class="sd">        graph_mode : bool, default=False</span>
</span><span id="FSWEmbedding-1017"><a href="#FSWEmbedding-1017"><span class="linenos">1017</span></a><span class="sd">            If True, interprets `W` as an adjacency matrix and computes a neighbor-aggregated</span>
</span><span id="FSWEmbedding-1018"><a href="#FSWEmbedding-1018"><span class="linenos">1018</span></a><span class="sd">            embedding.</span>
</span><span id="FSWEmbedding-1019"><a href="#FSWEmbedding-1019"><span class="linenos">1019</span></a><span class="sd">        max_parallel_slices : int, optional</span>
</span><span id="FSWEmbedding-1020"><a href="#FSWEmbedding-1020"><span class="linenos">1020</span></a><span class="sd">            Limits the number of slices processed in parallel. Reduces memory usage by computing</span>
</span><span id="FSWEmbedding-1021"><a href="#FSWEmbedding-1021"><span class="linenos">1021</span></a><span class="sd">            the embedding in smaller blocks without changing the result.</span>
</span><span id="FSWEmbedding-1022"><a href="#FSWEmbedding-1022"><span class="linenos">1022</span></a>
</span><span id="FSWEmbedding-1023"><a href="#FSWEmbedding-1023"><span class="linenos">1023</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding-1024"><a href="#FSWEmbedding-1024"><span class="linenos">1024</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding-1025"><a href="#FSWEmbedding-1025"><span class="linenos">1025</span></a><span class="sd">        torch.Tensor</span>
</span><span id="FSWEmbedding-1026"><a href="#FSWEmbedding-1026"><span class="linenos">1026</span></a><span class="sd">            The embedding tensor. Shape depends on the mode:</span>
</span><span id="FSWEmbedding-1027"><a href="#FSWEmbedding-1027"><span class="linenos">1027</span></a><span class="sd">            - `(d_out,)` or `(..., d_out)` in standard mode.</span>
</span><span id="FSWEmbedding-1028"><a href="#FSWEmbedding-1028"><span class="linenos">1028</span></a><span class="sd">            - `(..., num_slices, num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=False`.</span>
</span><span id="FSWEmbedding-1029"><a href="#FSWEmbedding-1029"><span class="linenos">1029</span></a><span class="sd">            - `(..., num_slices * num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=True`.</span>
</span><span id="FSWEmbedding-1030"><a href="#FSWEmbedding-1030"><span class="linenos">1030</span></a>
</span><span id="FSWEmbedding-1031"><a href="#FSWEmbedding-1031"><span class="linenos">1031</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding-1032"><a href="#FSWEmbedding-1032"><span class="linenos">1032</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding-1033"><a href="#FSWEmbedding-1033"><span class="linenos">1033</span></a><span class="sd">        Multisets and distributions:</span>
</span><span id="FSWEmbedding-1034"><a href="#FSWEmbedding-1034"><span class="linenos">1034</span></a><span class="sd">            If `X` is `(n, d_in)` and `W` is `(n,)`, the pair represents a weighted point cloud.</span>
</span><span id="FSWEmbedding-1035"><a href="#FSWEmbedding-1035"><span class="linenos">1035</span></a><span class="sd">            Weights must be non-negative with positive total mass.</span>
</span><span id="FSWEmbedding-1036"><a href="#FSWEmbedding-1036"><span class="linenos">1036</span></a><span class="sd">            If `W` is `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights are used internally.</span>
</span><span id="FSWEmbedding-1037"><a href="#FSWEmbedding-1037"><span class="linenos">1037</span></a>
</span><span id="FSWEmbedding-1038"><a href="#FSWEmbedding-1038"><span class="linenos">1038</span></a><span class="sd">        Batching:</span>
</span><span id="FSWEmbedding-1039"><a href="#FSWEmbedding-1039"><span class="linenos">1039</span></a><span class="sd">            Input tensors may include leading batch dimensions. For `X` of shape `(..., n, d_in)`</span>
</span><span id="FSWEmbedding-1040"><a href="#FSWEmbedding-1040"><span class="linenos">1040</span></a><span class="sd">            and `W` of shape `(..., n)`, the output shape is `(..., d_out)`.</span>
</span><span id="FSWEmbedding-1041"><a href="#FSWEmbedding-1041"><span class="linenos">1041</span></a>
</span><span id="FSWEmbedding-1042"><a href="#FSWEmbedding-1042"><span class="linenos">1042</span></a><span class="sd">        Graph mode:</span>
</span><span id="FSWEmbedding-1043"><a href="#FSWEmbedding-1043"><span class="linenos">1043</span></a><span class="sd">            When `graph_mode=True`, `W` must be of shape `(..., n_recipients, n)` and `X` of</span>
</span><span id="FSWEmbedding-1044"><a href="#FSWEmbedding-1044"><span class="linenos">1044</span></a><span class="sd">            shape `(..., n, d_in)` or broadcastable to that. The output will be</span>
</span><span id="FSWEmbedding-1045"><a href="#FSWEmbedding-1045"><span class="linenos">1045</span></a><span class="sd">            `(..., n_recipients, d_out)`, where each vector represents a weighted embedding of</span>
</span><span id="FSWEmbedding-1046"><a href="#FSWEmbedding-1046"><span class="linenos">1046</span></a><span class="sd">            neighboring nodes. This avoids expanding `X` across `n_recipients` explicitly.</span>
</span><span id="FSWEmbedding-1047"><a href="#FSWEmbedding-1047"><span class="linenos">1047</span></a>
</span><span id="FSWEmbedding-1048"><a href="#FSWEmbedding-1048"><span class="linenos">1048</span></a><span class="sd">        Cartesian mode:</span>
</span><span id="FSWEmbedding-1049"><a href="#FSWEmbedding-1049"><span class="linenos">1049</span></a><span class="sd">            If `d_out` is not specified but `num_slices` and `num_frequencies` are, the embedding</span>
</span><span id="FSWEmbedding-1050"><a href="#FSWEmbedding-1050"><span class="linenos">1050</span></a><span class="sd">            is computed over a Cartesian product. The output shape is:</span>
</span><span id="FSWEmbedding-1051"><a href="#FSWEmbedding-1051"><span class="linenos">1051</span></a><span class="sd">                - `(..., num_slices, num_frequencies)` if `flatten_cartesian_axes=False`</span>
</span><span id="FSWEmbedding-1052"><a href="#FSWEmbedding-1052"><span class="linenos">1052</span></a><span class="sd">                - `(..., num_slices * num_frequencies)` if `flatten_cartesian_axes=True`</span>
</span><span id="FSWEmbedding-1053"><a href="#FSWEmbedding-1053"><span class="linenos">1053</span></a>
</span><span id="FSWEmbedding-1054"><a href="#FSWEmbedding-1054"><span class="linenos">1054</span></a><span class="sd">        Slice serialization:</span>
</span><span id="FSWEmbedding-1055"><a href="#FSWEmbedding-1055"><span class="linenos">1055</span></a><span class="sd">            If `max_parallel_slices=t` is set, the computation is performed in blocks of size `t`,</span>
</span><span id="FSWEmbedding-1056"><a href="#FSWEmbedding-1056"><span class="linenos">1056</span></a><span class="sd">            reducing memory complexity by a factor of `num_slices / t`. The output remains unchanged.</span>
</span><span id="FSWEmbedding-1057"><a href="#FSWEmbedding-1057"><span class="linenos">1057</span></a>
</span><span id="FSWEmbedding-1058"><a href="#FSWEmbedding-1058"><span class="linenos">1058</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding-1059"><a href="#FSWEmbedding-1059"><span class="linenos">1059</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding-1060"><a href="#FSWEmbedding-1060"><span class="linenos">1060</span></a><span class="sd">        FSWEmbedding.__init__ : Constructor for model configuration options.</span>
</span><span id="FSWEmbedding-1061"><a href="#FSWEmbedding-1061"><span class="linenos">1061</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding-1062"><a href="#FSWEmbedding-1062"><span class="linenos">1062</span></a>
</span><span id="FSWEmbedding-1063"><a href="#FSWEmbedding-1063"><span class="linenos">1063</span></a>
</span><span id="FSWEmbedding-1064"><a href="#FSWEmbedding-1064"><span class="linenos">1064</span></a>        <span class="c1"># Verify slices and frequencies at each forward pass if they are learnable</span>
</span><span id="FSWEmbedding-1065"><a href="#FSWEmbedding-1065"><span class="linenos">1065</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="FSWEmbedding-1066"><a href="#FSWEmbedding-1066"><span class="linenos">1066</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain NaNs&#39;</span>
</span><span id="FSWEmbedding-1067"><a href="#FSWEmbedding-1067"><span class="linenos">1067</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain infs&#39;</span>
</span><span id="FSWEmbedding-1068"><a href="#FSWEmbedding-1068"><span class="linenos">1068</span></a>            <span class="c1"># Note: We allow them to contain zero vectors when they are learnable, in case i.e. when sparsity is desired</span>
</span><span id="FSWEmbedding-1069"><a href="#FSWEmbedding-1069"><span class="linenos">1069</span></a>            <span class="c1"># assert not (self.slice_vectors == 0).all(dim=1).any(), &#39;Slice vectors contain a zero vector&#39;</span>
</span><span id="FSWEmbedding-1070"><a href="#FSWEmbedding-1070"><span class="linenos">1070</span></a>
</span><span id="FSWEmbedding-1071"><a href="#FSWEmbedding-1071"><span class="linenos">1071</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding-1072"><a href="#FSWEmbedding-1072"><span class="linenos">1072</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain NaNs&#39;</span>
</span><span id="FSWEmbedding-1073"><a href="#FSWEmbedding-1073"><span class="linenos">1073</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain infs&#39;</span>
</span><span id="FSWEmbedding-1074"><a href="#FSWEmbedding-1074"><span class="linenos">1074</span></a>
</span><span id="FSWEmbedding-1075"><a href="#FSWEmbedding-1075"><span class="linenos">1075</span></a>        <span class="c1">### A. Verify input types and content</span>
</span><span id="FSWEmbedding-1076"><a href="#FSWEmbedding-1076"><span class="linenos">1076</span></a>
</span><span id="FSWEmbedding-1077"><a href="#FSWEmbedding-1077"><span class="linenos">1077</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="FSWEmbedding-1078"><a href="#FSWEmbedding-1078"><span class="linenos">1078</span></a>
</span><span id="FSWEmbedding-1079"><a href="#FSWEmbedding-1079"><span class="linenos">1079</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1080"><a href="#FSWEmbedding-1080"><span class="linenos">1080</span></a>            <span class="k">assert</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="s1">&#39;d_edge &gt; 0 (given at initialization) necessitates graph_mode=True on forward call&#39;</span>
</span><span id="FSWEmbedding-1081"><a href="#FSWEmbedding-1081"><span class="linenos">1081</span></a>            <span class="k">assert</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;X_edge must be provided since d_edge &gt; 0&#39;</span>
</span><span id="FSWEmbedding-1082"><a href="#FSWEmbedding-1082"><span class="linenos">1082</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1083"><a href="#FSWEmbedding-1083"><span class="linenos">1083</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;X_edge should be None or empty since d_edge == 0&#39;</span>
</span><span id="FSWEmbedding-1084"><a href="#FSWEmbedding-1084"><span class="linenos">1084</span></a>            <span class="n">X_edge</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-1085"><a href="#FSWEmbedding-1085"><span class="linenos">1085</span></a>
</span><span id="FSWEmbedding-1086"><a href="#FSWEmbedding-1086"><span class="linenos">1086</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;X must be a pytorch tensor. Instead got type </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span><span id="FSWEmbedding-1087"><a href="#FSWEmbedding-1087"><span class="linenos">1087</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="ow">or</span> <span class="n">W</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">},</span> <span class="s1">&#39;W must be a pytorch tensor, </span><span class="se">\&#39;</span><span class="s1">unit</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">uniform</span><span class="se">\&#39;</span><span class="s1">&#39;</span>
</span><span id="FSWEmbedding-1088"><a href="#FSWEmbedding-1088"><span class="linenos">1088</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1089"><a href="#FSWEmbedding-1089"><span class="linenos">1089</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1090"><a href="#FSWEmbedding-1090"><span class="linenos">1090</span></a>
</span><span id="FSWEmbedding-1091"><a href="#FSWEmbedding-1091"><span class="linenos">1091</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1092"><a href="#FSWEmbedding-1092"><span class="linenos">1092</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;The entries of X cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding-1093"><a href="#FSWEmbedding-1093"><span class="linenos">1093</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X must be finite&quot;</span>
</span><span id="FSWEmbedding-1094"><a href="#FSWEmbedding-1094"><span class="linenos">1094</span></a>
</span><span id="FSWEmbedding-1095"><a href="#FSWEmbedding-1095"><span class="linenos">1095</span></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="FSWEmbedding-1096"><a href="#FSWEmbedding-1096"><span class="linenos">1096</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1097"><a href="#FSWEmbedding-1097"><span class="linenos">1097</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1098"><a href="#FSWEmbedding-1098"><span class="linenos">1098</span></a>
</span><span id="FSWEmbedding-1099"><a href="#FSWEmbedding-1099"><span class="linenos">1099</span></a>            <span class="c1"># Check if W is sparse. If so, ensure that W is of the correct layout.</span>
</span><span id="FSWEmbedding-1100"><a href="#FSWEmbedding-1100"><span class="linenos">1100</span></a>            <span class="c1"># Note: Strangely enough, sparse tensors of layouts other than COO (e.g. CSR) may have is_sparse=False.</span>
</span><span id="FSWEmbedding-1101"><a href="#FSWEmbedding-1101"><span class="linenos">1101</span></a>            <span class="c1">#       This may lead us to mistakenly treat a, e.g. W that is sparse CSR as dense.</span>
</span><span id="FSWEmbedding-1102"><a href="#FSWEmbedding-1102"><span class="linenos">1102</span></a>            <span class="c1">#       Currently there is no is_dense() function in torch, so reading the layout string directly is the second best.</span>
</span><span id="FSWEmbedding-1103"><a href="#FSWEmbedding-1103"><span class="linenos">1103</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="FSWEmbedding-1104"><a href="#FSWEmbedding-1104"><span class="linenos">1104</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse W has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1105"><a href="#FSWEmbedding-1105"><span class="linenos">1105</span></a>
</span><span id="FSWEmbedding-1106"><a href="#FSWEmbedding-1106"><span class="linenos">1106</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse W must be coalesced&#39;</span>
</span><span id="FSWEmbedding-1107"><a href="#FSWEmbedding-1107"><span class="linenos">1107</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;W.dense_dim() must be zero&#39;</span>
</span><span id="FSWEmbedding-1108"><a href="#FSWEmbedding-1108"><span class="linenos">1108</span></a>
</span><span id="FSWEmbedding-1109"><a href="#FSWEmbedding-1109"><span class="linenos">1109</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1110"><a href="#FSWEmbedding-1110"><span class="linenos">1110</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="FSWEmbedding-1111"><a href="#FSWEmbedding-1111"><span class="linenos">1111</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1112"><a href="#FSWEmbedding-1112"><span class="linenos">1112</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1113"><a href="#FSWEmbedding-1113"><span class="linenos">1113</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span>
</span><span id="FSWEmbedding-1114"><a href="#FSWEmbedding-1114"><span class="linenos">1114</span></a>
</span><span id="FSWEmbedding-1115"><a href="#FSWEmbedding-1115"><span class="linenos">1115</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1116"><a href="#FSWEmbedding-1116"><span class="linenos">1116</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W_vals</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding-1117"><a href="#FSWEmbedding-1117"><span class="linenos">1117</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;W cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding-1118"><a href="#FSWEmbedding-1118"><span class="linenos">1118</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be finite&quot;</span>
</span><span id="FSWEmbedding-1119"><a href="#FSWEmbedding-1119"><span class="linenos">1119</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">W_vals</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be nonnegative&quot;</span>
</span><span id="FSWEmbedding-1120"><a href="#FSWEmbedding-1120"><span class="linenos">1120</span></a>                <span class="k">del</span> <span class="n">W_vals</span>
</span><span id="FSWEmbedding-1121"><a href="#FSWEmbedding-1121"><span class="linenos">1121</span></a>
</span><span id="FSWEmbedding-1122"><a href="#FSWEmbedding-1122"><span class="linenos">1122</span></a>        <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-1123"><a href="#FSWEmbedding-1123"><span class="linenos">1123</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;When X_edge is provided, W must be provided explicitly&#39;</span>
</span><span id="FSWEmbedding-1124"><a href="#FSWEmbedding-1124"><span class="linenos">1124</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1125"><a href="#FSWEmbedding-1125"><span class="linenos">1125</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1126"><a href="#FSWEmbedding-1126"><span class="linenos">1126</span></a>
</span><span id="FSWEmbedding-1127"><a href="#FSWEmbedding-1127"><span class="linenos">1127</span></a>            <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="FSWEmbedding-1128"><a href="#FSWEmbedding-1128"><span class="linenos">1128</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse X_edge has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1129"><a href="#FSWEmbedding-1129"><span class="linenos">1129</span></a>
</span><span id="FSWEmbedding-1130"><a href="#FSWEmbedding-1130"><span class="linenos">1130</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must be coalesced&#39;</span>
</span><span id="FSWEmbedding-1131"><a href="#FSWEmbedding-1131"><span class="linenos">1131</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 or 0&#39;</span>
</span><span id="FSWEmbedding-1132"><a href="#FSWEmbedding-1132"><span class="linenos">1132</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 since d_edge &gt; 1&#39;</span>
</span><span id="FSWEmbedding-1133"><a href="#FSWEmbedding-1133"><span class="linenos">1133</span></a>
</span><span id="FSWEmbedding-1134"><a href="#FSWEmbedding-1134"><span class="linenos">1134</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1135"><a href="#FSWEmbedding-1135"><span class="linenos">1135</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="FSWEmbedding-1136"><a href="#FSWEmbedding-1136"><span class="linenos">1136</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1137"><a href="#FSWEmbedding-1137"><span class="linenos">1137</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1138"><a href="#FSWEmbedding-1138"><span class="linenos">1138</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span>
</span><span id="FSWEmbedding-1139"><a href="#FSWEmbedding-1139"><span class="linenos">1139</span></a>
</span><span id="FSWEmbedding-1140"><a href="#FSWEmbedding-1140"><span class="linenos">1140</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1141"><a href="#FSWEmbedding-1141"><span class="linenos">1141</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;X_edge_vals cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding-1142"><a href="#FSWEmbedding-1142"><span class="linenos">1142</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X_edge_vals must be finite&quot;</span>
</span><span id="FSWEmbedding-1143"><a href="#FSWEmbedding-1143"><span class="linenos">1143</span></a>                <span class="k">del</span> <span class="n">X_edge_vals</span>
</span><span id="FSWEmbedding-1144"><a href="#FSWEmbedding-1144"><span class="linenos">1144</span></a>
</span><span id="FSWEmbedding-1145"><a href="#FSWEmbedding-1145"><span class="linenos">1145</span></a>            <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;X_edge and W must either both or neither be sparse&#39;</span>
</span><span id="FSWEmbedding-1146"><a href="#FSWEmbedding-1146"><span class="linenos">1146</span></a>
</span><span id="FSWEmbedding-1147"><a href="#FSWEmbedding-1147"><span class="linenos">1147</span></a>
</span><span id="FSWEmbedding-1148"><a href="#FSWEmbedding-1148"><span class="linenos">1148</span></a>        <span class="c1">### B. Verify input sizes</span>
</span><span id="FSWEmbedding-1149"><a href="#FSWEmbedding-1149"><span class="linenos">1149</span></a>
</span><span id="FSWEmbedding-1150"><a href="#FSWEmbedding-1150"><span class="linenos">1150</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;X must be a tensor of order at least 2&quot;</span>
</span><span id="FSWEmbedding-1151"><a href="#FSWEmbedding-1151"><span class="linenos">1151</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="s2">&quot;The last dimension of X must equal d_in=</span><span class="si">%d</span><span class="s2">. Instead got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="FSWEmbedding-1152"><a href="#FSWEmbedding-1152"><span class="linenos">1152</span></a>
</span><span id="FSWEmbedding-1153"><a href="#FSWEmbedding-1153"><span class="linenos">1153</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1154"><a href="#FSWEmbedding-1154"><span class="linenos">1154</span></a>            <span class="c1"># batch_dims contains everything that precedes (n,d_in) in X.shape</span>
</span><span id="FSWEmbedding-1155"><a href="#FSWEmbedding-1155"><span class="linenos">1155</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="FSWEmbedding-1156"><a href="#FSWEmbedding-1156"><span class="linenos">1156</span></a>            <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)]</span>
</span><span id="FSWEmbedding-1157"><a href="#FSWEmbedding-1157"><span class="linenos">1157</span></a>
</span><span id="FSWEmbedding-1158"><a href="#FSWEmbedding-1158"><span class="linenos">1158</span></a>            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="FSWEmbedding-1159"><a href="#FSWEmbedding-1159"><span class="linenos">1159</span></a>                <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
</span><span id="FSWEmbedding-1160"><a href="#FSWEmbedding-1160"><span class="linenos">1160</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (Perhaps missing argument graph_mode=True?)&quot;</span>
</span><span id="FSWEmbedding-1161"><a href="#FSWEmbedding-1161"><span class="linenos">1161</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1162"><a href="#FSWEmbedding-1162"><span class="linenos">1162</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (unless graph_mode=True)&quot;</span>
</span><span id="FSWEmbedding-1163"><a href="#FSWEmbedding-1163"><span class="linenos">1163</span></a>
</span><span id="FSWEmbedding-1164"><a href="#FSWEmbedding-1164"><span class="linenos">1164</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">err_str</span>
</span><span id="FSWEmbedding-1165"><a href="#FSWEmbedding-1165"><span class="linenos">1165</span></a>
</span><span id="FSWEmbedding-1166"><a href="#FSWEmbedding-1166"><span class="linenos">1166</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;unit&#39;</span><span class="p">:</span>
</span><span id="FSWEmbedding-1167"><a href="#FSWEmbedding-1167"><span class="linenos">1167</span></a>                <span class="c1"># Initialize with unit weights</span>
</span><span id="FSWEmbedding-1168"><a href="#FSWEmbedding-1168"><span class="linenos">1168</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-1169"><a href="#FSWEmbedding-1169"><span class="linenos">1169</span></a>
</span><span id="FSWEmbedding-1170"><a href="#FSWEmbedding-1170"><span class="linenos">1170</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
</span><span id="FSWEmbedding-1171"><a href="#FSWEmbedding-1171"><span class="linenos">1171</span></a>                <span class="c1"># Initialize with uniform weights</span>
</span><span id="FSWEmbedding-1172"><a href="#FSWEmbedding-1172"><span class="linenos">1172</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-1173"><a href="#FSWEmbedding-1173"><span class="linenos">1173</span></a>
</span><span id="FSWEmbedding-1174"><a href="#FSWEmbedding-1174"><span class="linenos">1174</span></a>        <span class="k">elif</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1175"><a href="#FSWEmbedding-1175"><span class="linenos">1175</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;W must be explicitly provided when graph_mode=True&#39;</span>
</span><span id="FSWEmbedding-1176"><a href="#FSWEmbedding-1176"><span class="linenos">1176</span></a>
</span><span id="FSWEmbedding-1177"><a href="#FSWEmbedding-1177"><span class="linenos">1177</span></a>            <span class="c1"># batch_dims contains everything that precedes (nRecipients, n) in W.shape</span>
</span><span id="FSWEmbedding-1178"><a href="#FSWEmbedding-1178"><span class="linenos">1178</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="FSWEmbedding-1179"><a href="#FSWEmbedding-1179"><span class="linenos">1179</span></a>            <span class="n">nRecipients</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span><span id="FSWEmbedding-1180"><a href="#FSWEmbedding-1180"><span class="linenos">1180</span></a>            <span class="c1">#n = W.shape[-1]</span>
</span><span id="FSWEmbedding-1181"><a href="#FSWEmbedding-1181"><span class="linenos">1181</span></a>
</span><span id="FSWEmbedding-1182"><a href="#FSWEmbedding-1182"><span class="linenos">1182</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span> <span class="s2">&quot;Shape mismatch between X and W: When graph_mode=True, if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,n,d_in)&quot;</span>
</span><span id="FSWEmbedding-1183"><a href="#FSWEmbedding-1183"><span class="linenos">1183</span></a>
</span><span id="FSWEmbedding-1184"><a href="#FSWEmbedding-1184"><span class="linenos">1184</span></a>            <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-1185"><a href="#FSWEmbedding-1185"><span class="linenos">1185</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># For PyCharm to know</span>
</span><span id="FSWEmbedding-1186"><a href="#FSWEmbedding-1186"><span class="linenos">1186</span></a>
</span><span id="FSWEmbedding-1187"><a href="#FSWEmbedding-1187"><span class="linenos">1187</span></a>                <span class="c1"># Verify that X_edge has the right shape and is compatible with W</span>
</span><span id="FSWEmbedding-1188"><a href="#FSWEmbedding-1188"><span class="linenos">1188</span></a>                <span class="k">assert</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">or</span>
</span><span id="FSWEmbedding-1189"><a href="#FSWEmbedding-1189"><span class="linenos">1189</span></a>                        <span class="p">((</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">))),</span> <span class="p">(</span>
</span><span id="FSWEmbedding-1190"><a href="#FSWEmbedding-1190"><span class="linenos">1190</span></a>                    <span class="s2">&quot;Shape mismatch between X_edge and W: if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,nRecipients,n,d_edge) (with the possible exception (b1,b2,...,bk,nRecipients,n) when d_edge=1&quot;</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1191"><a href="#FSWEmbedding-1191"><span class="linenos">1191</span></a>
</span><span id="FSWEmbedding-1192"><a href="#FSWEmbedding-1192"><span class="linenos">1192</span></a>                <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding-1193"><a href="#FSWEmbedding-1193"><span class="linenos">1193</span></a>                    <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;Sparse X_edge must have the same number of values() as W&#39;</span>
</span><span id="FSWEmbedding-1194"><a href="#FSWEmbedding-1194"><span class="linenos">1194</span></a>                    <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding-1195"><a href="#FSWEmbedding-1195"><span class="linenos">1195</span></a>                        <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must have the same nonzero pattern as W&#39;</span>
</span><span id="FSWEmbedding-1196"><a href="#FSWEmbedding-1196"><span class="linenos">1196</span></a>
</span><span id="FSWEmbedding-1197"><a href="#FSWEmbedding-1197"><span class="linenos">1197</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1198"><a href="#FSWEmbedding-1198"><span class="linenos">1198</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_dense_dim</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">)</span>
</span><span id="FSWEmbedding-1199"><a href="#FSWEmbedding-1199"><span class="linenos">1199</span></a>
</span><span id="FSWEmbedding-1200"><a href="#FSWEmbedding-1200"><span class="linenos">1200</span></a>
</span><span id="FSWEmbedding-1201"><a href="#FSWEmbedding-1201"><span class="linenos">1201</span></a>        <span class="c1">### C. Precalculate axis indices and output shape</span>
</span><span id="FSWEmbedding-1202"><a href="#FSWEmbedding-1202"><span class="linenos">1202</span></a>
</span><span id="FSWEmbedding-1203"><a href="#FSWEmbedding-1203"><span class="linenos">1203</span></a>        <span class="c1"># These are the different axes we use to store data for processing. These definitions are repeated in forward_helper()</span>
</span><span id="FSWEmbedding-1204"><a href="#FSWEmbedding-1204"><span class="linenos">1204</span></a>        <span class="c1"># element_axis corresponds to the index of the multiset elements</span>
</span><span id="FSWEmbedding-1205"><a href="#FSWEmbedding-1205"><span class="linenos">1205</span></a>        <span class="c1"># ambspace_axis corresponds to the elements&#39; coordinate index in the ambient space R^d_in</span>
</span><span id="FSWEmbedding-1206"><a href="#FSWEmbedding-1206"><span class="linenos">1206</span></a>        <span class="c1"># After projection, the ambient space coordinates are replaced by the slice number; thus slice_axis=ambspace_axis</span>
</span><span id="FSWEmbedding-1207"><a href="#FSWEmbedding-1207"><span class="linenos">1207</span></a>        <span class="c1"># If we&#39;re in Cartesian mode, the frequencies have their own axis freq_axis, otherwise it is the same axis as slice_axis.</span>
</span><span id="FSWEmbedding-1208"><a href="#FSWEmbedding-1208"><span class="linenos">1208</span></a>        <span class="n">recipient_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># Message-recipient vertices</span>
</span><span id="FSWEmbedding-1209"><a href="#FSWEmbedding-1209"><span class="linenos">1209</span></a>        <span class="n">element_axis</span>  <span class="o">=</span> <span class="n">recipient_axis</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="c1"># In graph mode this axis denotes the message-sender vertices</span>
</span><span id="FSWEmbedding-1210"><a href="#FSWEmbedding-1210"><span class="linenos">1210</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-1211"><a href="#FSWEmbedding-1211"><span class="linenos">1211</span></a>        <span class="n">slice_axis</span>     <span class="o">=</span> <span class="n">ambspace_axis</span>
</span><span id="FSWEmbedding-1212"><a href="#FSWEmbedding-1212"><span class="linenos">1212</span></a>        <span class="c1"># noinspection PyUnusedLocal</span>
</span><span id="FSWEmbedding-1213"><a href="#FSWEmbedding-1213"><span class="linenos">1213</span></a>        <span class="n">freq_axis</span>     <span class="o">=</span> <span class="n">slice_axis</span> <span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="n">slice_axis</span>
</span><span id="FSWEmbedding-1214"><a href="#FSWEmbedding-1214"><span class="linenos">1214</span></a>        <span class="n">output_slice_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="c1"># In the output, the element axis is replaced by the slice axis</span>
</span><span id="FSWEmbedding-1215"><a href="#FSWEmbedding-1215"><span class="linenos">1215</span></a>
</span><span id="FSWEmbedding-1216"><a href="#FSWEmbedding-1216"><span class="linenos">1216</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">=</span>  <span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">nRecipients</span><span class="p">,)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="n">batch_dims</span>
</span><span id="FSWEmbedding-1217"><a href="#FSWEmbedding-1217"><span class="linenos">1217</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,)</span>
</span><span id="FSWEmbedding-1218"><a href="#FSWEmbedding-1218"><span class="linenos">1218</span></a>
</span><span id="FSWEmbedding-1219"><a href="#FSWEmbedding-1219"><span class="linenos">1219</span></a>        <span class="c1">### D. Input is ok. Start working.</span>
</span><span id="FSWEmbedding-1220"><a href="#FSWEmbedding-1220"><span class="linenos">1220</span></a>
</span><span id="FSWEmbedding-1221"><a href="#FSWEmbedding-1221"><span class="linenos">1221</span></a>        <span class="c1"># Calculate W_sum, which contains the total mass of the input measures</span>
</span><span id="FSWEmbedding-1222"><a href="#FSWEmbedding-1222"><span class="linenos">1222</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding-1223"><a href="#FSWEmbedding-1223"><span class="linenos">1223</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding-1224"><a href="#FSWEmbedding-1224"><span class="linenos">1224</span></a>            <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-1225"><a href="#FSWEmbedding-1225"><span class="linenos">1225</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1226"><a href="#FSWEmbedding-1226"><span class="linenos">1226</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sum_sparseToDense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1227"><a href="#FSWEmbedding-1227"><span class="linenos">1227</span></a>
</span><span id="FSWEmbedding-1228"><a href="#FSWEmbedding-1228"><span class="linenos">1228</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1229"><a href="#FSWEmbedding-1229"><span class="linenos">1229</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding-1230"><a href="#FSWEmbedding-1230"><span class="linenos">1230</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FSWEmbedding-1231"><a href="#FSWEmbedding-1231"><span class="linenos">1231</span></a>
</span><span id="FSWEmbedding-1232"><a href="#FSWEmbedding-1232"><span class="linenos">1232</span></a>        <span class="c1"># Total-mass deficit to be compensated for by padding</span>
</span><span id="FSWEmbedding-1233"><a href="#FSWEmbedding-1233"><span class="linenos">1233</span></a>        <span class="n">W_pad</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">-</span> <span class="n">W_sum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="FSWEmbedding-1234"><a href="#FSWEmbedding-1234"><span class="linenos">1234</span></a>
</span><span id="FSWEmbedding-1235"><a href="#FSWEmbedding-1235"><span class="linenos">1235</span></a>        <span class="c1"># Detect weight deficit and augment W and X accordingly</span>
</span><span id="FSWEmbedding-1236"><a href="#FSWEmbedding-1236"><span class="linenos">1236</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">W_pad</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
</span><span id="FSWEmbedding-1237"><a href="#FSWEmbedding-1237"><span class="linenos">1237</span></a>            <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="FSWEmbedding-1238"><a href="#FSWEmbedding-1238"><span class="linenos">1238</span></a>            <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-1239"><a href="#FSWEmbedding-1239"><span class="linenos">1239</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1240"><a href="#FSWEmbedding-1240"><span class="linenos">1240</span></a>
</span><span id="FSWEmbedding-1241"><a href="#FSWEmbedding-1241"><span class="linenos">1241</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding-1242"><a href="#FSWEmbedding-1242"><span class="linenos">1242</span></a>                <span class="c1"># Make sure this works</span>
</span><span id="FSWEmbedding-1243"><a href="#FSWEmbedding-1243"><span class="linenos">1243</span></a>                <span class="n">W_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">to_sparse_full</span><span class="p">(</span><span class="n">W_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding-1244"><a href="#FSWEmbedding-1244"><span class="linenos">1244</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding-1245"><a href="#FSWEmbedding-1245"><span class="linenos">1245</span></a>                <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-1246"><a href="#FSWEmbedding-1246"><span class="linenos">1246</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1247"><a href="#FSWEmbedding-1247"><span class="linenos">1247</span></a>
</span><span id="FSWEmbedding-1248"><a href="#FSWEmbedding-1248"><span class="linenos">1248</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-1249"><a href="#FSWEmbedding-1249"><span class="linenos">1249</span></a>                    <span class="n">X_edge_pad_inds</span> <span class="o">=</span> <span class="n">W_pad</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="FSWEmbedding-1250"><a href="#FSWEmbedding-1250"><span class="linenos">1250</span></a>                    <span class="n">X_edge_pad_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nRecipients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="FSWEmbedding-1251"><a href="#FSWEmbedding-1251"><span class="linenos">1251</span></a>                    <span class="n">X_edge_pad_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1252"><a href="#FSWEmbedding-1252"><span class="linenos">1252</span></a>
</span><span id="FSWEmbedding-1253"><a href="#FSWEmbedding-1253"><span class="linenos">1253</span></a>                    <span class="n">X_edge_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">X_edge_pad_inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">X_edge_pad_vals</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">X_edge_pad_shape</span><span class="p">)</span>
</span><span id="FSWEmbedding-1254"><a href="#FSWEmbedding-1254"><span class="linenos">1254</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">X_edge_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding-1255"><a href="#FSWEmbedding-1255"><span class="linenos">1255</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1256"><a href="#FSWEmbedding-1256"><span class="linenos">1256</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding-1257"><a href="#FSWEmbedding-1257"><span class="linenos">1257</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1258"><a href="#FSWEmbedding-1258"><span class="linenos">1258</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding-1259"><a href="#FSWEmbedding-1259"><span class="linenos">1259</span></a>                    <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span> <span class="p">]</span>
</span><span id="FSWEmbedding-1260"><a href="#FSWEmbedding-1260"><span class="linenos">1260</span></a>                    <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-1261"><a href="#FSWEmbedding-1261"><span class="linenos">1261</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
</span><span id="FSWEmbedding-1262"><a href="#FSWEmbedding-1262"><span class="linenos">1262</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1263"><a href="#FSWEmbedding-1263"><span class="linenos">1263</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1264"><a href="#FSWEmbedding-1264"><span class="linenos">1264</span></a>
</span><span id="FSWEmbedding-1265"><a href="#FSWEmbedding-1265"><span class="linenos">1265</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="FSWEmbedding-1266"><a href="#FSWEmbedding-1266"><span class="linenos">1266</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1267"><a href="#FSWEmbedding-1267"><span class="linenos">1267</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding-1268"><a href="#FSWEmbedding-1268"><span class="linenos">1268</span></a>
</span><span id="FSWEmbedding-1269"><a href="#FSWEmbedding-1269"><span class="linenos">1269</span></a>        <span class="k">del</span> <span class="n">W_pad</span>
</span><span id="FSWEmbedding-1270"><a href="#FSWEmbedding-1270"><span class="linenos">1270</span></a>
</span><span id="FSWEmbedding-1271"><a href="#FSWEmbedding-1271"><span class="linenos">1271</span></a>        <span class="c1"># Normalize W according to W_sum_padded</span>
</span><span id="FSWEmbedding-1272"><a href="#FSWEmbedding-1272"><span class="linenos">1272</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding-1273"><a href="#FSWEmbedding-1273"><span class="linenos">1273</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">div_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_sum_padded</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span>
</span><span id="FSWEmbedding-1274"><a href="#FSWEmbedding-1274"><span class="linenos">1274</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1275"><a href="#FSWEmbedding-1275"><span class="linenos">1275</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1276"><a href="#FSWEmbedding-1276"><span class="linenos">1276</span></a>            <span class="k">del</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding-1277"><a href="#FSWEmbedding-1277"><span class="linenos">1277</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1278"><a href="#FSWEmbedding-1278"><span class="linenos">1278</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">/</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding-1279"><a href="#FSWEmbedding-1279"><span class="linenos">1279</span></a>            <span class="k">del</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding-1280"><a href="#FSWEmbedding-1280"><span class="linenos">1280</span></a>
</span><span id="FSWEmbedding-1281"><a href="#FSWEmbedding-1281"><span class="linenos">1281</span></a>        <span class="c1"># For compatibility reasons, we support the case of zero-dimensional output tensor</span>
</span><span id="FSWEmbedding-1282"><a href="#FSWEmbedding-1282"><span class="linenos">1282</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1283"><a href="#FSWEmbedding-1283"><span class="linenos">1283</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-1284"><a href="#FSWEmbedding-1284"><span class="linenos">1284</span></a>
</span><span id="FSWEmbedding-1285"><a href="#FSWEmbedding-1285"><span class="linenos">1285</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">):</span>
</span><span id="FSWEmbedding-1286"><a href="#FSWEmbedding-1286"><span class="linenos">1286</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="FSWEmbedding-1287"><a href="#FSWEmbedding-1287"><span class="linenos">1287</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1288"><a href="#FSWEmbedding-1288"><span class="linenos">1288</span></a>                                                 <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1289"><a href="#FSWEmbedding-1289"><span class="linenos">1289</span></a>
</span><span id="FSWEmbedding-1290"><a href="#FSWEmbedding-1290"><span class="linenos">1290</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1291"><a href="#FSWEmbedding-1291"><span class="linenos">1291</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;max_parallel_slices must be None or a positive integer&#39;</span>
</span><span id="FSWEmbedding-1292"><a href="#FSWEmbedding-1292"><span class="linenos">1292</span></a>
</span><span id="FSWEmbedding-1293"><a href="#FSWEmbedding-1293"><span class="linenos">1293</span></a>            <span class="n">nIter</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">%</span> <span class="n">max_parallel_slices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding-1294"><a href="#FSWEmbedding-1294"><span class="linenos">1294</span></a>
</span><span id="FSWEmbedding-1295"><a href="#FSWEmbedding-1295"><span class="linenos">1295</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-1296"><a href="#FSWEmbedding-1296"><span class="linenos">1296</span></a>
</span><span id="FSWEmbedding-1297"><a href="#FSWEmbedding-1297"><span class="linenos">1297</span></a>            <span class="k">for</span> <span class="n">iIter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nIter</span><span class="p">):</span>
</span><span id="FSWEmbedding-1298"><a href="#FSWEmbedding-1298"><span class="linenos">1298</span></a>                <span class="n">inds_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">iIter</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="p">(</span><span class="n">iIter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding-1299"><a href="#FSWEmbedding-1299"><span class="linenos">1299</span></a>                <span class="n">slice_vecs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="FSWEmbedding-1300"><a href="#FSWEmbedding-1300"><span class="linenos">1300</span></a>                <span class="n">freqs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">]</span>
</span><span id="FSWEmbedding-1301"><a href="#FSWEmbedding-1301"><span class="linenos">1301</span></a>
</span><span id="FSWEmbedding-1302"><a href="#FSWEmbedding-1302"><span class="linenos">1302</span></a>                <span class="n">out_curr</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">slice_vecs_curr</span><span class="p">,</span> <span class="n">freqs_curr</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="FSWEmbedding-1303"><a href="#FSWEmbedding-1303"><span class="linenos">1303</span></a>                                                        <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1304"><a href="#FSWEmbedding-1304"><span class="linenos">1304</span></a>                                                        <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1305"><a href="#FSWEmbedding-1305"><span class="linenos">1305</span></a>
</span><span id="FSWEmbedding-1306"><a href="#FSWEmbedding-1306"><span class="linenos">1306</span></a>                <span class="n">assign_at</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">out_curr</span><span class="p">,</span> <span class="n">output_slice_axis</span><span class="p">,</span> <span class="n">inds_curr</span><span class="p">)</span>
</span><span id="FSWEmbedding-1307"><a href="#FSWEmbedding-1307"><span class="linenos">1307</span></a>
</span><span id="FSWEmbedding-1308"><a href="#FSWEmbedding-1308"><span class="linenos">1308</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding-1309"><a href="#FSWEmbedding-1309"><span class="linenos">1309</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1310"><a href="#FSWEmbedding-1310"><span class="linenos">1310</span></a>
</span><span id="FSWEmbedding-1311"><a href="#FSWEmbedding-1311"><span class="linenos">1311</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span>
</span><span id="FSWEmbedding-1312"><a href="#FSWEmbedding-1312"><span class="linenos">1312</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="p">:</span>
</span><span id="FSWEmbedding-1313"><a href="#FSWEmbedding-1313"><span class="linenos">1313</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">IDENTITY</span><span class="p">:</span>
</span><span id="FSWEmbedding-1314"><a href="#FSWEmbedding-1314"><span class="linenos">1314</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding-1315"><a href="#FSWEmbedding-1315"><span class="linenos">1315</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">SQRT</span><span class="p">:</span>
</span><span id="FSWEmbedding-1316"><a href="#FSWEmbedding-1316"><span class="linenos">1316</span></a>                    <span class="c1"># x/(sqrt(x+1)+1) is a numerically-safe formulation of sqrt(1+x)-1</span>
</span><span id="FSWEmbedding-1317"><a href="#FSWEmbedding-1317"><span class="linenos">1317</span></a>                    <span class="c1"># note that we don&#39;t use sqrt(1+x) since we need the function to vanish at x=0,</span>
</span><span id="FSWEmbedding-1318"><a href="#FSWEmbedding-1318"><span class="linenos">1318</span></a>                    <span class="c1"># and we don&#39;t use sqrt(x) since we need it to have a gradient at x=0.</span>
</span><span id="FSWEmbedding-1319"><a href="#FSWEmbedding-1319"><span class="linenos">1319</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span> <span class="n">W_sum</span> <span class="o">/</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">W_sum</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding-1320"><a href="#FSWEmbedding-1320"><span class="linenos">1320</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">LOG</span><span class="p">:</span>
</span><span id="FSWEmbedding-1321"><a href="#FSWEmbedding-1321"><span class="linenos">1321</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">W_sum</span><span class="p">)</span>
</span><span id="FSWEmbedding-1322"><a href="#FSWEmbedding-1322"><span class="linenos">1322</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="FSWEmbedding-1323"><a href="#FSWEmbedding-1323"><span class="linenos">1323</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding-1324"><a href="#FSWEmbedding-1324"><span class="linenos">1324</span></a>
</span><span id="FSWEmbedding-1325"><a href="#FSWEmbedding-1325"><span class="linenos">1325</span></a>            <span class="n">encoded_total_mass</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span><span id="FSWEmbedding-1326"><a href="#FSWEmbedding-1326"><span class="linenos">1326</span></a>
</span><span id="FSWEmbedding-1327"><a href="#FSWEmbedding-1327"><span class="linenos">1327</span></a>            <span class="k">del</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding-1328"><a href="#FSWEmbedding-1328"><span class="linenos">1328</span></a>
</span><span id="FSWEmbedding-1329"><a href="#FSWEmbedding-1329"><span class="linenos">1329</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="FSWEmbedding-1330"><a href="#FSWEmbedding-1330"><span class="linenos">1330</span></a>
</span><span id="FSWEmbedding-1331"><a href="#FSWEmbedding-1331"><span class="linenos">1331</span></a>            <span class="n">needs_emb_norm</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="ow">in</span>
</span><span id="FSWEmbedding-1332"><a href="#FSWEmbedding-1332"><span class="linenos">1332</span></a>                              <span class="p">{</span><span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">,</span>
</span><span id="FSWEmbedding-1333"><a href="#FSWEmbedding-1333"><span class="linenos">1333</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">,</span>
</span><span id="FSWEmbedding-1334"><a href="#FSWEmbedding-1334"><span class="linenos">1334</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">})</span>
</span><span id="FSWEmbedding-1335"><a href="#FSWEmbedding-1335"><span class="linenos">1335</span></a>
</span><span id="FSWEmbedding-1336"><a href="#FSWEmbedding-1336"><span class="linenos">1336</span></a>            <span class="k">if</span> <span class="n">needs_emb_norm</span><span class="p">:</span>
</span><span id="FSWEmbedding-1337"><a href="#FSWEmbedding-1337"><span class="linenos">1337</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span><span id="FSWEmbedding-1338"><a href="#FSWEmbedding-1338"><span class="linenos">1338</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1339"><a href="#FSWEmbedding-1339"><span class="linenos">1339</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-1340"><a href="#FSWEmbedding-1340"><span class="linenos">1340</span></a>
</span><span id="FSWEmbedding-1341"><a href="#FSWEmbedding-1341"><span class="linenos">1341</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="p">:</span>
</span><span id="FSWEmbedding-1342"><a href="#FSWEmbedding-1342"><span class="linenos">1342</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">DECOUPLED</span><span class="p">:</span>
</span><span id="FSWEmbedding-1343"><a href="#FSWEmbedding-1343"><span class="linenos">1343</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1344"><a href="#FSWEmbedding-1344"><span class="linenos">1344</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">SCALED</span><span class="p">:</span>
</span><span id="FSWEmbedding-1345"><a href="#FSWEmbedding-1345"><span class="linenos">1345</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1346"><a href="#FSWEmbedding-1346"><span class="linenos">1346</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">:</span>
</span><span id="FSWEmbedding-1347"><a href="#FSWEmbedding-1347"><span class="linenos">1347</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1348"><a href="#FSWEmbedding-1348"><span class="linenos">1348</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">:</span>
</span><span id="FSWEmbedding-1349"><a href="#FSWEmbedding-1349"><span class="linenos">1349</span></a>                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="FSWEmbedding-1350"><a href="#FSWEmbedding-1350"><span class="linenos">1350</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1351"><a href="#FSWEmbedding-1351"><span class="linenos">1351</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">:</span>
</span><span id="FSWEmbedding-1352"><a href="#FSWEmbedding-1352"><span class="linenos">1352</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part1</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span>
</span><span id="FSWEmbedding-1353"><a href="#FSWEmbedding-1353"><span class="linenos">1353</span></a>                                       <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part2</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1354"><a href="#FSWEmbedding-1354"><span class="linenos">1354</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>  <span class="c1"># fallback</span>
</span><span id="FSWEmbedding-1355"><a href="#FSWEmbedding-1355"><span class="linenos">1355</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding method: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding-1356"><a href="#FSWEmbedding-1356"><span class="linenos">1356</span></a>
</span><span id="FSWEmbedding-1357"><a href="#FSWEmbedding-1357"><span class="linenos">1357</span></a>            <span class="k">del</span> <span class="n">X_emb_norm</span>
</span><span id="FSWEmbedding-1358"><a href="#FSWEmbedding-1358"><span class="linenos">1358</span></a>
</span><span id="FSWEmbedding-1359"><a href="#FSWEmbedding-1359"><span class="linenos">1359</span></a>        <span class="c1"># Add bias</span>
</span><span id="FSWEmbedding-1360"><a href="#FSWEmbedding-1360"><span class="linenos">1360</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding-1361"><a href="#FSWEmbedding-1361"><span class="linenos">1361</span></a>            <span class="n">X_emb</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span><span id="FSWEmbedding-1362"><a href="#FSWEmbedding-1362"><span class="linenos">1362</span></a>
</span><span id="FSWEmbedding-1363"><a href="#FSWEmbedding-1363"><span class="linenos">1363</span></a>        <span class="k">return</span> <span class="n">X_emb</span>
</span><span id="FSWEmbedding-1364"><a href="#FSWEmbedding-1364"><span class="linenos">1364</span></a>
</span><span id="FSWEmbedding-1365"><a href="#FSWEmbedding-1365"><span class="linenos">1365</span></a>
</span><span id="FSWEmbedding-1366"><a href="#FSWEmbedding-1366"><span class="linenos">1366</span></a>    <span class="nd">@staticmethod</span>
</span><span id="FSWEmbedding-1367"><a href="#FSWEmbedding-1367"><span class="linenos">1367</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="n">cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="FSWEmbedding-1368"><a href="#FSWEmbedding-1368"><span class="linenos">1368</span></a>                        <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1369"><a href="#FSWEmbedding-1369"><span class="linenos">1369</span></a>                        <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">):</span>
</span><span id="FSWEmbedding-1370"><a href="#FSWEmbedding-1370"><span class="linenos">1370</span></a>        <span class="c1"># This function computes the embedding of (X,W) for a subset of the slices and frequencies.</span>
</span><span id="FSWEmbedding-1371"><a href="#FSWEmbedding-1371"><span class="linenos">1371</span></a>        <span class="c1"># slice_vectors should be of size (num_slices x d_in), and frequencies should be of size num_frequencies (not num_frequencies x 1).</span>
</span><span id="FSWEmbedding-1372"><a href="#FSWEmbedding-1372"><span class="linenos">1372</span></a>
</span><span id="FSWEmbedding-1373"><a href="#FSWEmbedding-1373"><span class="linenos">1373</span></a>        <span class="n">d_in</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="FSWEmbedding-1374"><a href="#FSWEmbedding-1374"><span class="linenos">1374</span></a>        <span class="c1">#n = W.shape[-1]</span>
</span><span id="FSWEmbedding-1375"><a href="#FSWEmbedding-1375"><span class="linenos">1375</span></a>        <span class="n">nRecepients</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="FSWEmbedding-1376"><a href="#FSWEmbedding-1376"><span class="linenos">1376</span></a>        <span class="n">num_slices</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="FSWEmbedding-1377"><a href="#FSWEmbedding-1377"><span class="linenos">1377</span></a>        <span class="n">num_frequencies</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding-1378"><a href="#FSWEmbedding-1378"><span class="linenos">1378</span></a>        <span class="n">sparse_mode</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span>
</span><span id="FSWEmbedding-1379"><a href="#FSWEmbedding-1379"><span class="linenos">1379</span></a>        <span class="n">d_edge</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-1380"><a href="#FSWEmbedding-1380"><span class="linenos">1380</span></a>
</span><span id="FSWEmbedding-1381"><a href="#FSWEmbedding-1381"><span class="linenos">1381</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;This should not happen&quot;</span>
</span><span id="FSWEmbedding-1382"><a href="#FSWEmbedding-1382"><span class="linenos">1382</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">d_in</span> <span class="o">+</span> <span class="n">d_edge</span><span class="p">),</span> <span class="s2">&quot;This should not happen&quot;</span>
</span><span id="FSWEmbedding-1383"><a href="#FSWEmbedding-1383"><span class="linenos">1383</span></a>
</span><span id="FSWEmbedding-1384"><a href="#FSWEmbedding-1384"><span class="linenos">1384</span></a>        <span class="c1"># Calculate the projections of X</span>
</span><span id="FSWEmbedding-1385"><a href="#FSWEmbedding-1385"><span class="linenos">1385</span></a>        <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1386"><a href="#FSWEmbedding-1386"><span class="linenos">1386</span></a>            <span class="n">Xp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="FSWEmbedding-1387"><a href="#FSWEmbedding-1387"><span class="linenos">1387</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1388"><a href="#FSWEmbedding-1388"><span class="linenos">1388</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="FSWEmbedding-1389"><a href="#FSWEmbedding-1389"><span class="linenos">1389</span></a>            <span class="n">Xp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">d_in</span><span class="p">],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="FSWEmbedding-1390"><a href="#FSWEmbedding-1390"><span class="linenos">1390</span></a>
</span><span id="FSWEmbedding-1391"><a href="#FSWEmbedding-1391"><span class="linenos">1391</span></a>        <span class="k">del</span> <span class="n">X</span>
</span><span id="FSWEmbedding-1392"><a href="#FSWEmbedding-1392"><span class="linenos">1392</span></a>
</span><span id="FSWEmbedding-1393"><a href="#FSWEmbedding-1393"><span class="linenos">1393</span></a>        <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1394"><a href="#FSWEmbedding-1394"><span class="linenos">1394</span></a>            <span class="c1"># Sort the projected elements </span>
</span><span id="FSWEmbedding-1395"><a href="#FSWEmbedding-1395"><span class="linenos">1395</span></a>            <span class="c1"># Note: We sort before the graph-mode expansion because it makes things simpler in the case when W is sparse</span>
</span><span id="FSWEmbedding-1396"><a href="#FSWEmbedding-1396"><span class="linenos">1396</span></a>
</span><span id="FSWEmbedding-1397"><a href="#FSWEmbedding-1397"><span class="linenos">1397</span></a>            <span class="c1"># Sort along element/sender axis</span>
</span><span id="FSWEmbedding-1398"><a href="#FSWEmbedding-1398"><span class="linenos">1398</span></a>            <span class="k">if</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1399"><a href="#FSWEmbedding-1399"><span class="linenos">1399</span></a>                <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xp</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="FSWEmbedding-1400"><a href="#FSWEmbedding-1400"><span class="linenos">1400</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1401"><a href="#FSWEmbedding-1401"><span class="linenos">1401</span></a>                <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">Xp</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="FSWEmbedding-1402"><a href="#FSWEmbedding-1402"><span class="linenos">1402</span></a>
</span><span id="FSWEmbedding-1403"><a href="#FSWEmbedding-1403"><span class="linenos">1403</span></a>            <span class="k">del</span> <span class="n">Xp</span>
</span><span id="FSWEmbedding-1404"><a href="#FSWEmbedding-1404"><span class="linenos">1404</span></a>
</span><span id="FSWEmbedding-1405"><a href="#FSWEmbedding-1405"><span class="linenos">1405</span></a>            <span class="k">if</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1406"><a href="#FSWEmbedding-1406"><span class="linenos">1406</span></a>                <span class="c1"># Create recepient axis before sender axis and slice axis</span>
</span><span id="FSWEmbedding-1407"><a href="#FSWEmbedding-1407"><span class="linenos">1407</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
</span><span id="FSWEmbedding-1408"><a href="#FSWEmbedding-1408"><span class="linenos">1408</span></a>                <span class="n">Xpi</span> <span class="o">=</span> <span class="n">Xpi</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
</span><span id="FSWEmbedding-1409"><a href="#FSWEmbedding-1409"><span class="linenos">1409</span></a>
</span><span id="FSWEmbedding-1410"><a href="#FSWEmbedding-1410"><span class="linenos">1410</span></a>        <span class="k">elif</span> <span class="n">sparse_mode</span><span class="p">:</span> <span class="c1"># d_edge &gt; 0, sparse_mode=True</span>
</span><span id="FSWEmbedding-1411"><a href="#FSWEmbedding-1411"><span class="linenos">1411</span></a>            <span class="n">Xe</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="FSWEmbedding-1412"><a href="#FSWEmbedding-1412"><span class="linenos">1412</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="FSWEmbedding-1413"><a href="#FSWEmbedding-1413"><span class="linenos">1413</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">Xe</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="n">d_in</span><span class="p">:],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="FSWEmbedding-1414"><a href="#FSWEmbedding-1414"><span class="linenos">1414</span></a>            <span class="k">del</span> <span class="n">Xe</span>
</span><span id="FSWEmbedding-1415"><a href="#FSWEmbedding-1415"><span class="linenos">1415</span></a>
</span><span id="FSWEmbedding-1416"><a href="#FSWEmbedding-1416"><span class="linenos">1416</span></a>            <span class="n">inds</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="FSWEmbedding-1417"><a href="#FSWEmbedding-1417"><span class="linenos">1417</span></a>
</span><span id="FSWEmbedding-1418"><a href="#FSWEmbedding-1418"><span class="linenos">1418</span></a>            <span class="c1"># Remove recepient axis from inds</span>
</span><span id="FSWEmbedding-1419"><a href="#FSWEmbedding-1419"><span class="linenos">1419</span></a>            <span class="n">dims_without_recipient</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)]</span>
</span><span id="FSWEmbedding-1420"><a href="#FSWEmbedding-1420"><span class="linenos">1420</span></a>
</span><span id="FSWEmbedding-1421"><a href="#FSWEmbedding-1421"><span class="linenos">1421</span></a>            <span class="c1"># For each edge, get the corresponding sender vertex feature vector after projection</span>
</span><span id="FSWEmbedding-1422"><a href="#FSWEmbedding-1422"><span class="linenos">1422</span></a>            <span class="n">Xp_temp</span> <span class="o">=</span> <span class="n">Xp</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">inds</span><span class="p">[</span><span class="n">dims_without_recipient</span><span class="p">,:])]</span>
</span><span id="FSWEmbedding-1423"><a href="#FSWEmbedding-1423"><span class="linenos">1423</span></a>
</span><span id="FSWEmbedding-1424"><a href="#FSWEmbedding-1424"><span class="linenos">1424</span></a>            <span class="n">Xep</span> <span class="o">+=</span> <span class="n">Xp_temp</span>
</span><span id="FSWEmbedding-1425"><a href="#FSWEmbedding-1425"><span class="linenos">1425</span></a>            <span class="k">del</span> <span class="n">Xp_temp</span><span class="p">,</span> <span class="n">inds</span>
</span><span id="FSWEmbedding-1426"><a href="#FSWEmbedding-1426"><span class="linenos">1426</span></a>
</span><span id="FSWEmbedding-1427"><a href="#FSWEmbedding-1427"><span class="linenos">1427</span></a>            <span class="n">Xep_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Xep</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span id="FSWEmbedding-1428"><a href="#FSWEmbedding-1428"><span class="linenos">1428</span></a>
</span><span id="FSWEmbedding-1429"><a href="#FSWEmbedding-1429"><span class="linenos">1429</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">(),</span> <span class="n">values</span><span class="o">=</span><span class="n">Xep</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Xep_shape</span><span class="p">)</span>
</span><span id="FSWEmbedding-1430"><a href="#FSWEmbedding-1430"><span class="linenos">1430</span></a>            <span class="n">Xep</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">flatten_dense_dim</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xep</span><span class="p">)</span>
</span><span id="FSWEmbedding-1431"><a href="#FSWEmbedding-1431"><span class="linenos">1431</span></a>            <span class="n">Xeps</span><span class="p">,</span> <span class="n">Xepi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xep</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="FSWEmbedding-1432"><a href="#FSWEmbedding-1432"><span class="linenos">1432</span></a>            <span class="k">del</span> <span class="n">Xep</span>
</span><span id="FSWEmbedding-1433"><a href="#FSWEmbedding-1433"><span class="linenos">1433</span></a>
</span><span id="FSWEmbedding-1434"><a href="#FSWEmbedding-1434"><span class="linenos">1434</span></a>        <span class="k">else</span><span class="p">:</span> <span class="c1"># d_edge &gt; 0, sparse_mode=False</span>
</span><span id="FSWEmbedding-1435"><a href="#FSWEmbedding-1435"><span class="linenos">1435</span></a>            <span class="c1"># Create recepient axis before sender axis and slice axis</span>
</span><span id="FSWEmbedding-1436"><a href="#FSWEmbedding-1436"><span class="linenos">1436</span></a>            <span class="n">Xpx</span> <span class="o">=</span> <span class="n">Xp</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#.expand(tuple(W.shape) + (num_slices,))</span>
</span><span id="FSWEmbedding-1437"><a href="#FSWEmbedding-1437"><span class="linenos">1437</span></a>            <span class="c1"># Replicate Xpx along recepient axis</span>
</span><span id="FSWEmbedding-1438"><a href="#FSWEmbedding-1438"><span class="linenos">1438</span></a>            <span class="n">Xpx</span> <span class="o">=</span> <span class="n">Xpx</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">replace_in_tuple</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span><span class="o">*</span><span class="n">Xpx</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="n">nRecepients</span><span class="p">))</span>
</span><span id="FSWEmbedding-1439"><a href="#FSWEmbedding-1439"><span class="linenos">1439</span></a>            <span class="c1"># Add edge-feature part of inner product to each recepient for each sender and projection</span>
</span><span id="FSWEmbedding-1440"><a href="#FSWEmbedding-1440"><span class="linenos">1440</span></a>            <span class="c1"># noinspection PyUnresolvedReferences</span>
</span><span id="FSWEmbedding-1441"><a href="#FSWEmbedding-1441"><span class="linenos">1441</span></a>            <span class="n">Xpx</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">slice_vectors</span><span class="p">[:,</span><span class="n">d_in</span><span class="p">:],</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,]))</span>
</span><span id="FSWEmbedding-1442"><a href="#FSWEmbedding-1442"><span class="linenos">1442</span></a>            <span class="c1"># Sort along element/sender axis</span>
</span><span id="FSWEmbedding-1443"><a href="#FSWEmbedding-1443"><span class="linenos">1443</span></a>            <span class="n">Xps</span><span class="p">,</span> <span class="n">Xpi</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Xpx</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="FSWEmbedding-1444"><a href="#FSWEmbedding-1444"><span class="linenos">1444</span></a>            <span class="c1">#Xps, Xpi = torch.sort(Xpx, dim=-2, descending=False, stable=True)</span>
</span><span id="FSWEmbedding-1445"><a href="#FSWEmbedding-1445"><span class="linenos">1445</span></a>
</span><span id="FSWEmbedding-1446"><a href="#FSWEmbedding-1446"><span class="linenos">1446</span></a>            <span class="k">del</span> <span class="n">Xpx</span>
</span><span id="FSWEmbedding-1447"><a href="#FSWEmbedding-1447"><span class="linenos">1447</span></a>
</span><span id="FSWEmbedding-1448"><a href="#FSWEmbedding-1448"><span class="linenos">1448</span></a>        <span class="c1"># Axis numbers as in the implementation of forward()</span>
</span><span id="FSWEmbedding-1449"><a href="#FSWEmbedding-1449"><span class="linenos">1449</span></a>        <span class="c1"># Note: These numbers are true only from here</span>
</span><span id="FSWEmbedding-1450"><a href="#FSWEmbedding-1450"><span class="linenos">1450</span></a>        <span class="n">recipient_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># Message-recipient vertices</span>
</span><span id="FSWEmbedding-1451"><a href="#FSWEmbedding-1451"><span class="linenos">1451</span></a>        <span class="n">element_axis</span>  <span class="o">=</span> <span class="n">recipient_axis</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="c1"># In graph mode this axis denotes the message-sender vertices</span>
</span><span id="FSWEmbedding-1452"><a href="#FSWEmbedding-1452"><span class="linenos">1452</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="o">+</span> <span class="mi">1</span>        
</span><span id="FSWEmbedding-1453"><a href="#FSWEmbedding-1453"><span class="linenos">1453</span></a>        <span class="n">slice_axis</span>     <span class="o">=</span> <span class="n">ambspace_axis</span>
</span><span id="FSWEmbedding-1454"><a href="#FSWEmbedding-1454"><span class="linenos">1454</span></a>        <span class="n">freq_axis</span>     <span class="o">=</span> <span class="n">slice_axis</span> <span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">cartesian_mode</span> <span class="k">else</span> <span class="n">slice_axis</span>
</span><span id="FSWEmbedding-1455"><a href="#FSWEmbedding-1455"><span class="linenos">1455</span></a>        <span class="c1"># noinspection PyUnusedLocal</span>
</span><span id="FSWEmbedding-1456"><a href="#FSWEmbedding-1456"><span class="linenos">1456</span></a>        <span class="n">output_slice_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="c1"># In the output, the element axis is replaced by the slice axis</span>
</span><span id="FSWEmbedding-1457"><a href="#FSWEmbedding-1457"><span class="linenos">1457</span></a>
</span><span id="FSWEmbedding-1458"><a href="#FSWEmbedding-1458"><span class="linenos">1458</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequencies</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
</span><span id="FSWEmbedding-1459"><a href="#FSWEmbedding-1459"><span class="linenos">1459</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">freq_axis</span><span class="p">):</span>
</span><span id="FSWEmbedding-1460"><a href="#FSWEmbedding-1460"><span class="linenos">1460</span></a>            <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="FSWEmbedding-1461"><a href="#FSWEmbedding-1461"><span class="linenos">1461</span></a>
</span><span id="FSWEmbedding-1462"><a href="#FSWEmbedding-1462"><span class="linenos">1462</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1463"><a href="#FSWEmbedding-1463"><span class="linenos">1463</span></a>            <span class="k">if</span> <span class="n">graph_mode</span> <span class="ow">and</span> <span class="p">(</span><span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="FSWEmbedding-1464"><a href="#FSWEmbedding-1464"><span class="linenos">1464</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,))</span>
</span><span id="FSWEmbedding-1465"><a href="#FSWEmbedding-1465"><span class="linenos">1465</span></a>                <span class="n">Xpi</span> <span class="o">=</span> <span class="n">Xpi</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,))</span>
</span><span id="FSWEmbedding-1466"><a href="#FSWEmbedding-1466"><span class="linenos">1466</span></a>
</span><span id="FSWEmbedding-1467"><a href="#FSWEmbedding-1467"><span class="linenos">1467</span></a>            <span class="c1"># Sort the weights according to their corresponding projected elements</span>
</span><span id="FSWEmbedding-1468"><a href="#FSWEmbedding-1468"><span class="linenos">1468</span></a>            <span class="n">W_big</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">Xps</span><span class="p">)</span>
</span><span id="FSWEmbedding-1469"><a href="#FSWEmbedding-1469"><span class="linenos">1469</span></a>            <span class="n">Wps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">Xpi</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span><span id="FSWEmbedding-1470"><a href="#FSWEmbedding-1470"><span class="linenos">1470</span></a>
</span><span id="FSWEmbedding-1471"><a href="#FSWEmbedding-1471"><span class="linenos">1471</span></a>            <span class="k">if</span> <span class="n">cartesian_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1472"><a href="#FSWEmbedding-1472"><span class="linenos">1472</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">Wps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1473"><a href="#FSWEmbedding-1473"><span class="linenos">1473</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">Xps</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,))</span>
</span><span id="FSWEmbedding-1474"><a href="#FSWEmbedding-1474"><span class="linenos">1474</span></a>
</span><span id="FSWEmbedding-1475"><a href="#FSWEmbedding-1475"><span class="linenos">1475</span></a>            <span class="c1"># Once we have Wps we don&#39;t need W_big and Xpi</span>
</span><span id="FSWEmbedding-1476"><a href="#FSWEmbedding-1476"><span class="linenos">1476</span></a>            <span class="k">del</span> <span class="n">W_big</span><span class="p">,</span> <span class="n">Xpi</span>
</span><span id="FSWEmbedding-1477"><a href="#FSWEmbedding-1477"><span class="linenos">1477</span></a>
</span><span id="FSWEmbedding-1478"><a href="#FSWEmbedding-1478"><span class="linenos">1478</span></a>            <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1479"><a href="#FSWEmbedding-1479"><span class="linenos">1479</span></a>
</span><span id="FSWEmbedding-1480"><a href="#FSWEmbedding-1480"><span class="linenos">1480</span></a>            <span class="c1"># Here we assume sinc(x) = sin(pi*x)/(pi*x)</span>
</span><span id="FSWEmbedding-1481"><a href="#FSWEmbedding-1481"><span class="linenos">1481</span></a>            <span class="n">sincs</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">Wps_sum</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinc</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">frequencies</span> <span class="o">*</span> <span class="n">Wps_sum</span><span class="p">)</span>
</span><span id="FSWEmbedding-1482"><a href="#FSWEmbedding-1482"><span class="linenos">1482</span></a>            <span class="n">sinc_diffs</span> <span class="o">=</span> <span class="n">diff_zeropad</span><span class="p">(</span><span class="n">sincs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1483"><a href="#FSWEmbedding-1483"><span class="linenos">1483</span></a>            <span class="k">del</span> <span class="n">sincs</span>
</span><span id="FSWEmbedding-1484"><a href="#FSWEmbedding-1484"><span class="linenos">1484</span></a>
</span><span id="FSWEmbedding-1485"><a href="#FSWEmbedding-1485"><span class="linenos">1485</span></a>        <span class="k">elif</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1486"><a href="#FSWEmbedding-1486"><span class="linenos">1486</span></a>            <span class="c1"># We unsqueeze W to add a slice axis, in order to sort W according to each projection of X</span>
</span><span id="FSWEmbedding-1487"><a href="#FSWEmbedding-1487"><span class="linenos">1487</span></a>            <span class="c1"># Note: This repmat is unavoidable, because we sort the weights according to different permutations along slice_axis</span>
</span><span id="FSWEmbedding-1488"><a href="#FSWEmbedding-1488"><span class="linenos">1488</span></a>            <span class="n">W_unsqueeze</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1489"><a href="#FSWEmbedding-1489"><span class="linenos">1489</span></a>            <span class="k">del</span> <span class="n">W</span>
</span><span id="FSWEmbedding-1490"><a href="#FSWEmbedding-1490"><span class="linenos">1490</span></a>
</span><span id="FSWEmbedding-1491"><a href="#FSWEmbedding-1491"><span class="linenos">1491</span></a>            <span class="c1"># 1.71 seconds</span>
</span><span id="FSWEmbedding-1492"><a href="#FSWEmbedding-1492"><span class="linenos">1492</span></a>            <span class="n">W_big</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_unsqueeze</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">,</span> <span class="n">slice_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1493"><a href="#FSWEmbedding-1493"><span class="linenos">1493</span></a>            <span class="k">del</span> <span class="n">W_unsqueeze</span>
</span><span id="FSWEmbedding-1494"><a href="#FSWEmbedding-1494"><span class="linenos">1494</span></a>
</span><span id="FSWEmbedding-1495"><a href="#FSWEmbedding-1495"><span class="linenos">1495</span></a>            <span class="k">if</span> <span class="n">d_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1496"><a href="#FSWEmbedding-1496"><span class="linenos">1496</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse_vals</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">Xepi</span><span class="p">)</span>
</span><span id="FSWEmbedding-1497"><a href="#FSWEmbedding-1497"><span class="linenos">1497</span></a>                <span class="k">del</span> <span class="n">Xepi</span>
</span><span id="FSWEmbedding-1498"><a href="#FSWEmbedding-1498"><span class="linenos">1498</span></a>            <span class="k">elif</span> <span class="n">graph_mode</span><span class="p">:</span> 
</span><span id="FSWEmbedding-1499"><a href="#FSWEmbedding-1499"><span class="linenos">1499</span></a>                <span class="c1"># 1.82 seconds</span>
</span><span id="FSWEmbedding-1500"><a href="#FSWEmbedding-1500"><span class="linenos">1500</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">Xpi</span><span class="p">,</span> <span class="n">recipient_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1501"><a href="#FSWEmbedding-1501"><span class="linenos">1501</span></a>                <span class="k">del</span> <span class="n">Xpi</span>
</span><span id="FSWEmbedding-1502"><a href="#FSWEmbedding-1502"><span class="linenos">1502</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1503"><a href="#FSWEmbedding-1503"><span class="linenos">1503</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">permute_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_big</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">Xpi</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span><span id="FSWEmbedding-1504"><a href="#FSWEmbedding-1504"><span class="linenos">1504</span></a>                <span class="k">del</span> <span class="n">Xpi</span>
</span><span id="FSWEmbedding-1505"><a href="#FSWEmbedding-1505"><span class="linenos">1505</span></a>
</span><span id="FSWEmbedding-1506"><a href="#FSWEmbedding-1506"><span class="linenos">1506</span></a>            <span class="c1"># Once we have Wps we don&#39;t need W_big and Xpi</span>
</span><span id="FSWEmbedding-1507"><a href="#FSWEmbedding-1507"><span class="linenos">1507</span></a>            <span class="k">del</span> <span class="n">W_big</span>
</span><span id="FSWEmbedding-1508"><a href="#FSWEmbedding-1508"><span class="linenos">1508</span></a>
</span><span id="FSWEmbedding-1509"><a href="#FSWEmbedding-1509"><span class="linenos">1509</span></a>            <span class="c1"># 2.6 seconds</span>
</span><span id="FSWEmbedding-1510"><a href="#FSWEmbedding-1510"><span class="linenos">1510</span></a>            <span class="n">slice_info_elements</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-1511"><a href="#FSWEmbedding-1511"><span class="linenos">1511</span></a>                                                    <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1512"><a href="#FSWEmbedding-1512"><span class="linenos">1512</span></a>            <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">cumsum_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">slice_info_elements</span><span class="p">,</span>
</span><span id="FSWEmbedding-1513"><a href="#FSWEmbedding-1513"><span class="linenos">1513</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1514"><a href="#FSWEmbedding-1514"><span class="linenos">1514</span></a>                                             <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1515"><a href="#FSWEmbedding-1515"><span class="linenos">1515</span></a>
</span><span id="FSWEmbedding-1516"><a href="#FSWEmbedding-1516"><span class="linenos">1516</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">Wps</span><span class="p">)</span>
</span><span id="FSWEmbedding-1517"><a href="#FSWEmbedding-1517"><span class="linenos">1517</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">Wps_sum</span><span class="p">)</span>
</span><span id="FSWEmbedding-1518"><a href="#FSWEmbedding-1518"><span class="linenos">1518</span></a>
</span><span id="FSWEmbedding-1519"><a href="#FSWEmbedding-1519"><span class="linenos">1519</span></a>            <span class="k">if</span> <span class="n">cartesian_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1520"><a href="#FSWEmbedding-1520"><span class="linenos">1520</span></a>                <span class="c1"># Note:</span>
</span><span id="FSWEmbedding-1521"><a href="#FSWEmbedding-1521"><span class="linenos">1521</span></a>                <span class="c1"># These repmats may be avoided if ag.sinc_cos_sparse could take frequencies as a separate input, and broadcast all inputs accordingly.</span>
</span><span id="FSWEmbedding-1522"><a href="#FSWEmbedding-1522"><span class="linenos">1522</span></a>                <span class="c1"># But sinc_diffs is of the same size as Wps and Wps_sum, so we could reduce the memory usage at most by 2/3, and only in cartesian mode.</span>
</span><span id="FSWEmbedding-1523"><a href="#FSWEmbedding-1523"><span class="linenos">1523</span></a>                <span class="c1"># This may not worth the effort.</span>
</span><span id="FSWEmbedding-1524"><a href="#FSWEmbedding-1524"><span class="linenos">1524</span></a>
</span><span id="FSWEmbedding-1525"><a href="#FSWEmbedding-1525"><span class="linenos">1525</span></a>                <span class="n">Wps</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">freq_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1526"><a href="#FSWEmbedding-1526"><span class="linenos">1526</span></a>                <span class="n">Wps_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">repmat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps_sum</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">freq_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1527"><a href="#FSWEmbedding-1527"><span class="linenos">1527</span></a>                <span class="n">Xps</span> <span class="o">=</span> <span class="n">Xps</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">Xps</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_frequencies</span><span class="p">,))</span>
</span><span id="FSWEmbedding-1528"><a href="#FSWEmbedding-1528"><span class="linenos">1528</span></a>               
</span><span id="FSWEmbedding-1529"><a href="#FSWEmbedding-1529"><span class="linenos">1529</span></a>            <span class="c1"># Here we use the sum-to-product identity sin(2a)-sin(2b) = 2*sin(a-b)*cos(a+b)            </span>
</span><span id="FSWEmbedding-1530"><a href="#FSWEmbedding-1530"><span class="linenos">1530</span></a>            <span class="c1"># This formula probably leads to a loss of one significant digit, but it is much easier in the sparse case than using diff().</span>
</span><span id="FSWEmbedding-1531"><a href="#FSWEmbedding-1531"><span class="linenos">1531</span></a>            
</span><span id="FSWEmbedding-1532"><a href="#FSWEmbedding-1532"><span class="linenos">1532</span></a>            <span class="c1"># Variant 2 is more memory efficient</span>
</span><span id="FSWEmbedding-1533"><a href="#FSWEmbedding-1533"><span class="linenos">1533</span></a>            <span class="n">variant</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="FSWEmbedding-1534"><a href="#FSWEmbedding-1534"><span class="linenos">1534</span></a>
</span><span id="FSWEmbedding-1535"><a href="#FSWEmbedding-1535"><span class="linenos">1535</span></a>            <span class="k">if</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding-1536"><a href="#FSWEmbedding-1536"><span class="linenos">1536</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">frequencies</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">Wps_sum</span> <span class="o">-</span> <span class="n">Wps</span><span class="p">)</span>
</span><span id="FSWEmbedding-1537"><a href="#FSWEmbedding-1537"><span class="linenos">1537</span></a>                <span class="k">del</span> <span class="n">Wps_sum</span>
</span><span id="FSWEmbedding-1538"><a href="#FSWEmbedding-1538"><span class="linenos">1538</span></a>                <span class="n">assert_coalesced</span><span class="p">(</span><span class="n">arg2</span><span class="p">)</span>
</span><span id="FSWEmbedding-1539"><a href="#FSWEmbedding-1539"><span class="linenos">1539</span></a>
</span><span id="FSWEmbedding-1540"><a href="#FSWEmbedding-1540"><span class="linenos">1540</span></a>            <span class="k">elif</span> <span class="n">variant</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>                               
</span><span id="FSWEmbedding-1541"><a href="#FSWEmbedding-1541"><span class="linenos">1541</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">add_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">Wps_sum</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="FSWEmbedding-1542"><a href="#FSWEmbedding-1542"><span class="linenos">1542</span></a>                <span class="k">del</span> <span class="n">Wps_sum</span>
</span><span id="FSWEmbedding-1543"><a href="#FSWEmbedding-1543"><span class="linenos">1543</span></a>                <span class="c1"># 1.22 seconds</span>
</span><span id="FSWEmbedding-1544"><a href="#FSWEmbedding-1544"><span class="linenos">1544</span></a>                <span class="n">slice_info_freqs</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">),</span>
</span><span id="FSWEmbedding-1545"><a href="#FSWEmbedding-1545"><span class="linenos">1545</span></a>                                                     <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-1546"><a href="#FSWEmbedding-1546"><span class="linenos">1546</span></a>                                                     <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1547"><a href="#FSWEmbedding-1547"><span class="linenos">1547</span></a>                                                     <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1548"><a href="#FSWEmbedding-1548"><span class="linenos">1548</span></a>                <span class="c1"># 0.15 seconds</span>
</span><span id="FSWEmbedding-1549"><a href="#FSWEmbedding-1549"><span class="linenos">1549</span></a>                <span class="n">arg2</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">arg2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">slice_info_freqs</span><span class="p">,</span>
</span><span id="FSWEmbedding-1550"><a href="#FSWEmbedding-1550"><span class="linenos">1550</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1551"><a href="#FSWEmbedding-1551"><span class="linenos">1551</span></a>                                                 <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1552"><a href="#FSWEmbedding-1552"><span class="linenos">1552</span></a>
</span><span id="FSWEmbedding-1553"><a href="#FSWEmbedding-1553"><span class="linenos">1553</span></a>            <span class="c1"># 0.14 seconds</span>
</span><span id="FSWEmbedding-1554"><a href="#FSWEmbedding-1554"><span class="linenos">1554</span></a>            <span class="n">arg1</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">Wps</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">slice_info_freqs</span><span class="p">,</span>
</span><span id="FSWEmbedding-1555"><a href="#FSWEmbedding-1555"><span class="linenos">1555</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1556"><a href="#FSWEmbedding-1556"><span class="linenos">1556</span></a>                                             <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1557"><a href="#FSWEmbedding-1557"><span class="linenos">1557</span></a>
</span><span id="FSWEmbedding-1558"><a href="#FSWEmbedding-1558"><span class="linenos">1558</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">arg1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1559"><a href="#FSWEmbedding-1559"><span class="linenos">1559</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">arg2</span><span class="p">)</span>
</span><span id="FSWEmbedding-1560"><a href="#FSWEmbedding-1560"><span class="linenos">1560</span></a>
</span><span id="FSWEmbedding-1561"><a href="#FSWEmbedding-1561"><span class="linenos">1561</span></a>            <span class="c1"># 0.53 seconds</span>
</span><span id="FSWEmbedding-1562"><a href="#FSWEmbedding-1562"><span class="linenos">1562</span></a>            <span class="n">sinc_cos</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sinc_cos_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</span><span id="FSWEmbedding-1563"><a href="#FSWEmbedding-1563"><span class="linenos">1563</span></a>            <span class="k">del</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span>
</span><span id="FSWEmbedding-1564"><a href="#FSWEmbedding-1564"><span class="linenos">1564</span></a>            <span class="n">sinc_diffs</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span> <span class="n">Wps</span><span class="p">,</span> <span class="n">sinc_cos</span><span class="p">,</span> <span class="mi">2</span> <span class="p">)</span> 
</span><span id="FSWEmbedding-1565"><a href="#FSWEmbedding-1565"><span class="linenos">1565</span></a>
</span><span id="FSWEmbedding-1566"><a href="#FSWEmbedding-1566"><span class="linenos">1566</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">sinc_cos</span><span class="p">)</span>
</span><span id="FSWEmbedding-1567"><a href="#FSWEmbedding-1567"><span class="linenos">1567</span></a>            <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">)</span>
</span><span id="FSWEmbedding-1568"><a href="#FSWEmbedding-1568"><span class="linenos">1568</span></a>
</span><span id="FSWEmbedding-1569"><a href="#FSWEmbedding-1569"><span class="linenos">1569</span></a>            <span class="k">del</span> <span class="n">Wps</span><span class="p">,</span> <span class="n">sinc_cos</span>
</span><span id="FSWEmbedding-1570"><a href="#FSWEmbedding-1570"><span class="linenos">1570</span></a>           
</span><span id="FSWEmbedding-1571"><a href="#FSWEmbedding-1571"><span class="linenos">1571</span></a>        <span class="c1"># From here we only need sinc_diffs and Xps               </span>
</span><span id="FSWEmbedding-1572"><a href="#FSWEmbedding-1572"><span class="linenos">1572</span></a>
</span><span id="FSWEmbedding-1573"><a href="#FSWEmbedding-1573"><span class="linenos">1573</span></a>        <span class="k">if</span> <span class="n">sparse_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding-1574"><a href="#FSWEmbedding-1574"><span class="linenos">1574</span></a>            <span class="k">if</span> <span class="n">d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding-1575"><a href="#FSWEmbedding-1575"><span class="linenos">1575</span></a>                <span class="c1"># 1.4 seconds</span>
</span><span id="FSWEmbedding-1576"><a href="#FSWEmbedding-1576"><span class="linenos">1576</span></a>                <span class="n">slice_info_Xps</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_broadcast_dims_B_to_A</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">),</span>
</span><span id="FSWEmbedding-1577"><a href="#FSWEmbedding-1577"><span class="linenos">1577</span></a>                                                   <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding-1578"><a href="#FSWEmbedding-1578"><span class="linenos">1578</span></a>                                                   <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1579"><a href="#FSWEmbedding-1579"><span class="linenos">1579</span></a>                                                   <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1580"><a href="#FSWEmbedding-1580"><span class="linenos">1580</span></a>                <span class="c1"># 0.26 seconds</span>
</span><span id="FSWEmbedding-1581"><a href="#FSWEmbedding-1581"><span class="linenos">1581</span></a>                <span class="n">products</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">slice_info_Xps</span><span class="p">,</span>
</span><span id="FSWEmbedding-1582"><a href="#FSWEmbedding-1582"><span class="linenos">1582</span></a>                                                     <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1583"><a href="#FSWEmbedding-1583"><span class="linenos">1583</span></a>                                                     <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1584"><a href="#FSWEmbedding-1584"><span class="linenos">1584</span></a>                <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">slice_info_Xps</span>
</span><span id="FSWEmbedding-1585"><a href="#FSWEmbedding-1585"><span class="linenos">1585</span></a>                <span class="n">sp</span><span class="o">.</span><span class="n">verify_coalescence</span><span class="p">(</span><span class="n">products</span><span class="p">)</span>
</span><span id="FSWEmbedding-1586"><a href="#FSWEmbedding-1586"><span class="linenos">1586</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding-1587"><a href="#FSWEmbedding-1587"><span class="linenos">1587</span></a>                <span class="n">products</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">mul_same_pattern</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xeps</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1588"><a href="#FSWEmbedding-1588"><span class="linenos">1588</span></a>                <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xeps</span>
</span><span id="FSWEmbedding-1589"><a href="#FSWEmbedding-1589"><span class="linenos">1589</span></a>
</span><span id="FSWEmbedding-1590"><a href="#FSWEmbedding-1590"><span class="linenos">1590</span></a>            <span class="c1"># 0.49 seconds</span>
</span><span id="FSWEmbedding-1591"><a href="#FSWEmbedding-1591"><span class="linenos">1591</span></a>            <span class="n">product_sums</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sum_sparseToDense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">products</span><span class="p">,</span> <span class="n">element_axis</span><span class="p">,</span> <span class="n">slice_info_elements</span><span class="p">,</span>
</span><span id="FSWEmbedding-1592"><a href="#FSWEmbedding-1592"><span class="linenos">1592</span></a>                                                      <span class="n">use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding-1593"><a href="#FSWEmbedding-1593"><span class="linenos">1593</span></a>                                                      <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding-1594"><a href="#FSWEmbedding-1594"><span class="linenos">1594</span></a>            <span class="k">del</span> <span class="n">products</span><span class="p">,</span> <span class="n">slice_info_elements</span>
</span><span id="FSWEmbedding-1595"><a href="#FSWEmbedding-1595"><span class="linenos">1595</span></a>            
</span><span id="FSWEmbedding-1596"><a href="#FSWEmbedding-1596"><span class="linenos">1596</span></a>        <span class="k">else</span><span class="p">:</span> <span class="c1"># not sparse</span>
</span><span id="FSWEmbedding-1597"><a href="#FSWEmbedding-1597"><span class="linenos">1597</span></a>            <span class="n">product_sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sinc_diffs</span> <span class="o">*</span> <span class="n">Xps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FSWEmbedding-1598"><a href="#FSWEmbedding-1598"><span class="linenos">1598</span></a>            <span class="k">del</span> <span class="n">sinc_diffs</span><span class="p">,</span> <span class="n">Xps</span>
</span><span id="FSWEmbedding-1599"><a href="#FSWEmbedding-1599"><span class="linenos">1599</span></a>
</span><span id="FSWEmbedding-1600"><a href="#FSWEmbedding-1600"><span class="linenos">1600</span></a>        <span class="c1"># We squeeze the element axis after having summed up along it</span>
</span><span id="FSWEmbedding-1601"><a href="#FSWEmbedding-1601"><span class="linenos">1601</span></a>        <span class="n">product_sums</span> <span class="o">=</span> <span class="n">product_sums</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1602"><a href="#FSWEmbedding-1602"><span class="linenos">1602</span></a>        <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">)</span>
</span><span id="FSWEmbedding-1603"><a href="#FSWEmbedding-1603"><span class="linenos">1603</span></a>
</span><span id="FSWEmbedding-1604"><a href="#FSWEmbedding-1604"><span class="linenos">1604</span></a>        <span class="c1"># frequencies and product_sums are always dense</span>
</span><span id="FSWEmbedding-1605"><a href="#FSWEmbedding-1605"><span class="linenos">1605</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">frequencies</span><span class="p">)</span> <span class="o">*</span> <span class="n">product_sums</span>            
</span><span id="FSWEmbedding-1606"><a href="#FSWEmbedding-1606"><span class="linenos">1606</span></a>        <span class="k">del</span> <span class="n">product_sums</span>
</span><span id="FSWEmbedding-1607"><a href="#FSWEmbedding-1607"><span class="linenos">1607</span></a>
</span><span id="FSWEmbedding-1608"><a href="#FSWEmbedding-1608"><span class="linenos">1608</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="FSWEmbedding-1609"><a href="#FSWEmbedding-1609"><span class="linenos">1609</span></a>
</span><span id="FSWEmbedding-1610"><a href="#FSWEmbedding-1610"><span class="linenos">1610</span></a>
</span><span id="FSWEmbedding-1611"><a href="#FSWEmbedding-1611"><span class="linenos">1611</span></a>
</span><span id="FSWEmbedding-1612"><a href="#FSWEmbedding-1612"><span class="linenos">1612</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_get_mutual_coherence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FSWEmbedding-1613"><a href="#FSWEmbedding-1613"><span class="linenos">1613</span></a>        <span class="n">gram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1614"><a href="#FSWEmbedding-1614"><span class="linenos">1614</span></a>        <span class="n">inds</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span><span class="p">)</span>
</span><span id="FSWEmbedding-1615"><a href="#FSWEmbedding-1615"><span class="linenos">1615</span></a>        <span class="n">gram</span><span class="p">[</span><span class="n">inds</span><span class="p">,</span><span class="n">inds</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="FSWEmbedding-1616"><a href="#FSWEmbedding-1616"><span class="linenos">1616</span></a>
</span><span id="FSWEmbedding-1617"><a href="#FSWEmbedding-1617"><span class="linenos">1617</span></a>        <span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">gram</span><span class="p">))</span>
</span><span id="FSWEmbedding-1618"><a href="#FSWEmbedding-1618"><span class="linenos">1618</span></a>        <span class="k">return</span> <span class="n">mu</span>
</span><span id="FSWEmbedding-1619"><a href="#FSWEmbedding-1619"><span class="linenos">1619</span></a>
</span><span id="FSWEmbedding-1620"><a href="#FSWEmbedding-1620"><span class="linenos">1620</span></a>
</span><span id="FSWEmbedding-1621"><a href="#FSWEmbedding-1621"><span class="linenos">1621</span></a>    <span class="nd">@staticmethod</span>
</span><span id="FSWEmbedding-1622"><a href="#FSWEmbedding-1622"><span class="linenos">1622</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_total_mass_homogeneous_legacy_encoding_part1</span><span class="p">(</span><span class="n">totmass</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="FSWEmbedding-1623"><a href="#FSWEmbedding-1623"><span class="linenos">1623</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">totmass</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">totmass</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="n">totmass</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1624"><a href="#FSWEmbedding-1624"><span class="linenos">1624</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="FSWEmbedding-1625"><a href="#FSWEmbedding-1625"><span class="linenos">1625</span></a>
</span><span id="FSWEmbedding-1626"><a href="#FSWEmbedding-1626"><span class="linenos">1626</span></a>    <span class="nd">@staticmethod</span>
</span><span id="FSWEmbedding-1627"><a href="#FSWEmbedding-1627"><span class="linenos">1627</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_total_mass_homogeneous_legacy_encoding_part2</span><span class="p">(</span><span class="n">totmass</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="FSWEmbedding-1628"><a href="#FSWEmbedding-1628"><span class="linenos">1628</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">totmass</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">totmass</span><span class="o">.</span><span class="n">square</span><span class="p">(),</span> <span class="mi">2</span><span class="o">*</span><span class="n">totmass</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding-1629"><a href="#FSWEmbedding-1629"><span class="linenos">1629</span></a>        <span class="k">return</span> <span class="n">out</span>
</span></pre></div>


            <div class="docstring"><p>Fourier Sliced-Wasserstein (FSW) embedding module.</p>

<p>Maps input multisets (or, more generally, discrete measures) in
$\mathbb{R}^{d_\text{in}}$ to fixed-length vectors in
$\mathbb{R}^{d_\text{out}}$ via the Fourier Sliced-Wasserstein
embedding as described in [Amir &amp; Dym, ICLR 2025].</p>

<h6 id="features">Features</h6>

<p>• <strong>Batched inputs</strong>: eupports arbitrary number of batch dimensions.
• <strong>Graph mode</strong>: efficient message-aggregation, including sparse adjacency support.
• <strong>Differentiability</strong>: Full autograd/gradient support.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">FSWEmbedding.__init__</a></code>:  Constructor parameters.<br />
<code><a href="#FSWEmbedding.forward">FSWEmbedding.forward</a></code>:  Input/output tensor shapes and options.  </p>
</div>


                            <div id="FSWEmbedding.__init__" class="classattr">
                                        <input id="FSWEmbedding.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">FSWEmbedding</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">d_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">flatten_cartesian_axes</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">total_mass_encoding_transformation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n"><a href="#TotalMassEncodingTransformation">TotalMassEncodingTransformation</a></span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span>,</span><span class="param">	<span class="n">total_mass_encoding_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n"><a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span>,</span><span class="param">	<span class="n">total_mass_encoding_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>,</span><span class="param">	<span class="n">total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1.0</span>,</span><span class="param">	<span class="n">learnable_slices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">learnable_frequencies</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n"><a href="#FrequencyInitMethod">FrequencyInitMethod</a></span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span>,</span><span class="param">	<span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">enable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">use_custom_cuda_extension_if_available</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="FSWEmbedding.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.__init__-294"><a href="#FSWEmbedding.__init__-294"><span class="linenos">294</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-295"><a href="#FSWEmbedding.__init__-295"><span class="linenos">295</span></a>                 <span class="n">d_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-296"><a href="#FSWEmbedding.__init__-296"><span class="linenos">296</span></a>                 <span class="n">d_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-297"><a href="#FSWEmbedding.__init__-297"><span class="linenos">297</span></a>                 <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-298"><a href="#FSWEmbedding.__init__-298"><span class="linenos">298</span></a>                 <span class="n">num_frequencies</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-299"><a href="#FSWEmbedding.__init__-299"><span class="linenos">299</span></a>                 <span class="n">flatten_cartesian_axes</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-300"><a href="#FSWEmbedding.__init__-300"><span class="linenos">300</span></a>                 <span class="n">d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-301"><a href="#FSWEmbedding.__init__-301"><span class="linenos">301</span></a>                 <span class="n">encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-302"><a href="#FSWEmbedding.__init__-302"><span class="linenos">302</span></a>                 <span class="n">total_mass_encoding_transformation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingTransformation</span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-303"><a href="#FSWEmbedding.__init__-303"><span class="linenos">303</span></a>                 <span class="n">total_mass_encoding_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">TotalMassEncodingMethod</span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-304"><a href="#FSWEmbedding.__init__-304"><span class="linenos">304</span></a>                 <span class="n">total_mass_encoding_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-305"><a href="#FSWEmbedding.__init__-305"><span class="linenos">305</span></a>                 <span class="n">total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-306"><a href="#FSWEmbedding.__init__-306"><span class="linenos">306</span></a>                 <span class="n">learnable_slices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-307"><a href="#FSWEmbedding.__init__-307"><span class="linenos">307</span></a>                 <span class="n">learnable_frequencies</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-308"><a href="#FSWEmbedding.__init__-308"><span class="linenos">308</span></a>                 <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-309"><a href="#FSWEmbedding.__init__-309"><span class="linenos">309</span></a>                 <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-310"><a href="#FSWEmbedding.__init__-310"><span class="linenos">310</span></a>                 <span class="n">enable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-311"><a href="#FSWEmbedding.__init__-311"><span class="linenos">311</span></a>                 <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-312"><a href="#FSWEmbedding.__init__-312"><span class="linenos">312</span></a>                 <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-313"><a href="#FSWEmbedding.__init__-313"><span class="linenos">313</span></a>                 <span class="n">use_custom_cuda_extension_if_available</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-314"><a href="#FSWEmbedding.__init__-314"><span class="linenos">314</span></a>                 <span class="n">fail_if_cuda_extension_load_fails</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-315"><a href="#FSWEmbedding.__init__-315"><span class="linenos">315</span></a>                 <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.__init__-316"><a href="#FSWEmbedding.__init__-316"><span class="linenos">316</span></a>                 <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="FSWEmbedding.__init__-317"><a href="#FSWEmbedding.__init__-317"><span class="linenos">317</span></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.__init__-318"><a href="#FSWEmbedding.__init__-318"><span class="linenos">318</span></a><span class="sd">        Initialize an FSWEmbedding module.</span>
</span><span id="FSWEmbedding.__init__-319"><a href="#FSWEmbedding.__init__-319"><span class="linenos">319</span></a>
</span><span id="FSWEmbedding.__init__-320"><a href="#FSWEmbedding.__init__-320"><span class="linenos">320</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding.__init__-321"><a href="#FSWEmbedding.__init__-321"><span class="linenos">321</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding.__init__-322"><a href="#FSWEmbedding.__init__-322"><span class="linenos">322</span></a><span class="sd">        d_in : int</span>
</span><span id="FSWEmbedding.__init__-323"><a href="#FSWEmbedding.__init__-323"><span class="linenos">323</span></a><span class="sd">            The dimension of input multiset elements or, more generally, measure support points.  </span>
</span><span id="FSWEmbedding.__init__-324"><a href="#FSWEmbedding.__init__-324"><span class="linenos">324</span></a><span class="sd">            Coresponds to $d$ in $\mathcal{S}_{\leq N}\left(\mathbb{R}^d\right)$, $\mathcal{P}_{\leq N}\left(\mathbb{R}^d\right)$, or $\mathcal{M}_{\leq N}\left(\mathbb{R}^d\right)$ in our paper. </span>
</span><span id="FSWEmbedding.__init__-325"><a href="#FSWEmbedding.__init__-325"><span class="linenos">325</span></a><span class="sd">        d_out : int; optional</span>
</span><span id="FSWEmbedding.__init__-326"><a href="#FSWEmbedding.__init__-326"><span class="linenos">326</span></a><span class="sd">            Desired embedding dimension.  </span>
</span><span id="FSWEmbedding.__init__-327"><a href="#FSWEmbedding.__init__-327"><span class="linenos">327</span></a><span class="sd">            If not set, both `num_slices` and `num_frequencies` must be explicitly provided.</span>
</span><span id="FSWEmbedding.__init__-328"><a href="#FSWEmbedding.__init__-328"><span class="linenos">328</span></a><span class="sd">        num_slices : int; optional</span>
</span><span id="FSWEmbedding.__init__-329"><a href="#FSWEmbedding.__init__-329"><span class="linenos">329</span></a><span class="sd">            Number of slices.  </span>
</span><span id="FSWEmbedding.__init__-330"><a href="#FSWEmbedding.__init__-330"><span class="linenos">330</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="FSWEmbedding.__init__-331"><a href="#FSWEmbedding.__init__-331"><span class="linenos">331</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="FSWEmbedding.__init__-332"><a href="#FSWEmbedding.__init__-332"><span class="linenos">332</span></a><span class="sd">        num_frequencies : int; optional</span>
</span><span id="FSWEmbedding.__init__-333"><a href="#FSWEmbedding.__init__-333"><span class="linenos">333</span></a><span class="sd">            Number of frequencies per slice.  </span>
</span><span id="FSWEmbedding.__init__-334"><a href="#FSWEmbedding.__init__-334"><span class="linenos">334</span></a><span class="sd">            When provided, activates `cartesian_mode`, and `d_out` should be left None.  </span>
</span><span id="FSWEmbedding.__init__-335"><a href="#FSWEmbedding.__init__-335"><span class="linenos">335</span></a><span class="sd">            See also: `flatten_cartesian_axes`</span>
</span><span id="FSWEmbedding.__init__-336"><a href="#FSWEmbedding.__init__-336"><span class="linenos">336</span></a><span class="sd">        flatten_cartesian_axes : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-337"><a href="#FSWEmbedding.__init__-337"><span class="linenos">337</span></a><span class="sd">            If True, flattens the slice and frequency dimensions into a single output axis.  </span>
</span><span id="FSWEmbedding.__init__-338"><a href="#FSWEmbedding.__init__-338"><span class="linenos">338</span></a><span class="sd">            Only relevant if `num_slices` and `num_frequencies` are provided.</span>
</span><span id="FSWEmbedding.__init__-339"><a href="#FSWEmbedding.__init__-339"><span class="linenos">339</span></a><span class="sd">        d_edge : int; default=0</span>
</span><span id="FSWEmbedding.__init__-340"><a href="#FSWEmbedding.__init__-340"><span class="linenos">340</span></a><span class="sd">            Dimension of edge feature vectors. Used only for graph inputs.  </span>
</span><span id="FSWEmbedding.__init__-341"><a href="#FSWEmbedding.__init__-341"><span class="linenos">341</span></a><span class="sd">            See the `graph_mode` argument of `FSWEmbedding.forward` for details.</span>
</span><span id="FSWEmbedding.__init__-342"><a href="#FSWEmbedding.__init__-342"><span class="linenos">342</span></a><span class="sd">        encode_total_mass : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-343"><a href="#FSWEmbedding.__init__-343"><span class="linenos">343</span></a><span class="sd">            Whether to incorporate the input multiset size (or, more generally, the *total mass* of the input measure)</span>
</span><span id="FSWEmbedding.__init__-344"><a href="#FSWEmbedding.__init__-344"><span class="linenos">344</span></a><span class="sd">            into the embedding output.</span>
</span><span id="FSWEmbedding.__init__-345"><a href="#FSWEmbedding.__init__-345"><span class="linenos">345</span></a><span class="sd">        total_mass_encoding_transformation : {&#39;identity&#39;, &#39;sqrt&#39;, &#39;log&#39;} or TotalMassEncodingFunction; default=&#39;identity&#39;</span>
</span><span id="FSWEmbedding.__init__-346"><a href="#FSWEmbedding.__init__-346"><span class="linenos">346</span></a><span class="sd">            Transformation applied to the total mass *before* embedding.  </span>
</span><span id="FSWEmbedding.__init__-347"><a href="#FSWEmbedding.__init__-347"><span class="linenos">347</span></a><span class="sd">            See also: `TotalMassEncodingFunction`</span>
</span><span id="FSWEmbedding.__init__-348"><a href="#FSWEmbedding.__init__-348"><span class="linenos">348</span></a><span class="sd">        total_mass_encoding_method : {&#39;decoupled&#39;, &#39;scaled&#39;, &#39;homogeneous&#39;, &#39;homogeneous_scaled&#39;, &#39;homogeneous_legacy&#39;} or TotalMassEncodingMethod; default=&#39;decoupled&#39;</span>
</span><span id="FSWEmbedding.__init__-349"><a href="#FSWEmbedding.__init__-349"><span class="linenos">349</span></a><span class="sd">            Strategy for combining the total mass with the core embedding.  </span>
</span><span id="FSWEmbedding.__init__-350"><a href="#FSWEmbedding.__init__-350"><span class="linenos">350</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding.__init__-351"><a href="#FSWEmbedding.__init__-351"><span class="linenos">351</span></a><span class="sd">        total_mass_encoding_scale : float; default=1.0</span>
</span><span id="FSWEmbedding.__init__-352"><a href="#FSWEmbedding.__init__-352"><span class="linenos">352</span></a><span class="sd">            The encoded total mass is multiplied by this scaling factor.  </span>
</span><span id="FSWEmbedding.__init__-353"><a href="#FSWEmbedding.__init__-353"><span class="linenos">353</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding.__init__-354"><a href="#FSWEmbedding.__init__-354"><span class="linenos">354</span></a><span class="sd">        total_mass_padding_thresh : float or int; default=1.0</span>
</span><span id="FSWEmbedding.__init__-355"><a href="#FSWEmbedding.__init__-355"><span class="linenos">355</span></a><span class="sd">            Inputs with total mass below this threshold are padded with the zero vector to reach it; see</span>
</span><span id="FSWEmbedding.__init__-356"><a href="#FSWEmbedding.__init__-356"><span class="linenos">356</span></a><span class="sd">            in [Amir and Dym, ICLR 2025], Appendix A.1.  </span>
</span><span id="FSWEmbedding.__init__-357"><a href="#FSWEmbedding.__init__-357"><span class="linenos">357</span></a><span class="sd">            See also: `TotalMassEncodingMethod`</span>
</span><span id="FSWEmbedding.__init__-358"><a href="#FSWEmbedding.__init__-358"><span class="linenos">358</span></a><span class="sd">        learnable_slices : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-359"><a href="#FSWEmbedding.__init__-359"><span class="linenos">359</span></a><span class="sd">            If True, slice vectors are learnable parameters.  </span>
</span><span id="FSWEmbedding.__init__-360"><a href="#FSWEmbedding.__init__-360"><span class="linenos">360</span></a><span class="sd">        learnable_frequencies : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-361"><a href="#FSWEmbedding.__init__-361"><span class="linenos">361</span></a><span class="sd">            If True, frequency values are learnable parameters.</span>
</span><span id="FSWEmbedding.__init__-362"><a href="#FSWEmbedding.__init__-362"><span class="linenos">362</span></a><span class="sd">        frequency_init : float, str, tuple of float, or FrequencyInitMethod; default=&#39;random&#39;</span>
</span><span id="FSWEmbedding.__init__-363"><a href="#FSWEmbedding.__init__-363"><span class="linenos">363</span></a><span class="sd">            Initialization scheme for frequencies:</span>
</span><span id="FSWEmbedding.__init__-364"><a href="#FSWEmbedding.__init__-364"><span class="linenos">364</span></a><span class="sd">              - A float: sets all frequencies to the same value.</span>
</span><span id="FSWEmbedding.__init__-365"><a href="#FSWEmbedding.__init__-365"><span class="linenos">365</span></a><span class="sd">              - A tuple `(low, high)` of floats: sets evenly spaced values in that interval.</span>
</span><span id="FSWEmbedding.__init__-366"><a href="#FSWEmbedding.__init__-366"><span class="linenos">366</span></a><span class="sd">              - &#39;random&#39;: frequencies are drawn independently from the distribution $\mathcal{D_{\xi}}$, defined in</span>
</span><span id="FSWEmbedding.__init__-367"><a href="#FSWEmbedding.__init__-367"><span class="linenos">367</span></a><span class="sd">                          [Amir and Dym, ICLR 2025], Section 3.</span>
</span><span id="FSWEmbedding.__init__-368"><a href="#FSWEmbedding.__init__-368"><span class="linenos">368</span></a><span class="sd">              - &#39;even&#39;: frequencies are spaced evenly according to their distribution $\mathcal{D_{\xi}}$, with spaces</span>
</span><span id="FSWEmbedding.__init__-369"><a href="#FSWEmbedding.__init__-369"><span class="linenos">369</span></a><span class="sd">                        inversely proportional to the density.  </span>
</span><span id="FSWEmbedding.__init__-370"><a href="#FSWEmbedding.__init__-370"><span class="linenos">370</span></a><span class="sd">            See also: `FrequencyInitMethod`</span>
</span><span id="FSWEmbedding.__init__-371"><a href="#FSWEmbedding.__init__-371"><span class="linenos">371</span></a><span class="sd">        minimize_slice_coherence : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-372"><a href="#FSWEmbedding.__init__-372"><span class="linenos">372</span></a><span class="sd">            If True, minimizes the *mutual coherence* between slices for a more uniform spread on the unit sphere.  </span>
</span><span id="FSWEmbedding.__init__-373"><a href="#FSWEmbedding.__init__-373"><span class="linenos">373</span></a><span class="sd">            If False, slice vectors are drawn uniformly at random from the unit sphere.</span>
</span><span id="FSWEmbedding.__init__-374"><a href="#FSWEmbedding.__init__-374"><span class="linenos">374</span></a><span class="sd">        enable_bias : bool; default=True</span>
</span><span id="FSWEmbedding.__init__-375"><a href="#FSWEmbedding.__init__-375"><span class="linenos">375</span></a><span class="sd">            If True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized</span>
</span><span id="FSWEmbedding.__init__-376"><a href="#FSWEmbedding.__init__-376"><span class="linenos">376</span></a><span class="sd">            to zero.  </span>
</span><span id="FSWEmbedding.__init__-377"><a href="#FSWEmbedding.__init__-377"><span class="linenos">377</span></a><span class="sd">        device : torch.device, int, str, or None, optional</span>
</span><span id="FSWEmbedding.__init__-378"><a href="#FSWEmbedding.__init__-378"><span class="linenos">378</span></a><span class="sd">            The torch device on which to allocate tensors (e.g., &#39;cpu&#39;, &#39;cuda&#39;, or an index).  </span>
</span><span id="FSWEmbedding.__init__-379"><a href="#FSWEmbedding.__init__-379"><span class="linenos">379</span></a><span class="sd">            If not provided, the default device defined in Torch is used.</span>
</span><span id="FSWEmbedding.__init__-380"><a href="#FSWEmbedding.__init__-380"><span class="linenos">380</span></a><span class="sd">        dtype : torch.dtype, optional</span>
</span><span id="FSWEmbedding.__init__-381"><a href="#FSWEmbedding.__init__-381"><span class="linenos">381</span></a><span class="sd">            Data type of input and output tensors (e.g., torch.float32).</span>
</span><span id="FSWEmbedding.__init__-382"><a href="#FSWEmbedding.__init__-382"><span class="linenos">382</span></a><span class="sd">            If not provided, the default dtype defined in Torch is used.</span>
</span><span id="FSWEmbedding.__init__-383"><a href="#FSWEmbedding.__init__-383"><span class="linenos">383</span></a><span class="sd">        use_custom_cuda_extension_if_available : bool or None, optional</span>
</span><span id="FSWEmbedding.__init__-384"><a href="#FSWEmbedding.__init__-384"><span class="linenos">384</span></a><span class="sd">            Whether to use the custom CUDA kernel if present.</span>
</span><span id="FSWEmbedding.__init__-385"><a href="#FSWEmbedding.__init__-385"><span class="linenos">385</span></a><span class="sd">            Default: Linux: True, all other systems: False</span>
</span><span id="FSWEmbedding.__init__-386"><a href="#FSWEmbedding.__init__-386"><span class="linenos">386</span></a><span class="sd">        fail_if_cuda_extension_load_fails : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-387"><a href="#FSWEmbedding.__init__-387"><span class="linenos">387</span></a><span class="sd">            Whether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</span>
</span><span id="FSWEmbedding.__init__-388"><a href="#FSWEmbedding.__init__-388"><span class="linenos">388</span></a><span class="sd">        report : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-389"><a href="#FSWEmbedding.__init__-389"><span class="linenos">389</span></a><span class="sd">            If True, prints a report with diagnostic information during initialization and forward computation.</span>
</span><span id="FSWEmbedding.__init__-390"><a href="#FSWEmbedding.__init__-390"><span class="linenos">390</span></a><span class="sd">        report_on_coherence_minimization : bool; default=False</span>
</span><span id="FSWEmbedding.__init__-391"><a href="#FSWEmbedding.__init__-391"><span class="linenos">391</span></a><span class="sd">            If True, prints special diagnostics during slice coherence minimization.</span>
</span><span id="FSWEmbedding.__init__-392"><a href="#FSWEmbedding.__init__-392"><span class="linenos">392</span></a>
</span><span id="FSWEmbedding.__init__-393"><a href="#FSWEmbedding.__init__-393"><span class="linenos">393</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.__init__-394"><a href="#FSWEmbedding.__init__-394"><span class="linenos">394</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.__init__-395"><a href="#FSWEmbedding.__init__-395"><span class="linenos">395</span></a><span class="sd">        If Cartesian mode is activated and `encode_total_mass` is True, `flatten_cartesian_axes` must be True.</span>
</span><span id="FSWEmbedding.__init__-396"><a href="#FSWEmbedding.__init__-396"><span class="linenos">396</span></a>
</span><span id="FSWEmbedding.__init__-397"><a href="#FSWEmbedding.__init__-397"><span class="linenos">397</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.__init__-398"><a href="#FSWEmbedding.__init__-398"><span class="linenos">398</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.__init__-399"><a href="#FSWEmbedding.__init__-399"><span class="linenos">399</span></a><span class="sd">        FrequencyInitMethod :</span>
</span><span id="FSWEmbedding.__init__-400"><a href="#FSWEmbedding.__init__-400"><span class="linenos">400</span></a><span class="sd">            Enum for selecting frequency initialization strategies.</span>
</span><span id="FSWEmbedding.__init__-401"><a href="#FSWEmbedding.__init__-401"><span class="linenos">401</span></a><span class="sd">        TotalMassEncodingTransformation :</span>
</span><span id="FSWEmbedding.__init__-402"><a href="#FSWEmbedding.__init__-402"><span class="linenos">402</span></a><span class="sd">            Enum for total mass transformations.</span>
</span><span id="FSWEmbedding.__init__-403"><a href="#FSWEmbedding.__init__-403"><span class="linenos">403</span></a><span class="sd">        TotalMassEncodingMethod :</span>
</span><span id="FSWEmbedding.__init__-404"><a href="#FSWEmbedding.__init__-404"><span class="linenos">404</span></a><span class="sd">            Enum for strategies to incorporate total mass into the embedding.</span>
</span><span id="FSWEmbedding.__init__-405"><a href="#FSWEmbedding.__init__-405"><span class="linenos">405</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.__init__-406"><a href="#FSWEmbedding.__init__-406"><span class="linenos">406</span></a>
</span><span id="FSWEmbedding.__init__-407"><a href="#FSWEmbedding.__init__-407"><span class="linenos">407</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="FSWEmbedding.__init__-408"><a href="#FSWEmbedding.__init__-408"><span class="linenos">408</span></a>
</span><span id="FSWEmbedding.__init__-409"><a href="#FSWEmbedding.__init__-409"><span class="linenos">409</span></a>        <span class="c1"># Process sizes</span>
</span><span id="FSWEmbedding.__init__-410"><a href="#FSWEmbedding.__init__-410"><span class="linenos">410</span></a>        <span class="k">assert</span> <span class="n">d_in</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_in must be nonnegative&#39;</span>
</span><span id="FSWEmbedding.__init__-411"><a href="#FSWEmbedding.__init__-411"><span class="linenos">411</span></a>        <span class="k">assert</span> <span class="n">d_edge</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_edge must be nonnegative&#39;</span>
</span><span id="FSWEmbedding.__init__-412"><a href="#FSWEmbedding.__init__-412"><span class="linenos">412</span></a>        <span class="k">assert</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;d_out must be nonnegative or None&#39;</span>
</span><span id="FSWEmbedding.__init__-413"><a href="#FSWEmbedding.__init__-413"><span class="linenos">413</span></a>
</span><span id="FSWEmbedding.__init__-414"><a href="#FSWEmbedding.__init__-414"><span class="linenos">414</span></a>        <span class="k">if</span> <span class="n">d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-415"><a href="#FSWEmbedding.__init__-415"><span class="linenos">415</span></a>            <span class="c1"># If the output should be empty, we force encode_total_mass to be False</span>
</span><span id="FSWEmbedding.__init__-416"><a href="#FSWEmbedding.__init__-416"><span class="linenos">416</span></a>            <span class="n">encode_total_mass</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="FSWEmbedding.__init__-417"><a href="#FSWEmbedding.__init__-417"><span class="linenos">417</span></a>
</span><span id="FSWEmbedding.__init__-418"><a href="#FSWEmbedding.__init__-418"><span class="linenos">418</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_in</span>
</span><span id="FSWEmbedding.__init__-419"><a href="#FSWEmbedding.__init__-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">d_edge</span>
</span><span id="FSWEmbedding.__init__-420"><a href="#FSWEmbedding.__init__-420"><span class="linenos">420</span></a>
</span><span id="FSWEmbedding.__init__-421"><a href="#FSWEmbedding.__init__-421"><span class="linenos">421</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">encode_total_mass</span>
</span><span id="FSWEmbedding.__init__-422"><a href="#FSWEmbedding.__init__-422"><span class="linenos">422</span></a>
</span><span id="FSWEmbedding.__init__-423"><a href="#FSWEmbedding.__init__-423"><span class="linenos">423</span></a>        <span class="n">total_mass_padding_thresh</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-424"><a href="#FSWEmbedding.__init__-424"><span class="linenos">424</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be inf&#39;</span>
</span><span id="FSWEmbedding.__init__-425"><a href="#FSWEmbedding.__init__-425"><span class="linenos">425</span></a>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">total_mass_padding_thresh</span><span class="p">),</span> <span class="s1">&#39;total_mass_padding_thresh cannot be NaN&#39;</span>
</span><span id="FSWEmbedding.__init__-426"><a href="#FSWEmbedding.__init__-426"><span class="linenos">426</span></a>        <span class="k">assert</span> <span class="n">total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="FSWEmbedding.__init__-427"><a href="#FSWEmbedding.__init__-427"><span class="linenos">427</span></a>
</span><span id="FSWEmbedding.__init__-428"><a href="#FSWEmbedding.__init__-428"><span class="linenos">428</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="FSWEmbedding.__init__-429"><a href="#FSWEmbedding.__init__-429"><span class="linenos">429</span></a>        <span class="k">del</span> <span class="n">total_mass_padding_thresh</span>
</span><span id="FSWEmbedding.__init__-430"><a href="#FSWEmbedding.__init__-430"><span class="linenos">430</span></a>
</span><span id="FSWEmbedding.__init__-431"><a href="#FSWEmbedding.__init__-431"><span class="linenos">431</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="o">=</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_method</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-432"><a href="#FSWEmbedding.__init__-432"><span class="linenos">432</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_method</span>
</span><span id="FSWEmbedding.__init__-433"><a href="#FSWEmbedding.__init__-433"><span class="linenos">433</span></a>
</span><span id="FSWEmbedding.__init__-434"><a href="#FSWEmbedding.__init__-434"><span class="linenos">434</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span> <span class="o">=</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="FSWEmbedding.__init__-435"><a href="#FSWEmbedding.__init__-435"><span class="linenos">435</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_scale</span>
</span><span id="FSWEmbedding.__init__-436"><a href="#FSWEmbedding.__init__-436"><span class="linenos">436</span></a>
</span><span id="FSWEmbedding.__init__-437"><a href="#FSWEmbedding.__init__-437"><span class="linenos">437</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span> <span class="o">=</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">total_mass_encoding_transformation</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-438"><a href="#FSWEmbedding.__init__-438"><span class="linenos">438</span></a>        <span class="k">del</span> <span class="n">total_mass_encoding_transformation</span>
</span><span id="FSWEmbedding.__init__-439"><a href="#FSWEmbedding.__init__-439"><span class="linenos">439</span></a>
</span><span id="FSWEmbedding.__init__-440"><a href="#FSWEmbedding.__init__-440"><span class="linenos">440</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-441"><a href="#FSWEmbedding.__init__-441"><span class="linenos">441</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span><span id="FSWEmbedding.__init__-442"><a href="#FSWEmbedding.__init__-442"><span class="linenos">442</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-443"><a href="#FSWEmbedding.__init__-443"><span class="linenos">443</span></a>            <span class="n">input_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^(</span><span class="si">%d</span><span class="s1">+</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-444"><a href="#FSWEmbedding.__init__-444"><span class="linenos">444</span></a>
</span><span id="FSWEmbedding.__init__-445"><a href="#FSWEmbedding.__init__-445"><span class="linenos">445</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="FSWEmbedding.__init__-446"><a href="#FSWEmbedding.__init__-446"><span class="linenos">446</span></a>
</span><span id="FSWEmbedding.__init__-447"><a href="#FSWEmbedding.__init__-447"><span class="linenos">447</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding.__init__-448"><a href="#FSWEmbedding.__init__-448"><span class="linenos">448</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding.__init__-449"><a href="#FSWEmbedding.__init__-449"><span class="linenos">449</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding.__init__-450"><a href="#FSWEmbedding.__init__-450"><span class="linenos">450</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">d_out</span>
</span><span id="FSWEmbedding.__init__-451"><a href="#FSWEmbedding.__init__-451"><span class="linenos">451</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding.__init__-452"><a href="#FSWEmbedding.__init__-452"><span class="linenos">452</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">-</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding.__init__-453"><a href="#FSWEmbedding.__init__-453"><span class="linenos">453</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span><span id="FSWEmbedding.__init__-454"><a href="#FSWEmbedding.__init__-454"><span class="linenos">454</span></a>
</span><span id="FSWEmbedding.__init__-455"><a href="#FSWEmbedding.__init__-455"><span class="linenos">455</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">d_out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_frequencies</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding.__init__-456"><a href="#FSWEmbedding.__init__-456"><span class="linenos">456</span></a>            <span class="k">assert</span> <span class="n">flatten_cartesian_axes</span>  <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">encode_total_mass</span><span class="p">),</span> <span class="s1">&#39;Cartesian mode with flatten_cartesian_axes =False is not supported when encode_total_mass=True&#39;</span>
</span><span id="FSWEmbedding.__init__-457"><a href="#FSWEmbedding.__init__-457"><span class="linenos">457</span></a>
</span><span id="FSWEmbedding.__init__-458"><a href="#FSWEmbedding.__init__-458"><span class="linenos">458</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="FSWEmbedding.__init__-459"><a href="#FSWEmbedding.__init__-459"><span class="linenos">459</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="o">=</span> <span class="n">flatten_cartesian_axes</span>
</span><span id="FSWEmbedding.__init__-460"><a href="#FSWEmbedding.__init__-460"><span class="linenos">460</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">=</span> <span class="n">num_slices</span>
</span><span id="FSWEmbedding.__init__-461"><a href="#FSWEmbedding.__init__-461"><span class="linenos">461</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span> <span class="o">=</span> <span class="n">num_frequencies</span>
</span><span id="FSWEmbedding.__init__-462"><a href="#FSWEmbedding.__init__-462"><span class="linenos">462</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">=</span> <span class="n">num_slices</span> <span class="o">*</span> <span class="n">num_frequencies</span> <span class="o">+</span> <span class="n">total_mass_encoding_dim</span>
</span><span id="FSWEmbedding.__init__-463"><a href="#FSWEmbedding.__init__-463"><span class="linenos">463</span></a>            <span class="n">output_space_name</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;R^</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>  <span class="k">else</span> <span class="p">(</span><span class="s1">&#39;R^(</span><span class="si">%d</span><span class="se">\u00d7</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="FSWEmbedding.__init__-464"><a href="#FSWEmbedding.__init__-464"><span class="linenos">464</span></a>
</span><span id="FSWEmbedding.__init__-465"><a href="#FSWEmbedding.__init__-465"><span class="linenos">465</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-466"><a href="#FSWEmbedding.__init__-466"><span class="linenos">466</span></a>            <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Expected exactly one of (d_out != None) or (num_slices != None and num_frequencies != None)&quot;</span>
</span><span id="FSWEmbedding.__init__-467"><a href="#FSWEmbedding.__init__-467"><span class="linenos">467</span></a>
</span><span id="FSWEmbedding.__init__-468"><a href="#FSWEmbedding.__init__-468"><span class="linenos">468</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;d_out must be nonnegative&#39;</span>
</span><span id="FSWEmbedding.__init__-469"><a href="#FSWEmbedding.__init__-469"><span class="linenos">469</span></a>
</span><span id="FSWEmbedding.__init__-470"><a href="#FSWEmbedding.__init__-470"><span class="linenos">470</span></a>        <span class="c1">#d_out = self.d_out</span>
</span><span id="FSWEmbedding.__init__-471"><a href="#FSWEmbedding.__init__-471"><span class="linenos">471</span></a>        <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span><span id="FSWEmbedding.__init__-472"><a href="#FSWEmbedding.__init__-472"><span class="linenos">472</span></a>        <span class="n">num_frequencies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span><span id="FSWEmbedding.__init__-473"><a href="#FSWEmbedding.__init__-473"><span class="linenos">473</span></a>
</span><span id="FSWEmbedding.__init__-474"><a href="#FSWEmbedding.__init__-474"><span class="linenos">474</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">minimize_slice_coherence</span>
</span><span id="FSWEmbedding.__init__-475"><a href="#FSWEmbedding.__init__-475"><span class="linenos">475</span></a>
</span><span id="FSWEmbedding.__init__-476"><a href="#FSWEmbedding.__init__-476"><span class="linenos">476</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="o">=</span> <span class="n">learnable_slices</span>
</span><span id="FSWEmbedding.__init__-477"><a href="#FSWEmbedding.__init__-477"><span class="linenos">477</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span> <span class="o">=</span> <span class="n">learnable_frequencies</span>
</span><span id="FSWEmbedding.__init__-478"><a href="#FSWEmbedding.__init__-478"><span class="linenos">478</span></a>
</span><span id="FSWEmbedding.__init__-479"><a href="#FSWEmbedding.__init__-479"><span class="linenos">479</span></a>        <span class="c1"># Note: frequency_init is checked for correctness downstream at generate_embedding_parameters()</span>
</span><span id="FSWEmbedding.__init__-480"><a href="#FSWEmbedding.__init__-480"><span class="linenos">480</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">frequency_init</span>
</span><span id="FSWEmbedding.__init__-481"><a href="#FSWEmbedding.__init__-481"><span class="linenos">481</span></a>
</span><span id="FSWEmbedding.__init__-482"><a href="#FSWEmbedding.__init__-482"><span class="linenos">482</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span> <span class="o">=</span> <span class="n">enable_bias</span>
</span><span id="FSWEmbedding.__init__-483"><a href="#FSWEmbedding.__init__-483"><span class="linenos">483</span></a>
</span><span id="FSWEmbedding.__init__-484"><a href="#FSWEmbedding.__init__-484"><span class="linenos">484</span></a>        <span class="c1"># _device_new and _dtype_new are only defined here on __init__ and passed on to reset_parameters(), which then deletes them</span>
</span><span id="FSWEmbedding.__init__-485"><a href="#FSWEmbedding.__init__-485"><span class="linenos">485</span></a>        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-486"><a href="#FSWEmbedding.__init__-486"><span class="linenos">486</span></a>            <span class="c1"># Use get_default_device if available (PyTorch 2.3+)</span>
</span><span id="FSWEmbedding.__init__-487"><a href="#FSWEmbedding.__init__-487"><span class="linenos">487</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_device&quot;</span><span class="p">):</span>
</span><span id="FSWEmbedding.__init__-488"><a href="#FSWEmbedding.__init__-488"><span class="linenos">488</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_device</span><span class="p">()</span>
</span><span id="FSWEmbedding.__init__-489"><a href="#FSWEmbedding.__init__-489"><span class="linenos">489</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-490"><a href="#FSWEmbedding.__init__-490"><span class="linenos">490</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="FSWEmbedding.__init__-491"><a href="#FSWEmbedding.__init__-491"><span class="linenos">491</span></a>                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">device</span>
</span><span id="FSWEmbedding.__init__-492"><a href="#FSWEmbedding.__init__-492"><span class="linenos">492</span></a>
</span><span id="FSWEmbedding.__init__-493"><a href="#FSWEmbedding.__init__-493"><span class="linenos">493</span></a>        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-494"><a href="#FSWEmbedding.__init__-494"><span class="linenos">494</span></a>            <span class="c1"># Use get_default_dtype if available</span>
</span><span id="FSWEmbedding.__init__-495"><a href="#FSWEmbedding.__init__-495"><span class="linenos">495</span></a>            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;get_default_dtype&quot;</span><span class="p">):</span>
</span><span id="FSWEmbedding.__init__-496"><a href="#FSWEmbedding.__init__-496"><span class="linenos">496</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>
</span><span id="FSWEmbedding.__init__-497"><a href="#FSWEmbedding.__init__-497"><span class="linenos">497</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-498"><a href="#FSWEmbedding.__init__-498"><span class="linenos">498</span></a>                <span class="c1"># Fallback: infer from a dummy tensor</span>
</span><span id="FSWEmbedding.__init__-499"><a href="#FSWEmbedding.__init__-499"><span class="linenos">499</span></a>                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="FSWEmbedding.__init__-500"><a href="#FSWEmbedding.__init__-500"><span class="linenos">500</span></a>
</span><span id="FSWEmbedding.__init__-501"><a href="#FSWEmbedding.__init__-501"><span class="linenos">501</span></a>        <span class="k">assert</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">),</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">dtype</span>
</span><span id="FSWEmbedding.__init__-502"><a href="#FSWEmbedding.__init__-502"><span class="linenos">502</span></a>
</span><span id="FSWEmbedding.__init__-503"><a href="#FSWEmbedding.__init__-503"><span class="linenos">503</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span> <span class="o">=</span> <span class="n">device</span>
</span><span id="FSWEmbedding.__init__-504"><a href="#FSWEmbedding.__init__-504"><span class="linenos">504</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span> <span class="o">=</span> <span class="n">dtype</span>
</span><span id="FSWEmbedding.__init__-505"><a href="#FSWEmbedding.__init__-505"><span class="linenos">505</span></a>
</span><span id="FSWEmbedding.__init__-506"><a href="#FSWEmbedding.__init__-506"><span class="linenos">506</span></a>        <span class="k">if</span> <span class="n">use_custom_cuda_extension_if_available</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-507"><a href="#FSWEmbedding.__init__-507"><span class="linenos">507</span></a>            <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;Windows&#39;</span><span class="p">,</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">}:</span>
</span><span id="FSWEmbedding.__init__-508"><a href="#FSWEmbedding.__init__-508"><span class="linenos">508</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="FSWEmbedding.__init__-509"><a href="#FSWEmbedding.__init__-509"><span class="linenos">509</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-510"><a href="#FSWEmbedding.__init__-510"><span class="linenos">510</span></a>                <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="FSWEmbedding.__init__-511"><a href="#FSWEmbedding.__init__-511"><span class="linenos">511</span></a>
</span><span id="FSWEmbedding.__init__-512"><a href="#FSWEmbedding.__init__-512"><span class="linenos">512</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="n">use_custom_cuda_extension_if_available</span>
</span><span id="FSWEmbedding.__init__-513"><a href="#FSWEmbedding.__init__-513"><span class="linenos">513</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="n">fail_if_cuda_extension_load_fails</span>
</span><span id="FSWEmbedding.__init__-514"><a href="#FSWEmbedding.__init__-514"><span class="linenos">514</span></a>
</span><span id="FSWEmbedding.__init__-515"><a href="#FSWEmbedding.__init__-515"><span class="linenos">515</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">report</span>
</span><span id="FSWEmbedding.__init__-516"><a href="#FSWEmbedding.__init__-516"><span class="linenos">516</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="FSWEmbedding.__init__-517"><a href="#FSWEmbedding.__init__-517"><span class="linenos">517</span></a>
</span><span id="FSWEmbedding.__init__-518"><a href="#FSWEmbedding.__init__-518"><span class="linenos">518</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-519"><a href="#FSWEmbedding.__init__-519"><span class="linenos">519</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Fourier Sliced-Wasserstein Embedding&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-520"><a href="#FSWEmbedding.__init__-520"><span class="linenos">520</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;version </span><span class="si">%s</span><span class="s1">, </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">version</span><span class="p">,</span> <span class="n">version_date</span><span class="p">))</span>
</span><span id="FSWEmbedding.__init__-521"><a href="#FSWEmbedding.__init__-521"><span class="linenos">521</span></a>
</span><span id="FSWEmbedding.__init__-522"><a href="#FSWEmbedding.__init__-522"><span class="linenos">522</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-523"><a href="#FSWEmbedding.__init__-523"><span class="linenos">523</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Based on our paper titled &quot;Fourier Sliced-Wasserstrin Embedding for Multisets and Measures&quot;, ICLR 2025&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-524"><a href="#FSWEmbedding.__init__-524"><span class="linenos">524</span></a>
</span><span id="FSWEmbedding.__init__-525"><a href="#FSWEmbedding.__init__-525"><span class="linenos">525</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-526"><a href="#FSWEmbedding.__init__-526"><span class="linenos">526</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;Constructing embedding for sets in </span><span class="si">%s</span><span class="s1"> into </span><span class="si">%s</span><span class="s1">  &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">input_space_name</span><span class="p">,</span> <span class="n">output_space_name</span><span class="p">))</span>
</span><span id="FSWEmbedding.__init__-527"><a href="#FSWEmbedding.__init__-527"><span class="linenos">527</span></a>
</span><span id="FSWEmbedding.__init__-528"><a href="#FSWEmbedding.__init__-528"><span class="linenos">528</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding.__init__-529"><a href="#FSWEmbedding.__init__-529"><span class="linenos">529</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%d</span><span class="s1"> frequencies, collapsed to one </span><span class="si">%d</span><span class="s1"> dimensional axis; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">,</span> <span class="n">num_slices</span><span class="o">*</span><span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-530"><a href="#FSWEmbedding.__init__-530"><span class="linenos">530</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-531"><a href="#FSWEmbedding.__init__-531"><span class="linenos">531</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> slices </span><span class="se">\u00d7</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> frequencies; &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-532"><a href="#FSWEmbedding.__init__-532"><span class="linenos">532</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-533"><a href="#FSWEmbedding.__init__-533"><span class="linenos">533</span></a>            <span class="n">slice_freq_str</span> <span class="o">=</span> <span class="s1">&#39;Using </span><span class="si">%d</span><span class="s1"> (slice, frequency) pairs; &#39;</span> <span class="o">%</span> <span class="n">num_slices</span>
</span><span id="FSWEmbedding.__init__-534"><a href="#FSWEmbedding.__init__-534"><span class="linenos">534</span></a>
</span><span id="FSWEmbedding.__init__-535"><a href="#FSWEmbedding.__init__-535"><span class="linenos">535</span></a>        <span class="n">qprint</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">slice_freq_str</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-536"><a href="#FSWEmbedding.__init__-536"><span class="linenos">536</span></a>
</span><span id="FSWEmbedding.__init__-537"><a href="#FSWEmbedding.__init__-537"><span class="linenos">537</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-538"><a href="#FSWEmbedding.__init__-538"><span class="linenos">538</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-539"><a href="#FSWEmbedding.__init__-539"><span class="linenos">539</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, frequences and biases&#39;</span>
</span><span id="FSWEmbedding.__init__-540"><a href="#FSWEmbedding.__init__-540"><span class="linenos">540</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-541"><a href="#FSWEmbedding.__init__-541"><span class="linenos">541</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and frequences, no bias&#39;</span>
</span><span id="FSWEmbedding.__init__-542"><a href="#FSWEmbedding.__init__-542"><span class="linenos">542</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-543"><a href="#FSWEmbedding.__init__-543"><span class="linenos">543</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-544"><a href="#FSWEmbedding.__init__-544"><span class="linenos">544</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices and biases, fixed frequencies&#39;</span>
</span><span id="FSWEmbedding.__init__-545"><a href="#FSWEmbedding.__init__-545"><span class="linenos">545</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-546"><a href="#FSWEmbedding.__init__-546"><span class="linenos">546</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;learnable slices, fixed frequences, no bias&#39;</span>
</span><span id="FSWEmbedding.__init__-547"><a href="#FSWEmbedding.__init__-547"><span class="linenos">547</span></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-548"><a href="#FSWEmbedding.__init__-548"><span class="linenos">548</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-549"><a href="#FSWEmbedding.__init__-549"><span class="linenos">549</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="FSWEmbedding.__init__-550"><a href="#FSWEmbedding.__init__-550"><span class="linenos">550</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-551"><a href="#FSWEmbedding.__init__-551"><span class="linenos">551</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices, learnable frequencies, no biases&#39;</span>
</span><span id="FSWEmbedding.__init__-552"><a href="#FSWEmbedding.__init__-552"><span class="linenos">552</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-553"><a href="#FSWEmbedding.__init__-553"><span class="linenos">553</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-554"><a href="#FSWEmbedding.__init__-554"><span class="linenos">554</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, fixed biases (initialized to zero)&#39;</span>
</span><span id="FSWEmbedding.__init__-555"><a href="#FSWEmbedding.__init__-555"><span class="linenos">555</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.__init__-556"><a href="#FSWEmbedding.__init__-556"><span class="linenos">556</span></a>                <span class="n">learnable_str</span> <span class="o">=</span> <span class="s1">&#39;fixed slices and frequencies, no bias&#39;</span>
</span><span id="FSWEmbedding.__init__-557"><a href="#FSWEmbedding.__init__-557"><span class="linenos">557</span></a>
</span><span id="FSWEmbedding.__init__-558"><a href="#FSWEmbedding.__init__-558"><span class="linenos">558</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">learnable_str</span><span class="p">)</span>
</span><span id="FSWEmbedding.__init__-559"><a href="#FSWEmbedding.__init__-559"><span class="linenos">559</span></a>
</span><span id="FSWEmbedding.__init__-560"><a href="#FSWEmbedding.__init__-560"><span class="linenos">560</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="s1">&#39;device: </span><span class="si">%s</span><span class="s1">    dtype: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span><span class="p">))</span>
</span><span id="FSWEmbedding.__init__-561"><a href="#FSWEmbedding.__init__-561"><span class="linenos">561</span></a>
</span><span id="FSWEmbedding.__init__-562"><a href="#FSWEmbedding.__init__-562"><span class="linenos">562</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.__init__-563"><a href="#FSWEmbedding.__init__-563"><span class="linenos">563</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.__init__-564"><a href="#FSWEmbedding.__init__-564"><span class="linenos">564</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.__init__-565"><a href="#FSWEmbedding.__init__-565"><span class="linenos">565</span></a>
</span><span id="FSWEmbedding.__init__-566"><a href="#FSWEmbedding.__init__-566"><span class="linenos">566</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Initialize an FSWEmbedding module.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>d_in</strong> (int):
The dimension of input multiset elements or, more generally, measure support points.<br />
Coresponds to $d$ in $\mathcal{S}_{\leq N}\left(\mathbb{R}^d\right)$, $\mathcal{P}_{\leq N}\left(\mathbb{R}^d\right)$, or $\mathcal{M}_{\leq N}\left(\mathbb{R}^d\right)$ in our paper.</li>
<li><strong>d_out</strong> (int; optional):
Desired embedding dimension.<br />
If not set, both <code><a href="#FSWEmbedding.num_slices">num_slices</a></code> and <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code> must be explicitly provided.</li>
<li><strong>num_slices</strong> (int; optional):
Number of slices.<br />
When provided, activates <code><a href="#FSWEmbedding.cartesian_mode">cartesian_mode</a></code>, and <code><a href="#FSWEmbedding.d_out">d_out</a></code> should be left None.<br />
See also: <code><a href="#FSWEmbedding.flatten_cartesian_axes">flatten_cartesian_axes</a></code></li>
<li><strong>num_frequencies</strong> (int; optional):
Number of frequencies per slice.<br />
When provided, activates <code><a href="#FSWEmbedding.cartesian_mode">cartesian_mode</a></code>, and <code><a href="#FSWEmbedding.d_out">d_out</a></code> should be left None.<br />
See also: <code><a href="#FSWEmbedding.flatten_cartesian_axes">flatten_cartesian_axes</a></code></li>
<li><strong>flatten_cartesian_axes</strong> (bool; default=False):
If True, flattens the slice and frequency dimensions into a single output axis.<br />
Only relevant if <code><a href="#FSWEmbedding.num_slices">num_slices</a></code> and <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code> are provided.</li>
<li><strong>d_edge</strong> (int; default=0):
Dimension of edge feature vectors. Used only for graph inputs.<br />
See the <code>graph_mode</code> argument of <code><a href="#FSWEmbedding.forward">FSWEmbedding.forward</a></code> for details.</li>
<li><strong>encode_total_mass</strong> (bool; default=False):
Whether to incorporate the input multiset size (or, more generally, the <em>total mass</em> of the input measure)
into the embedding output.</li>
<li><strong>total_mass_encoding_transformation</strong> ({'identity', 'sqrt', 'log'} or TotalMassEncodingFunction; default='identity'):
Transformation applied to the total mass <em>before</em> embedding.<br />
See also: <code>TotalMassEncodingFunction</code></li>
<li><strong>total_mass_encoding_method</strong> ({'decoupled', 'scaled', 'homogeneous', 'homogeneous_scaled', 'homogeneous_legacy'} or TotalMassEncodingMethod; default='decoupled'):
Strategy for combining the total mass with the core embedding.<br />
See also: <code><a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></code></li>
<li><strong>total_mass_encoding_scale</strong> (float; default=1.0):
The encoded total mass is multiplied by this scaling factor.<br />
See also: <code><a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></code></li>
<li><strong>total_mass_padding_thresh</strong> (float or int; default=1.0):
Inputs with total mass below this threshold are padded with the zero vector to reach it; see
in [Amir and Dym, ICLR 2025], Appendix A.1.<br />
See also: <code><a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></code></li>
<li><strong>learnable_slices</strong> (bool; default=False):
If True, slice vectors are learnable parameters.</li>
<li><strong>learnable_frequencies</strong> (bool; default=False):
If True, frequency values are learnable parameters.</li>
<li><strong>frequency_init</strong> (float, str, tuple of float, or FrequencyInitMethod; default='random'):
Initialization scheme for frequencies:
<ul>
<li>A float: sets all frequencies to the same value.</li>
<li>A tuple <code>(low, high)</code> of floats: sets evenly spaced values in that interval.</li>
<li>'random': frequencies are drawn independently from the distribution $\mathcal{D_{\xi}}$, defined in
[Amir and Dym, ICLR 2025], Section 3.</li>
<li>'even': frequencies are spaced evenly according to their distribution $\mathcal{D_{\xi}}$, with spaces
inversely proportional to the density.<br />
See also: <code><a href="#FrequencyInitMethod">FrequencyInitMethod</a></code></li>
</ul></li>
<li><strong>minimize_slice_coherence</strong> (bool; default=False):
If True, minimizes the <em>mutual coherence</em> between slices for a more uniform spread on the unit sphere.<br />
If False, slice vectors are drawn uniformly at random from the unit sphere.</li>
<li><strong>enable_bias</strong> (bool; default=True):
If True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized
to zero.</li>
<li><strong>device</strong> (torch.device, int, str, or None, optional):
The torch device on which to allocate tensors (e.g., 'cpu', 'cuda', or an index).<br />
If not provided, the default device defined in Torch is used.</li>
<li><strong>dtype</strong> (torch.dtype, optional):
Data type of input and output tensors (e.g., torch.float32).
If not provided, the default dtype defined in Torch is used.</li>
<li><strong>use_custom_cuda_extension_if_available</strong> (bool or None, optional):
Whether to use the custom CUDA kernel if present.
Default: Linux: True, all other systems: False</li>
<li><strong>fail_if_cuda_extension_load_fails</strong> (bool; default=False):
Whether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</li>
<li><strong>report</strong> (bool; default=False):
If True, prints a report with diagnostic information during initialization and forward computation.</li>
<li><strong>report_on_coherence_minimization</strong> (bool; default=False):
If True, prints special diagnostics during slice coherence minimization.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>If Cartesian mode is activated and <code><a href="#FSWEmbedding.encode_total_mass">encode_total_mass</a></code> is True, <code><a href="#FSWEmbedding.flatten_cartesian_axes">flatten_cartesian_axes</a></code> must be True.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FrequencyInitMethod">FrequencyInitMethod</a></code>: 
Enum for selecting frequency initialization strategies.<br />
<code><a href="#TotalMassEncodingTransformation">TotalMassEncodingTransformation</a></code>: 
Enum for total mass transformations.<br />
<code><a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></code>: 
Enum for strategies to incorporate total mass into the embedding.  </p>
</div>


                            </div>
                            <div id="FSWEmbedding.slice_vectors" class="classattr">
                                <div class="attr variable">
            <span class="name">slice_vectors</span>

        
    </div>
    <a class="headerlink" href="#FSWEmbedding.slice_vectors"></a>
    
    

                            </div>
                            <div id="FSWEmbedding.frequencies" class="classattr">
                                <div class="attr variable">
            <span class="name">frequencies</span>

        
    </div>
    <a class="headerlink" href="#FSWEmbedding.frequencies"></a>
    
    

                            </div>
                            <div id="FSWEmbedding.bias" class="classattr">
                                <div class="attr variable">
            <span class="name">bias</span>

        
    </div>
    <a class="headerlink" href="#FSWEmbedding.bias"></a>
    
    

                            </div>
                            <div id="FSWEmbedding.from_config" class="classattr">
                                        <input id="FSWEmbedding.from_config-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-classmethod">@classmethod</div>

        <span class="def">def</span>
        <span class="name">from_config</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">cls</span>, </span><span class="param"><span class="n">config</span><span class="p">:</span> <span class="nb">dict</span></span><span class="return-annotation">) -> <span class="n"><a href="#FSWEmbedding">FSWEmbedding</a></span>:</span></span>

                <label class="view-source-button" for="FSWEmbedding.from_config-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.from_config"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.from_config-568"><a href="#FSWEmbedding.from_config-568"><span class="linenos">568</span></a>    <span class="nd">@classmethod</span>
</span><span id="FSWEmbedding.from_config-569"><a href="#FSWEmbedding.from_config-569"><span class="linenos">569</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;FSWEmbedding&quot;</span><span class="p">:</span>
</span><span id="FSWEmbedding.from_config-570"><a href="#FSWEmbedding.from_config-570"><span class="linenos">570</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.from_config-571"><a href="#FSWEmbedding.from_config-571"><span class="linenos">571</span></a><span class="sd">        Construct an FSWEmbedding instance from a configuration dictionary.</span>
</span><span id="FSWEmbedding.from_config-572"><a href="#FSWEmbedding.from_config-572"><span class="linenos">572</span></a>
</span><span id="FSWEmbedding.from_config-573"><a href="#FSWEmbedding.from_config-573"><span class="linenos">573</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding.from_config-574"><a href="#FSWEmbedding.from_config-574"><span class="linenos">574</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding.from_config-575"><a href="#FSWEmbedding.from_config-575"><span class="linenos">575</span></a><span class="sd">        config : dict</span>
</span><span id="FSWEmbedding.from_config-576"><a href="#FSWEmbedding.from_config-576"><span class="linenos">576</span></a><span class="sd">            Dictionary of keyword arguments matching the `__init__` parameters.</span>
</span><span id="FSWEmbedding.from_config-577"><a href="#FSWEmbedding.from_config-577"><span class="linenos">577</span></a>
</span><span id="FSWEmbedding.from_config-578"><a href="#FSWEmbedding.from_config-578"><span class="linenos">578</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.from_config-579"><a href="#FSWEmbedding.from_config-579"><span class="linenos">579</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.from_config-580"><a href="#FSWEmbedding.from_config-580"><span class="linenos">580</span></a><span class="sd">        FSWEmbedding</span>
</span><span id="FSWEmbedding.from_config-581"><a href="#FSWEmbedding.from_config-581"><span class="linenos">581</span></a><span class="sd">            A new instance initialized with the provided configuration.</span>
</span><span id="FSWEmbedding.from_config-582"><a href="#FSWEmbedding.from_config-582"><span class="linenos">582</span></a>
</span><span id="FSWEmbedding.from_config-583"><a href="#FSWEmbedding.from_config-583"><span class="linenos">583</span></a><span class="sd">        Raises</span>
</span><span id="FSWEmbedding.from_config-584"><a href="#FSWEmbedding.from_config-584"><span class="linenos">584</span></a><span class="sd">        ------</span>
</span><span id="FSWEmbedding.from_config-585"><a href="#FSWEmbedding.from_config-585"><span class="linenos">585</span></a><span class="sd">        TypeError</span>
</span><span id="FSWEmbedding.from_config-586"><a href="#FSWEmbedding.from_config-586"><span class="linenos">586</span></a><span class="sd">            If any keys in the dictionary are not valid constructor arguments.</span>
</span><span id="FSWEmbedding.from_config-587"><a href="#FSWEmbedding.from_config-587"><span class="linenos">587</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.from_config-588"><a href="#FSWEmbedding.from_config-588"><span class="linenos">588</span></a>        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
</span><span id="FSWEmbedding.from_config-589"><a href="#FSWEmbedding.from_config-589"><span class="linenos">589</span></a>        <span class="n">valid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s1">&#39;self&#39;</span><span class="p">}</span>
</span><span id="FSWEmbedding.from_config-590"><a href="#FSWEmbedding.from_config-590"><span class="linenos">590</span></a>        <span class="n">invalid_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="o">-</span> <span class="n">valid_keys</span>
</span><span id="FSWEmbedding.from_config-591"><a href="#FSWEmbedding.from_config-591"><span class="linenos">591</span></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding.from_config-592"><a href="#FSWEmbedding.from_config-592"><span class="linenos">592</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config key: &#39;</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding.from_config-593"><a href="#FSWEmbedding.from_config-593"><span class="linenos">593</span></a>        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">invalid_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="FSWEmbedding.from_config-594"><a href="#FSWEmbedding.from_config-594"><span class="linenos">594</span></a>            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected config keys: </span><span class="si">{</span><span class="n">invalid_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding.from_config-595"><a href="#FSWEmbedding.from_config-595"><span class="linenos">595</span></a>        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Construct an FSWEmbedding instance from a configuration dictionary.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>config</strong> (dict):
Dictionary of keyword arguments matching the <code><a href="#FSWEmbedding.__init__">__init__</a></code> parameters.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>FSWEmbedding</strong>: A new instance initialized with the provided configuration.</li>
</ul>

<h6 id="raises">Raises</h6>

<ul>
<li><strong>TypeError</strong>: If any keys in the dictionary are not valid constructor arguments.</li>
</ul>
</div>


                            </div>
                            <div id="FSWEmbedding.reset_parameters" class="classattr">
                                        <input id="FSWEmbedding.reset_parameters-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">reset_parameters</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n"><a href="#FrequencyInitMethod">FrequencyInitMethod</a></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FSWEmbedding.reset_parameters-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.reset_parameters"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.reset_parameters-598"><a href="#FSWEmbedding.reset_parameters-598"><span class="linenos">598</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-599"><a href="#FSWEmbedding.reset_parameters-599"><span class="linenos">599</span></a>                         <span class="n">frequency_init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">FrequencyInitMethod</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-600"><a href="#FSWEmbedding.reset_parameters-600"><span class="linenos">600</span></a>                         <span class="n">minimize_slice_coherence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-601"><a href="#FSWEmbedding.reset_parameters-601"><span class="linenos">601</span></a>                         <span class="n">report</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-602"><a href="#FSWEmbedding.reset_parameters-602"><span class="linenos">602</span></a>                         <span class="n">report_on_coherence_minimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding.reset_parameters-603"><a href="#FSWEmbedding.reset_parameters-603"><span class="linenos">603</span></a>
</span><span id="FSWEmbedding.reset_parameters-604"><a href="#FSWEmbedding.reset_parameters-604"><span class="linenos">604</span></a>        <span class="c1"># Apply user updates for these parameters</span>
</span><span id="FSWEmbedding.reset_parameters-605"><a href="#FSWEmbedding.reset_parameters-605"><span class="linenos">605</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">frequency_init</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-606"><a href="#FSWEmbedding.reset_parameters-606"><span class="linenos">606</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-607"><a href="#FSWEmbedding.reset_parameters-607"><span class="linenos">607</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-608"><a href="#FSWEmbedding.reset_parameters-608"><span class="linenos">608</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span> <span class="o">=</span> <span class="n">ifnone</span><span class="p">(</span><span class="n">report_on_coherence_minimization</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-609"><a href="#FSWEmbedding.reset_parameters-609"><span class="linenos">609</span></a>
</span><span id="FSWEmbedding.reset_parameters-610"><a href="#FSWEmbedding.reset_parameters-610"><span class="linenos">610</span></a>        <span class="c1"># To make sure we don&#39;t use these values inside the function; if any of then is None, we must use its self. counterpart.</span>
</span><span id="FSWEmbedding.reset_parameters-611"><a href="#FSWEmbedding.reset_parameters-611"><span class="linenos">611</span></a>        <span class="k">del</span> <span class="n">minimize_slice_coherence</span><span class="p">,</span> <span class="n">frequency_init</span><span class="p">,</span> <span class="n">report</span><span class="p">,</span> <span class="n">report_on_coherence_minimization</span>
</span><span id="FSWEmbedding.reset_parameters-612"><a href="#FSWEmbedding.reset_parameters-612"><span class="linenos">612</span></a>
</span><span id="FSWEmbedding.reset_parameters-613"><a href="#FSWEmbedding.reset_parameters-613"><span class="linenos">613</span></a>        <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-614"><a href="#FSWEmbedding.reset_parameters-614"><span class="linenos">614</span></a>
</span><span id="FSWEmbedding.reset_parameters-615"><a href="#FSWEmbedding.reset_parameters-615"><span class="linenos">615</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding.reset_parameters-616"><a href="#FSWEmbedding.reset_parameters-616"><span class="linenos">616</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Generating embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-617"><a href="#FSWEmbedding.reset_parameters-617"><span class="linenos">617</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-618"><a href="#FSWEmbedding.reset_parameters-618"><span class="linenos">618</span></a>            <span class="n">qprintln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span> <span class="s1">&#39;Resetting embedding parameters:&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-619"><a href="#FSWEmbedding.reset_parameters-619"><span class="linenos">619</span></a>
</span><span id="FSWEmbedding.reset_parameters-620"><a href="#FSWEmbedding.reset_parameters-620"><span class="linenos">620</span></a>
</span><span id="FSWEmbedding.reset_parameters-621"><a href="#FSWEmbedding.reset_parameters-621"><span class="linenos">621</span></a>        <span class="c1"># If we&#39;re running for the first time, get the device and dtype that were set in the __init__ method;</span>
</span><span id="FSWEmbedding.reset_parameters-622"><a href="#FSWEmbedding.reset_parameters-622"><span class="linenos">622</span></a>        <span class="c1"># otherwise use the current device and dtype.</span>
</span><span id="FSWEmbedding.reset_parameters-623"><a href="#FSWEmbedding.reset_parameters-623"><span class="linenos">623</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding.reset_parameters-624"><a href="#FSWEmbedding.reset_parameters-624"><span class="linenos">624</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_new</span>
</span><span id="FSWEmbedding.reset_parameters-625"><a href="#FSWEmbedding.reset_parameters-625"><span class="linenos">625</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_device_new&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-626"><a href="#FSWEmbedding.reset_parameters-626"><span class="linenos">626</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-627"><a href="#FSWEmbedding.reset_parameters-627"><span class="linenos">627</span></a>            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
</span><span id="FSWEmbedding.reset_parameters-628"><a href="#FSWEmbedding.reset_parameters-628"><span class="linenos">628</span></a>
</span><span id="FSWEmbedding.reset_parameters-629"><a href="#FSWEmbedding.reset_parameters-629"><span class="linenos">629</span></a>        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">):</span>
</span><span id="FSWEmbedding.reset_parameters-630"><a href="#FSWEmbedding.reset_parameters-630"><span class="linenos">630</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_new</span>
</span><span id="FSWEmbedding.reset_parameters-631"><a href="#FSWEmbedding.reset_parameters-631"><span class="linenos">631</span></a>            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dtype_new&#39;</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-632"><a href="#FSWEmbedding.reset_parameters-632"><span class="linenos">632</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-633"><a href="#FSWEmbedding.reset_parameters-633"><span class="linenos">633</span></a>            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="FSWEmbedding.reset_parameters-634"><a href="#FSWEmbedding.reset_parameters-634"><span class="linenos">634</span></a>
</span><span id="FSWEmbedding.reset_parameters-635"><a href="#FSWEmbedding.reset_parameters-635"><span class="linenos">635</span></a>
</span><span id="FSWEmbedding.reset_parameters-636"><a href="#FSWEmbedding.reset_parameters-636"><span class="linenos">636</span></a>        <span class="n">total_mass_encoding_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span> <span class="k">else</span> <span class="mi">0</span>
</span><span id="FSWEmbedding.reset_parameters-637"><a href="#FSWEmbedding.reset_parameters-637"><span class="linenos">637</span></a>
</span><span id="FSWEmbedding.reset_parameters-638"><a href="#FSWEmbedding.reset_parameters-638"><span class="linenos">638</span></a>        <span class="c1"># Generate slice vectors and frequencies</span>
</span><span id="FSWEmbedding.reset_parameters-639"><a href="#FSWEmbedding.reset_parameters-639"><span class="linenos">639</span></a>        <span class="c1"># We always generate (and optimize) them in float64 and then convert to the desired dtype.</span>
</span><span id="FSWEmbedding.reset_parameters-640"><a href="#FSWEmbedding.reset_parameters-640"><span class="linenos">640</span></a>        <span class="n">slice_vectors</span><span class="p">,</span> <span class="n">frequencies</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_generate_embedding_parameters</span><span class="p">(</span><span class="n">d_in</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-641"><a href="#FSWEmbedding.reset_parameters-641"><span class="linenos">641</span></a>                                                                                       <span class="n">num_slices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="n">num_frequencies</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-642"><a href="#FSWEmbedding.reset_parameters-642"><span class="linenos">642</span></a>                                                                                       <span class="n">cartesian_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-643"><a href="#FSWEmbedding.reset_parameters-643"><span class="linenos">643</span></a>                                                                                       <span class="n">flatten_cartesian_axes</span> <span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-644"><a href="#FSWEmbedding.reset_parameters-644"><span class="linenos">644</span></a>                                                                                       <span class="n">total_mass_encoding_dim</span><span class="o">=</span><span class="n">total_mass_encoding_dim</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-645"><a href="#FSWEmbedding.reset_parameters-645"><span class="linenos">645</span></a>                                                                                       <span class="n">frequency_init</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_init</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-646"><a href="#FSWEmbedding.reset_parameters-646"><span class="linenos">646</span></a>                                                                                       <span class="n">minimize_slice_coherence</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_minimize_slice_coherence</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-647"><a href="#FSWEmbedding.reset_parameters-647"><span class="linenos">647</span></a>                                                                                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-648"><a href="#FSWEmbedding.reset_parameters-648"><span class="linenos">648</span></a>                                                                                       <span class="n">report</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report</span><span class="p">,</span>
</span><span id="FSWEmbedding.reset_parameters-649"><a href="#FSWEmbedding.reset_parameters-649"><span class="linenos">649</span></a>                                                                                       <span class="n">report_on_coherence_minimization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_on_coherence_minimization</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-650"><a href="#FSWEmbedding.reset_parameters-650"><span class="linenos">650</span></a>
</span><span id="FSWEmbedding.reset_parameters-651"><a href="#FSWEmbedding.reset_parameters-651"><span class="linenos">651</span></a>        <span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">slice_vectors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-652"><a href="#FSWEmbedding.reset_parameters-652"><span class="linenos">652</span></a>        <span class="n">frequencies</span> <span class="o">=</span> <span class="n">frequencies</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-653"><a href="#FSWEmbedding.reset_parameters-653"><span class="linenos">653</span></a>
</span><span id="FSWEmbedding.reset_parameters-654"><a href="#FSWEmbedding.reset_parameters-654"><span class="linenos">654</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-655"><a href="#FSWEmbedding.reset_parameters-655"><span class="linenos">655</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-656"><a href="#FSWEmbedding.reset_parameters-656"><span class="linenos">656</span></a>
</span><span id="FSWEmbedding.reset_parameters-657"><a href="#FSWEmbedding.reset_parameters-657"><span class="linenos">657</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-658"><a href="#FSWEmbedding.reset_parameters-658"><span class="linenos">658</span></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-659"><a href="#FSWEmbedding.reset_parameters-659"><span class="linenos">659</span></a>
</span><span id="FSWEmbedding.reset_parameters-660"><a href="#FSWEmbedding.reset_parameters-660"><span class="linenos">660</span></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-661"><a href="#FSWEmbedding.reset_parameters-661"><span class="linenos">661</span></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">))</span>
</span><span id="FSWEmbedding.reset_parameters-662"><a href="#FSWEmbedding.reset_parameters-662"><span class="linenos">662</span></a>
</span><span id="FSWEmbedding.reset_parameters-663"><a href="#FSWEmbedding.reset_parameters-663"><span class="linenos">663</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-664"><a href="#FSWEmbedding.reset_parameters-664"><span class="linenos">664</span></a>
</span><span id="FSWEmbedding.reset_parameters-665"><a href="#FSWEmbedding.reset_parameters-665"><span class="linenos">665</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.reset_parameters-666"><a href="#FSWEmbedding.reset_parameters-666"><span class="linenos">666</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.reset_parameters-667"><a href="#FSWEmbedding.reset_parameters-667"><span class="linenos">667</span></a>
</span><span id="FSWEmbedding.reset_parameters-668"><a href="#FSWEmbedding.reset_parameters-668"><span class="linenos">668</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="FSWEmbedding.reset_parameters-669"><a href="#FSWEmbedding.reset_parameters-669"><span class="linenos">669</span></a>
</span><span id="FSWEmbedding.reset_parameters-670"><a href="#FSWEmbedding.reset_parameters-670"><span class="linenos">670</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span></pre></div>


    

                            </div>
                            <div id="FSWEmbedding.to" class="classattr">
                                        <input id="FSWEmbedding.to-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">to</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="o">*</span><span class="n">args</span>, </span><span class="param"><span class="o">**</span><span class="n">kwargs</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FSWEmbedding.to-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.to"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.to-674"><a href="#FSWEmbedding.to-674"><span class="linenos">674</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="FSWEmbedding.to-675"><a href="#FSWEmbedding.to-675"><span class="linenos">675</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the module to the specified device or dtype.</span>
</span><span id="FSWEmbedding.to-676"><a href="#FSWEmbedding.to-676"><span class="linenos">676</span></a>
</span><span id="FSWEmbedding.to-677"><a href="#FSWEmbedding.to-677"><span class="linenos">677</span></a><span class="sd">        Example:</span>
</span><span id="FSWEmbedding.to-678"><a href="#FSWEmbedding.to-678"><span class="linenos">678</span></a>
</span><span id="FSWEmbedding.to-679"><a href="#FSWEmbedding.to-679"><span class="linenos">679</span></a><span class="sd">            module.to(torch.float32)</span>
</span><span id="FSWEmbedding.to-680"><a href="#FSWEmbedding.to-680"><span class="linenos">680</span></a><span class="sd">            module.to(device=&#39;cuda&#39;)</span>
</span><span id="FSWEmbedding.to-681"><a href="#FSWEmbedding.to-681"><span class="linenos">681</span></a>
</span><span id="FSWEmbedding.to-682"><a href="#FSWEmbedding.to-682"><span class="linenos">682</span></a><span class="sd">        See also: torch.nn.Module.to()</span>
</span><span id="FSWEmbedding.to-683"><a href="#FSWEmbedding.to-683"><span class="linenos">683</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.to-684"><a href="#FSWEmbedding.to-684"><span class="linenos">684</span></a>        <span class="k">if</span> <span class="s1">&#39;dtype&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
</span><span id="FSWEmbedding.to-685"><a href="#FSWEmbedding.to-685"><span class="linenos">685</span></a>            <span class="n">arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
</span><span id="FSWEmbedding.to-686"><a href="#FSWEmbedding.to-686"><span class="linenos">686</span></a>
</span><span id="FSWEmbedding.to-687"><a href="#FSWEmbedding.to-687"><span class="linenos">687</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="s1">&#39;invalid input type </span><span class="si">%s</span><span class="s1"> at argument &#39;&#39;dtype&#39;&#39;&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
</span><span id="FSWEmbedding.to-688"><a href="#FSWEmbedding.to-688"><span class="linenos">688</span></a>            <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="FSWEmbedding.to-689"><a href="#FSWEmbedding.to-689"><span class="linenos">689</span></a>
</span><span id="FSWEmbedding.to-690"><a href="#FSWEmbedding.to-690"><span class="linenos">690</span></a>        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
</span><span id="FSWEmbedding.to-691"><a href="#FSWEmbedding.to-691"><span class="linenos">691</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
</span><span id="FSWEmbedding.to-692"><a href="#FSWEmbedding.to-692"><span class="linenos">692</span></a>                <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">arg</span><span class="o">.</span><span class="n">is_complex</span><span class="p">,</span> <span class="s1">&#39;dtype must be real floating-point; instead got dtype=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">arg</span>
</span><span id="FSWEmbedding.to-693"><a href="#FSWEmbedding.to-693"><span class="linenos">693</span></a>
</span><span id="FSWEmbedding.to-694"><a href="#FSWEmbedding.to-694"><span class="linenos">694</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="FSWEmbedding.to-695"><a href="#FSWEmbedding.to-695"><span class="linenos">695</span></a>
</span><span id="FSWEmbedding.to-696"><a href="#FSWEmbedding.to-696"><span class="linenos">696</span></a>        <span class="k">return</span> <span class="bp">self</span>
</span></pre></div>


            <div class="docstring"><p>Moves the module to the specified device or dtype.</p>

<p>Example:</p>

<pre><code>module.to(torch.float32)
module.to(device='cuda')
</code></pre>

<p>See also: torch.nn.Module.to()</p>
</div>


                            </div>
                            <div id="FSWEmbedding.num_slices" class="classattr">
                                        <input id="FSWEmbedding.num_slices-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">num_slices</span><span class="annotation">: int</span>

                <label class="view-source-button" for="FSWEmbedding.num_slices-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.num_slices"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.num_slices-699"><a href="#FSWEmbedding.num_slices-699"><span class="linenos">699</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.num_slices-700"><a href="#FSWEmbedding.num_slices-700"><span class="linenos">700</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding.num_slices-701"><a href="#FSWEmbedding.num_slices-701"><span class="linenos">701</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of slices used in the embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.num_slices-702"><a href="#FSWEmbedding.num_slices-702"><span class="linenos">702</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span>
</span></pre></div>


            <div class="docstring"><p>Number of slices used in the embedding.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.num_frequencies" class="classattr">
                                        <input id="FSWEmbedding.num_frequencies-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">num_frequencies</span><span class="annotation">: int</span>

                <label class="view-source-button" for="FSWEmbedding.num_frequencies-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.num_frequencies"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.num_frequencies-704"><a href="#FSWEmbedding.num_frequencies-704"><span class="linenos">704</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.num_frequencies-705"><a href="#FSWEmbedding.num_frequencies-705"><span class="linenos">705</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">num_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding.num_frequencies-706"><a href="#FSWEmbedding.num_frequencies-706"><span class="linenos">706</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.num_frequencies-707"><a href="#FSWEmbedding.num_frequencies-707"><span class="linenos">707</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span>
</span></pre></div>


            <div class="docstring"><p>Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.cartesian_mode" class="classattr">
                                        <input id="FSWEmbedding.cartesian_mode-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">cartesian_mode</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.cartesian_mode-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.cartesian_mode"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.cartesian_mode-709"><a href="#FSWEmbedding.cartesian_mode-709"><span class="linenos">709</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.cartesian_mode-710"><a href="#FSWEmbedding.cartesian_mode-710"><span class="linenos">710</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">cartesian_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.cartesian_mode-711"><a href="#FSWEmbedding.cartesian_mode-711"><span class="linenos">711</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices</span>
</span><span id="FSWEmbedding.cartesian_mode-712"><a href="#FSWEmbedding.cartesian_mode-712"><span class="linenos">712</span></a><span class="sd">        and frequencies.</span>
</span><span id="FSWEmbedding.cartesian_mode-713"><a href="#FSWEmbedding.cartesian_mode-713"><span class="linenos">713</span></a><span class="sd">        In Cartesian mode, the embeding dimension is `d_out` = `num_slices × num_frequencies`.</span>
</span><span id="FSWEmbedding.cartesian_mode-714"><a href="#FSWEmbedding.cartesian_mode-714"><span class="linenos">714</span></a><span class="sd">        Cartesian mode is activated by providing `num_slices` and `num_frequencies` to `FSWEmbedding.__init__`bool</span>
</span><span id="FSWEmbedding.cartesian_mode-715"><a href="#FSWEmbedding.cartesian_mode-715"><span class="linenos">715</span></a><span class="sd">        See also: `flatten_cartesian_axes`&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.cartesian_mode-716"><a href="#FSWEmbedding.cartesian_mode-716"><span class="linenos">716</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span>
</span></pre></div>


            <div class="docstring"><p>If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices
and frequencies.
In Cartesian mode, the embeding dimension is <code><a href="#FSWEmbedding.d_out">d_out</a></code> = <code>num_slices × num_frequencies</code>.
Cartesian mode is activated by providing <code><a href="#FSWEmbedding.num_slices">num_slices</a></code> and <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code> to <code><a href="#FSWEmbedding.__init__">FSWEmbedding.__init__</a></code>bool
See also: <code><a href="#FSWEmbedding.flatten_cartesian_axes">flatten_cartesian_axes</a></code></p>
</div>


                            </div>
                            <div id="FSWEmbedding.flatten_cartesian_axes" class="classattr">
                                        <input id="FSWEmbedding.flatten_cartesian_axes-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">flatten_cartesian_axes</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.flatten_cartesian_axes-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.flatten_cartesian_axes"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.flatten_cartesian_axes-718"><a href="#FSWEmbedding.flatten_cartesian_axes-718"><span class="linenos">718</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-719"><a href="#FSWEmbedding.flatten_cartesian_axes-719"><span class="linenos">719</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">flatten_cartesian_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-720"><a href="#FSWEmbedding.flatten_cartesian_axes-720"><span class="linenos">720</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-721"><a href="#FSWEmbedding.flatten_cartesian_axes-721"><span class="linenos">721</span></a><span class="sd">        If True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (`num_slices`, `num_frequencies`).</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-722"><a href="#FSWEmbedding.flatten_cartesian_axes-722"><span class="linenos">722</span></a><span class="sd">        If False, the otput is shaped `num_slices` × `num_frequencies`.</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-723"><a href="#FSWEmbedding.flatten_cartesian_axes-723"><span class="linenos">723</span></a><span class="sd">        This setting is only relevant if `cartesian_mode` is True.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.flatten_cartesian_axes-724"><a href="#FSWEmbedding.flatten_cartesian_axes-724"><span class="linenos">724</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span>
</span></pre></div>


            <div class="docstring"><p>In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.
If True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (<code><a href="#FSWEmbedding.num_slices">num_slices</a></code>, <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code>).
If False, the otput is shaped <code><a href="#FSWEmbedding.num_slices">num_slices</a></code> × <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code>.
This setting is only relevant if <code><a href="#FSWEmbedding.cartesian_mode">cartesian_mode</a></code> is True.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.learnable_slices" class="classattr">
                                        <input id="FSWEmbedding.learnable_slices-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">learnable_slices</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.learnable_slices-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.learnable_slices"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.learnable_slices-726"><a href="#FSWEmbedding.learnable_slices-726"><span class="linenos">726</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.learnable_slices-727"><a href="#FSWEmbedding.learnable_slices-727"><span class="linenos">727</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_slices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.learnable_slices-728"><a href="#FSWEmbedding.learnable_slices-728"><span class="linenos">728</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether slice directions are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.learnable_slices-729"><a href="#FSWEmbedding.learnable_slices-729"><span class="linenos">729</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span>
</span></pre></div>


            <div class="docstring"><p>Whether slice directions are learnable parameters.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.learnable_frequencies" class="classattr">
                                        <input id="FSWEmbedding.learnable_frequencies-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">learnable_frequencies</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.learnable_frequencies-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.learnable_frequencies"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.learnable_frequencies-731"><a href="#FSWEmbedding.learnable_frequencies-731"><span class="linenos">731</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.learnable_frequencies-732"><a href="#FSWEmbedding.learnable_frequencies-732"><span class="linenos">732</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.learnable_frequencies-733"><a href="#FSWEmbedding.learnable_frequencies-733"><span class="linenos">733</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether frequency values are learnable parameters.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.learnable_frequencies-734"><a href="#FSWEmbedding.learnable_frequencies-734"><span class="linenos">734</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span>
</span></pre></div>


            <div class="docstring"><p>Whether frequency values are learnable parameters.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.enable_bias" class="classattr">
                                        <input id="FSWEmbedding.enable_bias-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">enable_bias</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.enable_bias-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.enable_bias"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.enable_bias-736"><a href="#FSWEmbedding.enable_bias-736"><span class="linenos">736</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.enable_bias-737"><a href="#FSWEmbedding.enable_bias-737"><span class="linenos">737</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">enable_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.enable_bias-738"><a href="#FSWEmbedding.enable_bias-738"><span class="linenos">738</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether a learnable bias vector is added to the output embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.enable_bias-739"><a href="#FSWEmbedding.enable_bias-739"><span class="linenos">739</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span>
</span></pre></div>


            <div class="docstring"><p>Whether a learnable bias vector is added to the output embedding.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.encode_total_mass" class="classattr">
                                        <input id="FSWEmbedding.encode_total_mass-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">encode_total_mass</span><span class="annotation">: bool</span>

                <label class="view-source-button" for="FSWEmbedding.encode_total_mass-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.encode_total_mass"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.encode_total_mass-741"><a href="#FSWEmbedding.encode_total_mass-741"><span class="linenos">741</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.encode_total_mass-742"><a href="#FSWEmbedding.encode_total_mass-742"><span class="linenos">742</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">encode_total_mass</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span id="FSWEmbedding.encode_total_mass-743"><a href="#FSWEmbedding.encode_total_mass-743"><span class="linenos">743</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the total mass of the input measure is encoded into the embedding.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.encode_total_mass-744"><a href="#FSWEmbedding.encode_total_mass-744"><span class="linenos">744</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span>
</span></pre></div>


            <div class="docstring"><p>Whether the total mass of the input measure is encoded into the embedding.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.total_mass_encoding_transformation" class="classattr">
                                        <input id="FSWEmbedding.total_mass_encoding_transformation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">total_mass_encoding_transformation</span><span class="annotation">: <a href="#TotalMassEncodingTransformation">TotalMassEncodingTransformation</a></span>

                <label class="view-source-button" for="FSWEmbedding.total_mass_encoding_transformation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.total_mass_encoding_transformation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.total_mass_encoding_transformation-746"><a href="#FSWEmbedding.total_mass_encoding_transformation-746"><span class="linenos">746</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.total_mass_encoding_transformation-747"><a href="#FSWEmbedding.total_mass_encoding_transformation-747"><span class="linenos">747</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_transformation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingTransformation</span><span class="p">:</span>
</span><span id="FSWEmbedding.total_mass_encoding_transformation-748"><a href="#FSWEmbedding.total_mass_encoding_transformation-748"><span class="linenos">748</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Function applied to the total mass before it is stored.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.total_mass_encoding_transformation-749"><a href="#FSWEmbedding.total_mass_encoding_transformation-749"><span class="linenos">749</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span>
</span></pre></div>


            <div class="docstring"><p>Function applied to the total mass before it is stored.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.total_mass_encoding_method" class="classattr">
                                        <input id="FSWEmbedding.total_mass_encoding_method-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">total_mass_encoding_method</span><span class="annotation">: <a href="#TotalMassEncodingMethod">TotalMassEncodingMethod</a></span>

                <label class="view-source-button" for="FSWEmbedding.total_mass_encoding_method-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.total_mass_encoding_method"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.total_mass_encoding_method-751"><a href="#FSWEmbedding.total_mass_encoding_method-751"><span class="linenos">751</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.total_mass_encoding_method-752"><a href="#FSWEmbedding.total_mass_encoding_method-752"><span class="linenos">752</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_method</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TotalMassEncodingMethod</span><span class="p">:</span>
</span><span id="FSWEmbedding.total_mass_encoding_method-753"><a href="#FSWEmbedding.total_mass_encoding_method-753"><span class="linenos">753</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Strategy used to incorporate total mass into the final embedding vector.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.total_mass_encoding_method-754"><a href="#FSWEmbedding.total_mass_encoding_method-754"><span class="linenos">754</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span>
</span></pre></div>


            <div class="docstring"><p>Strategy used to incorporate total mass into the final embedding vector.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.total_mass_encoding_scale" class="classattr">
                                        <input id="FSWEmbedding.total_mass_encoding_scale-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">total_mass_encoding_scale</span><span class="annotation">: float</span>

                <label class="view-source-button" for="FSWEmbedding.total_mass_encoding_scale-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.total_mass_encoding_scale"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.total_mass_encoding_scale-756"><a href="#FSWEmbedding.total_mass_encoding_scale-756"><span class="linenos">756</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.total_mass_encoding_scale-757"><a href="#FSWEmbedding.total_mass_encoding_scale-757"><span class="linenos">757</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_encoding_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="FSWEmbedding.total_mass_encoding_scale-758"><a href="#FSWEmbedding.total_mass_encoding_scale-758"><span class="linenos">758</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;The encoded total mass is scaled by this factor.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.total_mass_encoding_scale-759"><a href="#FSWEmbedding.total_mass_encoding_scale-759"><span class="linenos">759</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span></pre></div>


            <div class="docstring"><p>The encoded total mass is scaled by this factor.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.total_mass_padding_thresh" class="classattr">
                                        <input id="FSWEmbedding.total_mass_padding_thresh-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">total_mass_padding_thresh</span><span class="annotation">: float</span>

                <label class="view-source-button" for="FSWEmbedding.total_mass_padding_thresh-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.total_mass_padding_thresh"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.total_mass_padding_thresh-761"><a href="#FSWEmbedding.total_mass_padding_thresh-761"><span class="linenos">761</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.total_mass_padding_thresh-762"><a href="#FSWEmbedding.total_mass_padding_thresh-762"><span class="linenos">762</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">total_mass_padding_thresh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="FSWEmbedding.total_mass_padding_thresh-763"><a href="#FSWEmbedding.total_mass_padding_thresh-763"><span class="linenos">763</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Minimum total mass threshold; inputs below this value are padded to reach it.&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.total_mass_padding_thresh-764"><a href="#FSWEmbedding.total_mass_padding_thresh-764"><span class="linenos">764</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span>
</span></pre></div>


            <div class="docstring"><p>Minimum total mass threshold; inputs below this value are padded to reach it.</p>
</div>


                            </div>
                            <div id="FSWEmbedding.d_in" class="classattr">
                                        <input id="FSWEmbedding.d_in-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">d_in</span><span class="annotation">: int</span>

                <label class="view-source-button" for="FSWEmbedding.d_in-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.d_in"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.d_in-767"><a href="#FSWEmbedding.d_in-767"><span class="linenos">767</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.d_in-768"><a href="#FSWEmbedding.d_in-768"><span class="linenos">768</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_in</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding.d_in-769"><a href="#FSWEmbedding.d_in-769"><span class="linenos">769</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Ambient dimension of the input elements.</span>
</span><span id="FSWEmbedding.d_in-770"><a href="#FSWEmbedding.d_in-770"><span class="linenos">770</span></a>
</span><span id="FSWEmbedding.d_in-771"><a href="#FSWEmbedding.d_in-771"><span class="linenos">771</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.d_in-772"><a href="#FSWEmbedding.d_in-772"><span class="linenos">772</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.d_in-773"><a href="#FSWEmbedding.d_in-773"><span class="linenos">773</span></a><span class="sd">        int</span>
</span><span id="FSWEmbedding.d_in-774"><a href="#FSWEmbedding.d_in-774"><span class="linenos">774</span></a><span class="sd">            The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</span>
</span><span id="FSWEmbedding.d_in-775"><a href="#FSWEmbedding.d_in-775"><span class="linenos">775</span></a>
</span><span id="FSWEmbedding.d_in-776"><a href="#FSWEmbedding.d_in-776"><span class="linenos">776</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.d_in-777"><a href="#FSWEmbedding.d_in-777"><span class="linenos">777</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.d_in-778"><a href="#FSWEmbedding.d_in-778"><span class="linenos">778</span></a><span class="sd">        This value is set at initialization and determines the expected feature dimension of input points.</span>
</span><span id="FSWEmbedding.d_in-779"><a href="#FSWEmbedding.d_in-779"><span class="linenos">779</span></a>
</span><span id="FSWEmbedding.d_in-780"><a href="#FSWEmbedding.d_in-780"><span class="linenos">780</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.d_in-781"><a href="#FSWEmbedding.d_in-781"><span class="linenos">781</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.d_in-782"><a href="#FSWEmbedding.d_in-782"><span class="linenos">782</span></a><span class="sd">        __init__ : The `d_in` argument specifies this value at initialization.</span>
</span><span id="FSWEmbedding.d_in-783"><a href="#FSWEmbedding.d_in-783"><span class="linenos">783</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.d_in-784"><a href="#FSWEmbedding.d_in-784"><span class="linenos">784</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span>
</span></pre></div>


            <div class="docstring"><p>int: Ambient dimension of the input elements.</p>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>int</strong>: The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</li>
</ul>

<h6 id="notes">Notes</h6>

<p>This value is set at initialization and determines the expected feature dimension of input points.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">__init__</a></code>:  The <code><a href="#FSWEmbedding.d_in">d_in</a></code> argument specifies this value at initialization.  </p>
</div>


                            </div>
                            <div id="FSWEmbedding.d_out" class="classattr">
                                        <input id="FSWEmbedding.d_out-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">d_out</span><span class="annotation">: int</span>

                <label class="view-source-button" for="FSWEmbedding.d_out-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.d_out"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.d_out-787"><a href="#FSWEmbedding.d_out-787"><span class="linenos">787</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.d_out-788"><a href="#FSWEmbedding.d_out-788"><span class="linenos">788</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">d_out</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="FSWEmbedding.d_out-789"><a href="#FSWEmbedding.d_out-789"><span class="linenos">789</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;int: Dimensionality of the embedding output.</span>
</span><span id="FSWEmbedding.d_out-790"><a href="#FSWEmbedding.d_out-790"><span class="linenos">790</span></a>
</span><span id="FSWEmbedding.d_out-791"><a href="#FSWEmbedding.d_out-791"><span class="linenos">791</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.d_out-792"><a href="#FSWEmbedding.d_out-792"><span class="linenos">792</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.d_out-793"><a href="#FSWEmbedding.d_out-793"><span class="linenos">793</span></a><span class="sd">        int</span>
</span><span id="FSWEmbedding.d_out-794"><a href="#FSWEmbedding.d_out-794"><span class="linenos">794</span></a><span class="sd">            The dimension of the vector produced by the embedding for each multiset or distribution.</span>
</span><span id="FSWEmbedding.d_out-795"><a href="#FSWEmbedding.d_out-795"><span class="linenos">795</span></a>
</span><span id="FSWEmbedding.d_out-796"><a href="#FSWEmbedding.d_out-796"><span class="linenos">796</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.d_out-797"><a href="#FSWEmbedding.d_out-797"><span class="linenos">797</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.d_out-798"><a href="#FSWEmbedding.d_out-798"><span class="linenos">798</span></a><span class="sd">        This value is set at initialization and governs the size of the embedding output.</span>
</span><span id="FSWEmbedding.d_out-799"><a href="#FSWEmbedding.d_out-799"><span class="linenos">799</span></a>
</span><span id="FSWEmbedding.d_out-800"><a href="#FSWEmbedding.d_out-800"><span class="linenos">800</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.d_out-801"><a href="#FSWEmbedding.d_out-801"><span class="linenos">801</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.d_out-802"><a href="#FSWEmbedding.d_out-802"><span class="linenos">802</span></a><span class="sd">        __init__ : The `d_out` argument specifies this value at initialization.</span>
</span><span id="FSWEmbedding.d_out-803"><a href="#FSWEmbedding.d_out-803"><span class="linenos">803</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.d_out-804"><a href="#FSWEmbedding.d_out-804"><span class="linenos">804</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span>
</span></pre></div>


            <div class="docstring"><p>int: Dimensionality of the embedding output.</p>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>int</strong>: The dimension of the vector produced by the embedding for each multiset or distribution.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>This value is set at initialization and governs the size of the embedding output.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">__init__</a></code>:  The <code><a href="#FSWEmbedding.d_out">d_out</a></code> argument specifies this value at initialization.  </p>
</div>


                            </div>
                            <div id="FSWEmbedding.device" class="classattr">
                                        <input id="FSWEmbedding.device-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">device</span>

                <label class="view-source-button" for="FSWEmbedding.device-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.device"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.device-806"><a href="#FSWEmbedding.device-806"><span class="linenos">806</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.device-807"><a href="#FSWEmbedding.device-807"><span class="linenos">807</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FSWEmbedding.device-808"><a href="#FSWEmbedding.device-808"><span class="linenos">808</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.device: The device on which the module&#39;s parameters and buffers are stored.</span>
</span><span id="FSWEmbedding.device-809"><a href="#FSWEmbedding.device-809"><span class="linenos">809</span></a>
</span><span id="FSWEmbedding.device-810"><a href="#FSWEmbedding.device-810"><span class="linenos">810</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.device-811"><a href="#FSWEmbedding.device-811"><span class="linenos">811</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.device-812"><a href="#FSWEmbedding.device-812"><span class="linenos">812</span></a><span class="sd">        torch.device</span>
</span><span id="FSWEmbedding.device-813"><a href="#FSWEmbedding.device-813"><span class="linenos">813</span></a><span class="sd">            The PyTorch device (`&#39;cpu&#39;`, `&#39;cuda&#39;`, etc.) where the embedding computations will take place.</span>
</span><span id="FSWEmbedding.device-814"><a href="#FSWEmbedding.device-814"><span class="linenos">814</span></a>
</span><span id="FSWEmbedding.device-815"><a href="#FSWEmbedding.device-815"><span class="linenos">815</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.device-816"><a href="#FSWEmbedding.device-816"><span class="linenos">816</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.device-817"><a href="#FSWEmbedding.device-817"><span class="linenos">817</span></a><span class="sd">        This behaves like the `device` property in standard PyTorch modules.</span>
</span><span id="FSWEmbedding.device-818"><a href="#FSWEmbedding.device-818"><span class="linenos">818</span></a>
</span><span id="FSWEmbedding.device-819"><a href="#FSWEmbedding.device-819"><span class="linenos">819</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.device-820"><a href="#FSWEmbedding.device-820"><span class="linenos">820</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.device-821"><a href="#FSWEmbedding.device-821"><span class="linenos">821</span></a><span class="sd">        __init__ : The `device` can be specified at initialization.</span>
</span><span id="FSWEmbedding.device-822"><a href="#FSWEmbedding.device-822"><span class="linenos">822</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.device-823"><a href="#FSWEmbedding.device-823"><span class="linenos">823</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">device</span>
</span></pre></div>


            <div class="docstring"><p>torch.device: The device on which the module's parameters and buffers are stored.</p>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.device</strong>: The PyTorch device (<code>'cpu'</code>, <code>'cuda'</code>, etc.) where the embedding computations will take place.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>This behaves like the <code><a href="#FSWEmbedding.device">device</a></code> property in standard PyTorch modules.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">__init__</a></code>:  The <code><a href="#FSWEmbedding.device">device</a></code> can be specified at initialization.  </p>
</div>


                            </div>
                            <div id="FSWEmbedding.dtype" class="classattr">
                                        <input id="FSWEmbedding.dtype-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr variable">
            <span class="name">dtype</span>

                <label class="view-source-button" for="FSWEmbedding.dtype-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.dtype"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.dtype-826"><a href="#FSWEmbedding.dtype-826"><span class="linenos">826</span></a>    <span class="nd">@property</span>
</span><span id="FSWEmbedding.dtype-827"><a href="#FSWEmbedding.dtype-827"><span class="linenos">827</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FSWEmbedding.dtype-828"><a href="#FSWEmbedding.dtype-828"><span class="linenos">828</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;torch.dtype: The default data type used by the module.</span>
</span><span id="FSWEmbedding.dtype-829"><a href="#FSWEmbedding.dtype-829"><span class="linenos">829</span></a>
</span><span id="FSWEmbedding.dtype-830"><a href="#FSWEmbedding.dtype-830"><span class="linenos">830</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.dtype-831"><a href="#FSWEmbedding.dtype-831"><span class="linenos">831</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.dtype-832"><a href="#FSWEmbedding.dtype-832"><span class="linenos">832</span></a><span class="sd">        torch.dtype</span>
</span><span id="FSWEmbedding.dtype-833"><a href="#FSWEmbedding.dtype-833"><span class="linenos">833</span></a><span class="sd">            The data type (`torch.float32`, `torch.float64`, etc.) of the module’s parameters and buffers.</span>
</span><span id="FSWEmbedding.dtype-834"><a href="#FSWEmbedding.dtype-834"><span class="linenos">834</span></a>
</span><span id="FSWEmbedding.dtype-835"><a href="#FSWEmbedding.dtype-835"><span class="linenos">835</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.dtype-836"><a href="#FSWEmbedding.dtype-836"><span class="linenos">836</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.dtype-837"><a href="#FSWEmbedding.dtype-837"><span class="linenos">837</span></a><span class="sd">        This behaves like the `dtype` property in standard PyTorch modules.</span>
</span><span id="FSWEmbedding.dtype-838"><a href="#FSWEmbedding.dtype-838"><span class="linenos">838</span></a>
</span><span id="FSWEmbedding.dtype-839"><a href="#FSWEmbedding.dtype-839"><span class="linenos">839</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.dtype-840"><a href="#FSWEmbedding.dtype-840"><span class="linenos">840</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.dtype-841"><a href="#FSWEmbedding.dtype-841"><span class="linenos">841</span></a><span class="sd">        __init__ : The `dtype` can be specified at initialization.</span>
</span><span id="FSWEmbedding.dtype-842"><a href="#FSWEmbedding.dtype-842"><span class="linenos">842</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.dtype-843"><a href="#FSWEmbedding.dtype-843"><span class="linenos">843</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="o">.</span><span class="n">dtype</span>
</span></pre></div>


            <div class="docstring"><p>torch.dtype: The default data type used by the module.</p>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.dtype</strong>: The data type (<code>torch.float32</code>, <code>torch.float64</code>, etc.) of the module’s parameters and buffers.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>This behaves like the <code><a href="#FSWEmbedding.dtype">dtype</a></code> property in standard PyTorch modules.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">__init__</a></code>:  The <code><a href="#FSWEmbedding.dtype">dtype</a></code> can be specified at initialization.  </p>
</div>


                            </div>
                            <div id="FSWEmbedding.forward" class="classattr">
                                        <input id="FSWEmbedding.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">W</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;unit&#39;</span>,</span><span class="param">	<span class="n">X_edge</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">graph_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">max_parallel_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FSWEmbedding.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FSWEmbedding.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FSWEmbedding.forward-994"><a href="#FSWEmbedding.forward-994"><span class="linenos"> 994</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-995"><a href="#FSWEmbedding.forward-995"><span class="linenos"> 995</span></a>                <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-996"><a href="#FSWEmbedding.forward-996"><span class="linenos"> 996</span></a>                <span class="n">W</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="s1">&#39;unit&#39;</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-997"><a href="#FSWEmbedding.forward-997"><span class="linenos"> 997</span></a>                <span class="n">X_edge</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-998"><a href="#FSWEmbedding.forward-998"><span class="linenos"> 998</span></a>                <span class="n">graph_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-999"><a href="#FSWEmbedding.forward-999"><span class="linenos"> 999</span></a>                <span class="n">max_parallel_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="FSWEmbedding.forward-1000"><a href="#FSWEmbedding.forward-1000"><span class="linenos">1000</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.forward-1001"><a href="#FSWEmbedding.forward-1001"><span class="linenos">1001</span></a><span class="sd">        Compute the FSW embedding of an input multiset, measure, or graph.</span>
</span><span id="FSWEmbedding.forward-1002"><a href="#FSWEmbedding.forward-1002"><span class="linenos">1002</span></a>
</span><span id="FSWEmbedding.forward-1003"><a href="#FSWEmbedding.forward-1003"><span class="linenos">1003</span></a><span class="sd">        This method maps input sets of vectors (optionally weighted) to vectors in ℝ^{d_out}</span>
</span><span id="FSWEmbedding.forward-1004"><a href="#FSWEmbedding.forward-1004"><span class="linenos">1004</span></a><span class="sd">        using the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and</span>
</span><span id="FSWEmbedding.forward-1005"><a href="#FSWEmbedding.forward-1005"><span class="linenos">1005</span></a><span class="sd">        graph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</span>
</span><span id="FSWEmbedding.forward-1006"><a href="#FSWEmbedding.forward-1006"><span class="linenos">1006</span></a>
</span><span id="FSWEmbedding.forward-1007"><a href="#FSWEmbedding.forward-1007"><span class="linenos">1007</span></a><span class="sd">        Parameters</span>
</span><span id="FSWEmbedding.forward-1008"><a href="#FSWEmbedding.forward-1008"><span class="linenos">1008</span></a><span class="sd">        ----------</span>
</span><span id="FSWEmbedding.forward-1009"><a href="#FSWEmbedding.forward-1009"><span class="linenos">1009</span></a><span class="sd">        X : torch.Tensor</span>
</span><span id="FSWEmbedding.forward-1010"><a href="#FSWEmbedding.forward-1010"><span class="linenos">1010</span></a><span class="sd">            Input tensor of shape `(n, d_in)` or `(..., n, d_in)` for batched input.</span>
</span><span id="FSWEmbedding.forward-1011"><a href="#FSWEmbedding.forward-1011"><span class="linenos">1011</span></a><span class="sd">        W : torch.Tensor or {&#39;unit&#39;, &#39;uniform&#39;}, default=&#39;unit&#39;</span>
</span><span id="FSWEmbedding.forward-1012"><a href="#FSWEmbedding.forward-1012"><span class="linenos">1012</span></a><span class="sd">            Weights tensor of shape `(n,)` or `(..., n)` corresponding to point importance.</span>
</span><span id="FSWEmbedding.forward-1013"><a href="#FSWEmbedding.forward-1013"><span class="linenos">1013</span></a><span class="sd">            If set to `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights of `1/n` are assumed.</span>
</span><span id="FSWEmbedding.forward-1014"><a href="#FSWEmbedding.forward-1014"><span class="linenos">1014</span></a><span class="sd">        X_edge : torch.Tensor, optional</span>
</span><span id="FSWEmbedding.forward-1015"><a href="#FSWEmbedding.forward-1015"><span class="linenos">1015</span></a><span class="sd">            Optional edge feature tensor. Required if `d_edge &gt; 0` was set at initialization.</span>
</span><span id="FSWEmbedding.forward-1016"><a href="#FSWEmbedding.forward-1016"><span class="linenos">1016</span></a><span class="sd">        graph_mode : bool, default=False</span>
</span><span id="FSWEmbedding.forward-1017"><a href="#FSWEmbedding.forward-1017"><span class="linenos">1017</span></a><span class="sd">            If True, interprets `W` as an adjacency matrix and computes a neighbor-aggregated</span>
</span><span id="FSWEmbedding.forward-1018"><a href="#FSWEmbedding.forward-1018"><span class="linenos">1018</span></a><span class="sd">            embedding.</span>
</span><span id="FSWEmbedding.forward-1019"><a href="#FSWEmbedding.forward-1019"><span class="linenos">1019</span></a><span class="sd">        max_parallel_slices : int, optional</span>
</span><span id="FSWEmbedding.forward-1020"><a href="#FSWEmbedding.forward-1020"><span class="linenos">1020</span></a><span class="sd">            Limits the number of slices processed in parallel. Reduces memory usage by computing</span>
</span><span id="FSWEmbedding.forward-1021"><a href="#FSWEmbedding.forward-1021"><span class="linenos">1021</span></a><span class="sd">            the embedding in smaller blocks without changing the result.</span>
</span><span id="FSWEmbedding.forward-1022"><a href="#FSWEmbedding.forward-1022"><span class="linenos">1022</span></a>
</span><span id="FSWEmbedding.forward-1023"><a href="#FSWEmbedding.forward-1023"><span class="linenos">1023</span></a><span class="sd">        Returns</span>
</span><span id="FSWEmbedding.forward-1024"><a href="#FSWEmbedding.forward-1024"><span class="linenos">1024</span></a><span class="sd">        -------</span>
</span><span id="FSWEmbedding.forward-1025"><a href="#FSWEmbedding.forward-1025"><span class="linenos">1025</span></a><span class="sd">        torch.Tensor</span>
</span><span id="FSWEmbedding.forward-1026"><a href="#FSWEmbedding.forward-1026"><span class="linenos">1026</span></a><span class="sd">            The embedding tensor. Shape depends on the mode:</span>
</span><span id="FSWEmbedding.forward-1027"><a href="#FSWEmbedding.forward-1027"><span class="linenos">1027</span></a><span class="sd">            - `(d_out,)` or `(..., d_out)` in standard mode.</span>
</span><span id="FSWEmbedding.forward-1028"><a href="#FSWEmbedding.forward-1028"><span class="linenos">1028</span></a><span class="sd">            - `(..., num_slices, num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=False`.</span>
</span><span id="FSWEmbedding.forward-1029"><a href="#FSWEmbedding.forward-1029"><span class="linenos">1029</span></a><span class="sd">            - `(..., num_slices * num_frequencies)` in Cartesian mode if `flatten_cartesian_axes=True`.</span>
</span><span id="FSWEmbedding.forward-1030"><a href="#FSWEmbedding.forward-1030"><span class="linenos">1030</span></a>
</span><span id="FSWEmbedding.forward-1031"><a href="#FSWEmbedding.forward-1031"><span class="linenos">1031</span></a><span class="sd">        Notes</span>
</span><span id="FSWEmbedding.forward-1032"><a href="#FSWEmbedding.forward-1032"><span class="linenos">1032</span></a><span class="sd">        -----</span>
</span><span id="FSWEmbedding.forward-1033"><a href="#FSWEmbedding.forward-1033"><span class="linenos">1033</span></a><span class="sd">        Multisets and distributions:</span>
</span><span id="FSWEmbedding.forward-1034"><a href="#FSWEmbedding.forward-1034"><span class="linenos">1034</span></a><span class="sd">            If `X` is `(n, d_in)` and `W` is `(n,)`, the pair represents a weighted point cloud.</span>
</span><span id="FSWEmbedding.forward-1035"><a href="#FSWEmbedding.forward-1035"><span class="linenos">1035</span></a><span class="sd">            Weights must be non-negative with positive total mass.</span>
</span><span id="FSWEmbedding.forward-1036"><a href="#FSWEmbedding.forward-1036"><span class="linenos">1036</span></a><span class="sd">            If `W` is `&#39;unit&#39;` or `&#39;uniform&#39;`, uniform weights are used internally.</span>
</span><span id="FSWEmbedding.forward-1037"><a href="#FSWEmbedding.forward-1037"><span class="linenos">1037</span></a>
</span><span id="FSWEmbedding.forward-1038"><a href="#FSWEmbedding.forward-1038"><span class="linenos">1038</span></a><span class="sd">        Batching:</span>
</span><span id="FSWEmbedding.forward-1039"><a href="#FSWEmbedding.forward-1039"><span class="linenos">1039</span></a><span class="sd">            Input tensors may include leading batch dimensions. For `X` of shape `(..., n, d_in)`</span>
</span><span id="FSWEmbedding.forward-1040"><a href="#FSWEmbedding.forward-1040"><span class="linenos">1040</span></a><span class="sd">            and `W` of shape `(..., n)`, the output shape is `(..., d_out)`.</span>
</span><span id="FSWEmbedding.forward-1041"><a href="#FSWEmbedding.forward-1041"><span class="linenos">1041</span></a>
</span><span id="FSWEmbedding.forward-1042"><a href="#FSWEmbedding.forward-1042"><span class="linenos">1042</span></a><span class="sd">        Graph mode:</span>
</span><span id="FSWEmbedding.forward-1043"><a href="#FSWEmbedding.forward-1043"><span class="linenos">1043</span></a><span class="sd">            When `graph_mode=True`, `W` must be of shape `(..., n_recipients, n)` and `X` of</span>
</span><span id="FSWEmbedding.forward-1044"><a href="#FSWEmbedding.forward-1044"><span class="linenos">1044</span></a><span class="sd">            shape `(..., n, d_in)` or broadcastable to that. The output will be</span>
</span><span id="FSWEmbedding.forward-1045"><a href="#FSWEmbedding.forward-1045"><span class="linenos">1045</span></a><span class="sd">            `(..., n_recipients, d_out)`, where each vector represents a weighted embedding of</span>
</span><span id="FSWEmbedding.forward-1046"><a href="#FSWEmbedding.forward-1046"><span class="linenos">1046</span></a><span class="sd">            neighboring nodes. This avoids expanding `X` across `n_recipients` explicitly.</span>
</span><span id="FSWEmbedding.forward-1047"><a href="#FSWEmbedding.forward-1047"><span class="linenos">1047</span></a>
</span><span id="FSWEmbedding.forward-1048"><a href="#FSWEmbedding.forward-1048"><span class="linenos">1048</span></a><span class="sd">        Cartesian mode:</span>
</span><span id="FSWEmbedding.forward-1049"><a href="#FSWEmbedding.forward-1049"><span class="linenos">1049</span></a><span class="sd">            If `d_out` is not specified but `num_slices` and `num_frequencies` are, the embedding</span>
</span><span id="FSWEmbedding.forward-1050"><a href="#FSWEmbedding.forward-1050"><span class="linenos">1050</span></a><span class="sd">            is computed over a Cartesian product. The output shape is:</span>
</span><span id="FSWEmbedding.forward-1051"><a href="#FSWEmbedding.forward-1051"><span class="linenos">1051</span></a><span class="sd">                - `(..., num_slices, num_frequencies)` if `flatten_cartesian_axes=False`</span>
</span><span id="FSWEmbedding.forward-1052"><a href="#FSWEmbedding.forward-1052"><span class="linenos">1052</span></a><span class="sd">                - `(..., num_slices * num_frequencies)` if `flatten_cartesian_axes=True`</span>
</span><span id="FSWEmbedding.forward-1053"><a href="#FSWEmbedding.forward-1053"><span class="linenos">1053</span></a>
</span><span id="FSWEmbedding.forward-1054"><a href="#FSWEmbedding.forward-1054"><span class="linenos">1054</span></a><span class="sd">        Slice serialization:</span>
</span><span id="FSWEmbedding.forward-1055"><a href="#FSWEmbedding.forward-1055"><span class="linenos">1055</span></a><span class="sd">            If `max_parallel_slices=t` is set, the computation is performed in blocks of size `t`,</span>
</span><span id="FSWEmbedding.forward-1056"><a href="#FSWEmbedding.forward-1056"><span class="linenos">1056</span></a><span class="sd">            reducing memory complexity by a factor of `num_slices / t`. The output remains unchanged.</span>
</span><span id="FSWEmbedding.forward-1057"><a href="#FSWEmbedding.forward-1057"><span class="linenos">1057</span></a>
</span><span id="FSWEmbedding.forward-1058"><a href="#FSWEmbedding.forward-1058"><span class="linenos">1058</span></a><span class="sd">        See Also</span>
</span><span id="FSWEmbedding.forward-1059"><a href="#FSWEmbedding.forward-1059"><span class="linenos">1059</span></a><span class="sd">        --------</span>
</span><span id="FSWEmbedding.forward-1060"><a href="#FSWEmbedding.forward-1060"><span class="linenos">1060</span></a><span class="sd">        FSWEmbedding.__init__ : Constructor for model configuration options.</span>
</span><span id="FSWEmbedding.forward-1061"><a href="#FSWEmbedding.forward-1061"><span class="linenos">1061</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="FSWEmbedding.forward-1062"><a href="#FSWEmbedding.forward-1062"><span class="linenos">1062</span></a>
</span><span id="FSWEmbedding.forward-1063"><a href="#FSWEmbedding.forward-1063"><span class="linenos">1063</span></a>
</span><span id="FSWEmbedding.forward-1064"><a href="#FSWEmbedding.forward-1064"><span class="linenos">1064</span></a>        <span class="c1"># Verify slices and frequencies at each forward pass if they are learnable</span>
</span><span id="FSWEmbedding.forward-1065"><a href="#FSWEmbedding.forward-1065"><span class="linenos">1065</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_slices</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1066"><a href="#FSWEmbedding.forward-1066"><span class="linenos">1066</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain NaNs&#39;</span>
</span><span id="FSWEmbedding.forward-1067"><a href="#FSWEmbedding.forward-1067"><span class="linenos">1067</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Slice vectors contain infs&#39;</span>
</span><span id="FSWEmbedding.forward-1068"><a href="#FSWEmbedding.forward-1068"><span class="linenos">1068</span></a>            <span class="c1"># Note: We allow them to contain zero vectors when they are learnable, in case i.e. when sparsity is desired</span>
</span><span id="FSWEmbedding.forward-1069"><a href="#FSWEmbedding.forward-1069"><span class="linenos">1069</span></a>            <span class="c1"># assert not (self.slice_vectors == 0).all(dim=1).any(), &#39;Slice vectors contain a zero vector&#39;</span>
</span><span id="FSWEmbedding.forward-1070"><a href="#FSWEmbedding.forward-1070"><span class="linenos">1070</span></a>
</span><span id="FSWEmbedding.forward-1071"><a href="#FSWEmbedding.forward-1071"><span class="linenos">1071</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learnable_frequencies</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1072"><a href="#FSWEmbedding.forward-1072"><span class="linenos">1072</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain NaNs&#39;</span>
</span><span id="FSWEmbedding.forward-1073"><a href="#FSWEmbedding.forward-1073"><span class="linenos">1073</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s1">&#39;Frequencies contain infs&#39;</span>
</span><span id="FSWEmbedding.forward-1074"><a href="#FSWEmbedding.forward-1074"><span class="linenos">1074</span></a>
</span><span id="FSWEmbedding.forward-1075"><a href="#FSWEmbedding.forward-1075"><span class="linenos">1075</span></a>        <span class="c1">### A. Verify input types and content</span>
</span><span id="FSWEmbedding.forward-1076"><a href="#FSWEmbedding.forward-1076"><span class="linenos">1076</span></a>
</span><span id="FSWEmbedding.forward-1077"><a href="#FSWEmbedding.forward-1077"><span class="linenos">1077</span></a>        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;total_mass_padding_thresh must be positive&#39;</span>
</span><span id="FSWEmbedding.forward-1078"><a href="#FSWEmbedding.forward-1078"><span class="linenos">1078</span></a>
</span><span id="FSWEmbedding.forward-1079"><a href="#FSWEmbedding.forward-1079"><span class="linenos">1079</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1080"><a href="#FSWEmbedding.forward-1080"><span class="linenos">1080</span></a>            <span class="k">assert</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="s1">&#39;d_edge &gt; 0 (given at initialization) necessitates graph_mode=True on forward call&#39;</span>
</span><span id="FSWEmbedding.forward-1081"><a href="#FSWEmbedding.forward-1081"><span class="linenos">1081</span></a>            <span class="k">assert</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;X_edge must be provided since d_edge &gt; 0&#39;</span>
</span><span id="FSWEmbedding.forward-1082"><a href="#FSWEmbedding.forward-1082"><span class="linenos">1082</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1083"><a href="#FSWEmbedding.forward-1083"><span class="linenos">1083</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;X_edge should be None or empty since d_edge == 0&#39;</span>
</span><span id="FSWEmbedding.forward-1084"><a href="#FSWEmbedding.forward-1084"><span class="linenos">1084</span></a>            <span class="n">X_edge</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.forward-1085"><a href="#FSWEmbedding.forward-1085"><span class="linenos">1085</span></a>
</span><span id="FSWEmbedding.forward-1086"><a href="#FSWEmbedding.forward-1086"><span class="linenos">1086</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;X must be a pytorch tensor. Instead got type </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span><span id="FSWEmbedding.forward-1087"><a href="#FSWEmbedding.forward-1087"><span class="linenos">1087</span></a>        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="ow">or</span> <span class="n">W</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">},</span> <span class="s1">&#39;W must be a pytorch tensor, </span><span class="se">\&#39;</span><span class="s1">unit</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">uniform</span><span class="se">\&#39;</span><span class="s1">&#39;</span>
</span><span id="FSWEmbedding.forward-1088"><a href="#FSWEmbedding.forward-1088"><span class="linenos">1088</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1089"><a href="#FSWEmbedding.forward-1089"><span class="linenos">1089</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;X is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1090"><a href="#FSWEmbedding.forward-1090"><span class="linenos">1090</span></a>
</span><span id="FSWEmbedding.forward-1091"><a href="#FSWEmbedding.forward-1091"><span class="linenos">1091</span></a>        <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1092"><a href="#FSWEmbedding.forward-1092"><span class="linenos">1092</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;The entries of X cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding.forward-1093"><a href="#FSWEmbedding.forward-1093"><span class="linenos">1093</span></a>            <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X must be finite&quot;</span>
</span><span id="FSWEmbedding.forward-1094"><a href="#FSWEmbedding.forward-1094"><span class="linenos">1094</span></a>
</span><span id="FSWEmbedding.forward-1095"><a href="#FSWEmbedding.forward-1095"><span class="linenos">1095</span></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="FSWEmbedding.forward-1096"><a href="#FSWEmbedding.forward-1096"><span class="linenos">1096</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1097"><a href="#FSWEmbedding.forward-1097"><span class="linenos">1097</span></a>            <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;W is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1098"><a href="#FSWEmbedding.forward-1098"><span class="linenos">1098</span></a>
</span><span id="FSWEmbedding.forward-1099"><a href="#FSWEmbedding.forward-1099"><span class="linenos">1099</span></a>            <span class="c1"># Check if W is sparse. If so, ensure that W is of the correct layout.</span>
</span><span id="FSWEmbedding.forward-1100"><a href="#FSWEmbedding.forward-1100"><span class="linenos">1100</span></a>            <span class="c1"># Note: Strangely enough, sparse tensors of layouts other than COO (e.g. CSR) may have is_sparse=False.</span>
</span><span id="FSWEmbedding.forward-1101"><a href="#FSWEmbedding.forward-1101"><span class="linenos">1101</span></a>            <span class="c1">#       This may lead us to mistakenly treat a, e.g. W that is sparse CSR as dense.</span>
</span><span id="FSWEmbedding.forward-1102"><a href="#FSWEmbedding.forward-1102"><span class="linenos">1102</span></a>            <span class="c1">#       Currently there is no is_dense() function in torch, so reading the layout string directly is the second best.</span>
</span><span id="FSWEmbedding.forward-1103"><a href="#FSWEmbedding.forward-1103"><span class="linenos">1103</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1104"><a href="#FSWEmbedding.forward-1104"><span class="linenos">1104</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse W has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">W</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1105"><a href="#FSWEmbedding.forward-1105"><span class="linenos">1105</span></a>
</span><span id="FSWEmbedding.forward-1106"><a href="#FSWEmbedding.forward-1106"><span class="linenos">1106</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse W must be coalesced&#39;</span>
</span><span id="FSWEmbedding.forward-1107"><a href="#FSWEmbedding.forward-1107"><span class="linenos">1107</span></a>                <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;W.dense_dim() must be zero&#39;</span>
</span><span id="FSWEmbedding.forward-1108"><a href="#FSWEmbedding.forward-1108"><span class="linenos">1108</span></a>
</span><span id="FSWEmbedding.forward-1109"><a href="#FSWEmbedding.forward-1109"><span class="linenos">1109</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1110"><a href="#FSWEmbedding.forward-1110"><span class="linenos">1110</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="FSWEmbedding.forward-1111"><a href="#FSWEmbedding.forward-1111"><span class="linenos">1111</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1112"><a href="#FSWEmbedding.forward-1112"><span class="linenos">1112</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1113"><a href="#FSWEmbedding.forward-1113"><span class="linenos">1113</span></a>                    <span class="n">W_vals</span> <span class="o">=</span> <span class="n">W</span>
</span><span id="FSWEmbedding.forward-1114"><a href="#FSWEmbedding.forward-1114"><span class="linenos">1114</span></a>
</span><span id="FSWEmbedding.forward-1115"><a href="#FSWEmbedding.forward-1115"><span class="linenos">1115</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1116"><a href="#FSWEmbedding.forward-1116"><span class="linenos">1116</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W_vals</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1117"><a href="#FSWEmbedding.forward-1117"><span class="linenos">1117</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;W cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding.forward-1118"><a href="#FSWEmbedding.forward-1118"><span class="linenos">1118</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">W_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be finite&quot;</span>
</span><span id="FSWEmbedding.forward-1119"><a href="#FSWEmbedding.forward-1119"><span class="linenos">1119</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="n">W_vals</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;All entries of W must be nonnegative&quot;</span>
</span><span id="FSWEmbedding.forward-1120"><a href="#FSWEmbedding.forward-1120"><span class="linenos">1120</span></a>                <span class="k">del</span> <span class="n">W_vals</span>
</span><span id="FSWEmbedding.forward-1121"><a href="#FSWEmbedding.forward-1121"><span class="linenos">1121</span></a>
</span><span id="FSWEmbedding.forward-1122"><a href="#FSWEmbedding.forward-1122"><span class="linenos">1122</span></a>        <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1123"><a href="#FSWEmbedding.forward-1123"><span class="linenos">1123</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;When X_edge is provided, W must be provided explicitly&#39;</span>
</span><span id="FSWEmbedding.forward-1124"><a href="#FSWEmbedding.forward-1124"><span class="linenos">1124</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge is on the wrong device. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1125"><a href="#FSWEmbedding.forward-1125"><span class="linenos">1125</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span> <span class="s2">&quot;X_edge has the wrong dtype. Expected </span><span class="si">%s</span><span class="s2">, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1126"><a href="#FSWEmbedding.forward-1126"><span class="linenos">1126</span></a>
</span><span id="FSWEmbedding.forward-1127"><a href="#FSWEmbedding.forward-1127"><span class="linenos">1127</span></a>            <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1128"><a href="#FSWEmbedding.forward-1128"><span class="linenos">1128</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">,</span> <span class="p">(</span> <span class="s2">&quot;Sparse X_edge has an unsupported sparsity layout &#39;</span><span class="si">%s</span><span class="s2">&#39;. Only the COO layout (torch.sparse_coo) is currently supported.&quot;</span> <span class="o">%</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">layout</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1129"><a href="#FSWEmbedding.forward-1129"><span class="linenos">1129</span></a>
</span><span id="FSWEmbedding.forward-1130"><a href="#FSWEmbedding.forward-1130"><span class="linenos">1130</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must be coalesced&#39;</span>
</span><span id="FSWEmbedding.forward-1131"><a href="#FSWEmbedding.forward-1131"><span class="linenos">1131</span></a>                <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 or 0&#39;</span>
</span><span id="FSWEmbedding.forward-1132"><a href="#FSWEmbedding.forward-1132"><span class="linenos">1132</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;X_edge.dense_dim() must be 1 since d_edge &gt; 1&#39;</span>
</span><span id="FSWEmbedding.forward-1133"><a href="#FSWEmbedding.forward-1133"><span class="linenos">1133</span></a>
</span><span id="FSWEmbedding.forward-1134"><a href="#FSWEmbedding.forward-1134"><span class="linenos">1134</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1135"><a href="#FSWEmbedding.forward-1135"><span class="linenos">1135</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span><span id="FSWEmbedding.forward-1136"><a href="#FSWEmbedding.forward-1136"><span class="linenos">1136</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1137"><a href="#FSWEmbedding.forward-1137"><span class="linenos">1137</span></a>                <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1138"><a href="#FSWEmbedding.forward-1138"><span class="linenos">1138</span></a>                    <span class="n">X_edge_vals</span> <span class="o">=</span> <span class="n">X_edge</span>
</span><span id="FSWEmbedding.forward-1139"><a href="#FSWEmbedding.forward-1139"><span class="linenos">1139</span></a>
</span><span id="FSWEmbedding.forward-1140"><a href="#FSWEmbedding.forward-1140"><span class="linenos">1140</span></a>            <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1141"><a href="#FSWEmbedding.forward-1141"><span class="linenos">1141</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;X_edge_vals cannot contain NaNs&quot;</span>
</span><span id="FSWEmbedding.forward-1142"><a href="#FSWEmbedding.forward-1142"><span class="linenos">1142</span></a>                <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_edge_vals</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(),</span> <span class="s2">&quot;All entries of X_edge_vals must be finite&quot;</span>
</span><span id="FSWEmbedding.forward-1143"><a href="#FSWEmbedding.forward-1143"><span class="linenos">1143</span></a>                <span class="k">del</span> <span class="n">X_edge_vals</span>
</span><span id="FSWEmbedding.forward-1144"><a href="#FSWEmbedding.forward-1144"><span class="linenos">1144</span></a>
</span><span id="FSWEmbedding.forward-1145"><a href="#FSWEmbedding.forward-1145"><span class="linenos">1145</span></a>            <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">,</span> <span class="s1">&#39;X_edge and W must either both or neither be sparse&#39;</span>
</span><span id="FSWEmbedding.forward-1146"><a href="#FSWEmbedding.forward-1146"><span class="linenos">1146</span></a>
</span><span id="FSWEmbedding.forward-1147"><a href="#FSWEmbedding.forward-1147"><span class="linenos">1147</span></a>
</span><span id="FSWEmbedding.forward-1148"><a href="#FSWEmbedding.forward-1148"><span class="linenos">1148</span></a>        <span class="c1">### B. Verify input sizes</span>
</span><span id="FSWEmbedding.forward-1149"><a href="#FSWEmbedding.forward-1149"><span class="linenos">1149</span></a>
</span><span id="FSWEmbedding.forward-1150"><a href="#FSWEmbedding.forward-1150"><span class="linenos">1150</span></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;X must be a tensor of order at least 2&quot;</span>
</span><span id="FSWEmbedding.forward-1151"><a href="#FSWEmbedding.forward-1151"><span class="linenos">1151</span></a>        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="s2">&quot;The last dimension of X must equal d_in=</span><span class="si">%d</span><span class="s2">. Instead got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_in</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="FSWEmbedding.forward-1152"><a href="#FSWEmbedding.forward-1152"><span class="linenos">1152</span></a>
</span><span id="FSWEmbedding.forward-1153"><a href="#FSWEmbedding.forward-1153"><span class="linenos">1153</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1154"><a href="#FSWEmbedding.forward-1154"><span class="linenos">1154</span></a>            <span class="c1"># batch_dims contains everything that precedes (n,d_in) in X.shape</span>
</span><span id="FSWEmbedding.forward-1155"><a href="#FSWEmbedding.forward-1155"><span class="linenos">1155</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="FSWEmbedding.forward-1156"><a href="#FSWEmbedding.forward-1156"><span class="linenos">1156</span></a>            <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)]</span>
</span><span id="FSWEmbedding.forward-1157"><a href="#FSWEmbedding.forward-1157"><span class="linenos">1157</span></a>
</span><span id="FSWEmbedding.forward-1158"><a href="#FSWEmbedding.forward-1158"><span class="linenos">1158</span></a>            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
</span><span id="FSWEmbedding.forward-1159"><a href="#FSWEmbedding.forward-1159"><span class="linenos">1159</span></a>                <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
</span><span id="FSWEmbedding.forward-1160"><a href="#FSWEmbedding.forward-1160"><span class="linenos">1160</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (Perhaps missing argument graph_mode=True?)&quot;</span>
</span><span id="FSWEmbedding.forward-1161"><a href="#FSWEmbedding.forward-1161"><span class="linenos">1161</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1162"><a href="#FSWEmbedding.forward-1162"><span class="linenos">1162</span></a>                    <span class="n">err_str</span> <span class="o">=</span> <span class="s2">&quot;Shape mismatch between X and W: If X.shape = (b1,b2,...,bk,n,d_in) then W.shape should be (b1,b2,...,bk,n) (unless graph_mode=True)&quot;</span>
</span><span id="FSWEmbedding.forward-1163"><a href="#FSWEmbedding.forward-1163"><span class="linenos">1163</span></a>
</span><span id="FSWEmbedding.forward-1164"><a href="#FSWEmbedding.forward-1164"><span class="linenos">1164</span></a>                <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">err_str</span>
</span><span id="FSWEmbedding.forward-1165"><a href="#FSWEmbedding.forward-1165"><span class="linenos">1165</span></a>
</span><span id="FSWEmbedding.forward-1166"><a href="#FSWEmbedding.forward-1166"><span class="linenos">1166</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;unit&#39;</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1167"><a href="#FSWEmbedding.forward-1167"><span class="linenos">1167</span></a>                <span class="c1"># Initialize with unit weights</span>
</span><span id="FSWEmbedding.forward-1168"><a href="#FSWEmbedding.forward-1168"><span class="linenos">1168</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1169"><a href="#FSWEmbedding.forward-1169"><span class="linenos">1169</span></a>
</span><span id="FSWEmbedding.forward-1170"><a href="#FSWEmbedding.forward-1170"><span class="linenos">1170</span></a>            <span class="k">elif</span> <span class="n">W</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1171"><a href="#FSWEmbedding.forward-1171"><span class="linenos">1171</span></a>                <span class="c1"># Initialize with uniform weights</span>
</span><span id="FSWEmbedding.forward-1172"><a href="#FSWEmbedding.forward-1172"><span class="linenos">1172</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span><span class="p">,),</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1173"><a href="#FSWEmbedding.forward-1173"><span class="linenos">1173</span></a>
</span><span id="FSWEmbedding.forward-1174"><a href="#FSWEmbedding.forward-1174"><span class="linenos">1174</span></a>        <span class="k">elif</span> <span class="n">graph_mode</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1175"><a href="#FSWEmbedding.forward-1175"><span class="linenos">1175</span></a>            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">W</span><span class="p">),</span> <span class="s1">&#39;W must be explicitly provided when graph_mode=True&#39;</span>
</span><span id="FSWEmbedding.forward-1176"><a href="#FSWEmbedding.forward-1176"><span class="linenos">1176</span></a>
</span><span id="FSWEmbedding.forward-1177"><a href="#FSWEmbedding.forward-1177"><span class="linenos">1177</span></a>            <span class="c1"># batch_dims contains everything that precedes (nRecipients, n) in W.shape</span>
</span><span id="FSWEmbedding.forward-1178"><a href="#FSWEmbedding.forward-1178"><span class="linenos">1178</span></a>            <span class="n">batch_dims</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</span><span id="FSWEmbedding.forward-1179"><a href="#FSWEmbedding.forward-1179"><span class="linenos">1179</span></a>            <span class="n">nRecipients</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</span><span id="FSWEmbedding.forward-1180"><a href="#FSWEmbedding.forward-1180"><span class="linenos">1180</span></a>            <span class="c1">#n = W.shape[-1]</span>
</span><span id="FSWEmbedding.forward-1181"><a href="#FSWEmbedding.forward-1181"><span class="linenos">1181</span></a>
</span><span id="FSWEmbedding.forward-1182"><a href="#FSWEmbedding.forward-1182"><span class="linenos">1182</span></a>            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span> <span class="s2">&quot;Shape mismatch between X and W: When graph_mode=True, if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,n,d_in)&quot;</span>
</span><span id="FSWEmbedding.forward-1183"><a href="#FSWEmbedding.forward-1183"><span class="linenos">1183</span></a>
</span><span id="FSWEmbedding.forward-1184"><a href="#FSWEmbedding.forward-1184"><span class="linenos">1184</span></a>            <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1185"><a href="#FSWEmbedding.forward-1185"><span class="linenos">1185</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># For PyCharm to know</span>
</span><span id="FSWEmbedding.forward-1186"><a href="#FSWEmbedding.forward-1186"><span class="linenos">1186</span></a>
</span><span id="FSWEmbedding.forward-1187"><a href="#FSWEmbedding.forward-1187"><span class="linenos">1187</span></a>                <span class="c1"># Verify that X_edge has the right shape and is compatible with W</span>
</span><span id="FSWEmbedding.forward-1188"><a href="#FSWEmbedding.forward-1188"><span class="linenos">1188</span></a>                <span class="k">assert</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="ow">or</span>
</span><span id="FSWEmbedding.forward-1189"><a href="#FSWEmbedding.forward-1189"><span class="linenos">1189</span></a>                        <span class="p">((</span><span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">))),</span> <span class="p">(</span>
</span><span id="FSWEmbedding.forward-1190"><a href="#FSWEmbedding.forward-1190"><span class="linenos">1190</span></a>                    <span class="s2">&quot;Shape mismatch between X_edge and W: if W.shape = (b1,b2,...,bk,nRecipients,n) then X.shape should be (b1,b2,...,bk,nRecipients,n,d_edge) (with the possible exception (b1,b2,...,bk,nRecipients,n) when d_edge=1&quot;</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1191"><a href="#FSWEmbedding.forward-1191"><span class="linenos">1191</span></a>
</span><span id="FSWEmbedding.forward-1192"><a href="#FSWEmbedding.forward-1192"><span class="linenos">1192</span></a>                <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1193"><a href="#FSWEmbedding.forward-1193"><span class="linenos">1193</span></a>                    <span class="k">assert</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;Sparse X_edge must have the same number of values() as W&#39;</span>
</span><span id="FSWEmbedding.forward-1194"><a href="#FSWEmbedding.forward-1194"><span class="linenos">1194</span></a>                    <span class="k">if</span> <span class="n">fsw_embedding_basic_safety_checks</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1195"><a href="#FSWEmbedding.forward-1195"><span class="linenos">1195</span></a>                        <span class="k">assert</span> <span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">indices</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s1">&#39;Sparse X_edge must have the same nonzero pattern as W&#39;</span>
</span><span id="FSWEmbedding.forward-1196"><a href="#FSWEmbedding.forward-1196"><span class="linenos">1196</span></a>
</span><span id="FSWEmbedding.forward-1197"><a href="#FSWEmbedding.forward-1197"><span class="linenos">1197</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1198"><a href="#FSWEmbedding.forward-1198"><span class="linenos">1198</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">unsqueeze_dense_dim</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1199"><a href="#FSWEmbedding.forward-1199"><span class="linenos">1199</span></a>
</span><span id="FSWEmbedding.forward-1200"><a href="#FSWEmbedding.forward-1200"><span class="linenos">1200</span></a>
</span><span id="FSWEmbedding.forward-1201"><a href="#FSWEmbedding.forward-1201"><span class="linenos">1201</span></a>        <span class="c1">### C. Precalculate axis indices and output shape</span>
</span><span id="FSWEmbedding.forward-1202"><a href="#FSWEmbedding.forward-1202"><span class="linenos">1202</span></a>
</span><span id="FSWEmbedding.forward-1203"><a href="#FSWEmbedding.forward-1203"><span class="linenos">1203</span></a>        <span class="c1"># These are the different axes we use to store data for processing. These definitions are repeated in forward_helper()</span>
</span><span id="FSWEmbedding.forward-1204"><a href="#FSWEmbedding.forward-1204"><span class="linenos">1204</span></a>        <span class="c1"># element_axis corresponds to the index of the multiset elements</span>
</span><span id="FSWEmbedding.forward-1205"><a href="#FSWEmbedding.forward-1205"><span class="linenos">1205</span></a>        <span class="c1"># ambspace_axis corresponds to the elements&#39; coordinate index in the ambient space R^d_in</span>
</span><span id="FSWEmbedding.forward-1206"><a href="#FSWEmbedding.forward-1206"><span class="linenos">1206</span></a>        <span class="c1"># After projection, the ambient space coordinates are replaced by the slice number; thus slice_axis=ambspace_axis</span>
</span><span id="FSWEmbedding.forward-1207"><a href="#FSWEmbedding.forward-1207"><span class="linenos">1207</span></a>        <span class="c1"># If we&#39;re in Cartesian mode, the frequencies have their own axis freq_axis, otherwise it is the same axis as slice_axis.</span>
</span><span id="FSWEmbedding.forward-1208"><a href="#FSWEmbedding.forward-1208"><span class="linenos">1208</span></a>        <span class="n">recipient_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># Message-recipient vertices</span>
</span><span id="FSWEmbedding.forward-1209"><a href="#FSWEmbedding.forward-1209"><span class="linenos">1209</span></a>        <span class="n">element_axis</span>  <span class="o">=</span> <span class="n">recipient_axis</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_dims</span><span class="p">)</span> <span class="c1"># In graph mode this axis denotes the message-sender vertices</span>
</span><span id="FSWEmbedding.forward-1210"><a href="#FSWEmbedding.forward-1210"><span class="linenos">1210</span></a>        <span class="n">ambspace_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="FSWEmbedding.forward-1211"><a href="#FSWEmbedding.forward-1211"><span class="linenos">1211</span></a>        <span class="n">slice_axis</span>     <span class="o">=</span> <span class="n">ambspace_axis</span>
</span><span id="FSWEmbedding.forward-1212"><a href="#FSWEmbedding.forward-1212"><span class="linenos">1212</span></a>        <span class="c1"># noinspection PyUnusedLocal</span>
</span><span id="FSWEmbedding.forward-1213"><a href="#FSWEmbedding.forward-1213"><span class="linenos">1213</span></a>        <span class="n">freq_axis</span>     <span class="o">=</span> <span class="n">slice_axis</span> <span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="n">slice_axis</span>
</span><span id="FSWEmbedding.forward-1214"><a href="#FSWEmbedding.forward-1214"><span class="linenos">1214</span></a>        <span class="n">output_slice_axis</span> <span class="o">=</span> <span class="n">element_axis</span> <span class="c1"># In the output, the element axis is replaced by the slice axis</span>
</span><span id="FSWEmbedding.forward-1215"><a href="#FSWEmbedding.forward-1215"><span class="linenos">1215</span></a>
</span><span id="FSWEmbedding.forward-1216"><a href="#FSWEmbedding.forward-1216"><span class="linenos">1216</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">=</span>  <span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">nRecipients</span><span class="p">,)</span> <span class="k">if</span> <span class="n">graph_mode</span> <span class="k">else</span> <span class="n">batch_dims</span>
</span><span id="FSWEmbedding.forward-1217"><a href="#FSWEmbedding.forward-1217"><span class="linenos">1217</span></a>        <span class="n">output_shape_before_collapse_and_totmass_augmentation</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequencies</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,)</span>
</span><span id="FSWEmbedding.forward-1218"><a href="#FSWEmbedding.forward-1218"><span class="linenos">1218</span></a>
</span><span id="FSWEmbedding.forward-1219"><a href="#FSWEmbedding.forward-1219"><span class="linenos">1219</span></a>        <span class="c1">### D. Input is ok. Start working.</span>
</span><span id="FSWEmbedding.forward-1220"><a href="#FSWEmbedding.forward-1220"><span class="linenos">1220</span></a>
</span><span id="FSWEmbedding.forward-1221"><a href="#FSWEmbedding.forward-1221"><span class="linenos">1221</span></a>        <span class="c1"># Calculate W_sum, which contains the total mass of the input measures</span>
</span><span id="FSWEmbedding.forward-1222"><a href="#FSWEmbedding.forward-1222"><span class="linenos">1222</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1223"><a href="#FSWEmbedding.forward-1223"><span class="linenos">1223</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1224"><a href="#FSWEmbedding.forward-1224"><span class="linenos">1224</span></a>            <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1225"><a href="#FSWEmbedding.forward-1225"><span class="linenos">1225</span></a>                                             <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1226"><a href="#FSWEmbedding.forward-1226"><span class="linenos">1226</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">sum_sparseToDense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1227"><a href="#FSWEmbedding.forward-1227"><span class="linenos">1227</span></a>
</span><span id="FSWEmbedding.forward-1228"><a href="#FSWEmbedding.forward-1228"><span class="linenos">1228</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1229"><a href="#FSWEmbedding.forward-1229"><span class="linenos">1229</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1230"><a href="#FSWEmbedding.forward-1230"><span class="linenos">1230</span></a>            <span class="n">W_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1231"><a href="#FSWEmbedding.forward-1231"><span class="linenos">1231</span></a>
</span><span id="FSWEmbedding.forward-1232"><a href="#FSWEmbedding.forward-1232"><span class="linenos">1232</span></a>        <span class="c1"># Total-mass deficit to be compensated for by padding</span>
</span><span id="FSWEmbedding.forward-1233"><a href="#FSWEmbedding.forward-1233"><span class="linenos">1233</span></a>        <span class="n">W_pad</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span> <span class="o">-</span> <span class="n">W_sum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1234"><a href="#FSWEmbedding.forward-1234"><span class="linenos">1234</span></a>
</span><span id="FSWEmbedding.forward-1235"><a href="#FSWEmbedding.forward-1235"><span class="linenos">1235</span></a>        <span class="c1"># Detect weight deficit and augment W and X accordingly</span>
</span><span id="FSWEmbedding.forward-1236"><a href="#FSWEmbedding.forward-1236"><span class="linenos">1236</span></a>        <span class="k">if</span> <span class="p">(</span><span class="n">W_pad</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
</span><span id="FSWEmbedding.forward-1237"><a href="#FSWEmbedding.forward-1237"><span class="linenos">1237</span></a>            <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1238"><a href="#FSWEmbedding.forward-1238"><span class="linenos">1238</span></a>            <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FSWEmbedding.forward-1239"><a href="#FSWEmbedding.forward-1239"><span class="linenos">1239</span></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1240"><a href="#FSWEmbedding.forward-1240"><span class="linenos">1240</span></a>
</span><span id="FSWEmbedding.forward-1241"><a href="#FSWEmbedding.forward-1241"><span class="linenos">1241</span></a>            <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1242"><a href="#FSWEmbedding.forward-1242"><span class="linenos">1242</span></a>                <span class="c1"># Make sure this works</span>
</span><span id="FSWEmbedding.forward-1243"><a href="#FSWEmbedding.forward-1243"><span class="linenos">1243</span></a>                <span class="n">W_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">to_sparse_full</span><span class="p">(</span><span class="n">W_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1244"><a href="#FSWEmbedding.forward-1244"><span class="linenos">1244</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1245"><a href="#FSWEmbedding.forward-1245"><span class="linenos">1245</span></a>                <span class="n">slice_info_W</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">get_slice_info</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">calc_nnz_per_slice</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1246"><a href="#FSWEmbedding.forward-1246"><span class="linenos">1246</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span> <span class="n">fail_if_cuda_extension_load_fails</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1247"><a href="#FSWEmbedding.forward-1247"><span class="linenos">1247</span></a>
</span><span id="FSWEmbedding.forward-1248"><a href="#FSWEmbedding.forward-1248"><span class="linenos">1248</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1249"><a href="#FSWEmbedding.forward-1249"><span class="linenos">1249</span></a>                    <span class="n">X_edge_pad_inds</span> <span class="o">=</span> <span class="n">W_pad</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
</span><span id="FSWEmbedding.forward-1250"><a href="#FSWEmbedding.forward-1250"><span class="linenos">1250</span></a>                    <span class="n">X_edge_pad_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nRecipients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1251"><a href="#FSWEmbedding.forward-1251"><span class="linenos">1251</span></a>                    <span class="n">X_edge_pad_shape</span> <span class="o">=</span> <span class="n">replace_in_tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">X_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1252"><a href="#FSWEmbedding.forward-1252"><span class="linenos">1252</span></a>
</span><span id="FSWEmbedding.forward-1253"><a href="#FSWEmbedding.forward-1253"><span class="linenos">1253</span></a>                    <span class="n">X_edge_pad</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse_coo_tensor_coalesced</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">X_edge_pad_inds</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">X_edge_pad_vals</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">X_edge_pad_shape</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1254"><a href="#FSWEmbedding.forward-1254"><span class="linenos">1254</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">concat_sparse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">X_edge_pad</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1255"><a href="#FSWEmbedding.forward-1255"><span class="linenos">1255</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1256"><a href="#FSWEmbedding.forward-1256"><span class="linenos">1256</span></a>                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1257"><a href="#FSWEmbedding.forward-1257"><span class="linenos">1257</span></a>                <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_pad</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1258"><a href="#FSWEmbedding.forward-1258"><span class="linenos">1258</span></a>                <span class="k">if</span> <span class="n">X_edge</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1259"><a href="#FSWEmbedding.forward-1259"><span class="linenos">1259</span></a>                    <span class="n">zshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_d_edge</span><span class="p">,</span> <span class="p">]</span>
</span><span id="FSWEmbedding.forward-1260"><a href="#FSWEmbedding.forward-1260"><span class="linenos">1260</span></a>                    <span class="n">zshape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="FSWEmbedding.forward-1261"><a href="#FSWEmbedding.forward-1261"><span class="linenos">1261</span></a>                    <span class="k">if</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
</span><span id="FSWEmbedding.forward-1262"><a href="#FSWEmbedding.forward-1262"><span class="linenos">1262</span></a>                        <span class="n">X_edge</span> <span class="o">=</span> <span class="n">X_edge</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1263"><a href="#FSWEmbedding.forward-1263"><span class="linenos">1263</span></a>                    <span class="n">X_edge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_edge</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">zshape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1264"><a href="#FSWEmbedding.forward-1264"><span class="linenos">1264</span></a>
</span><span id="FSWEmbedding.forward-1265"><a href="#FSWEmbedding.forward-1265"><span class="linenos">1265</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">custom_lowclamp</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W_sum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_padding_thresh</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1266"><a href="#FSWEmbedding.forward-1266"><span class="linenos">1266</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1267"><a href="#FSWEmbedding.forward-1267"><span class="linenos">1267</span></a>            <span class="n">W_sum_padded</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding.forward-1268"><a href="#FSWEmbedding.forward-1268"><span class="linenos">1268</span></a>
</span><span id="FSWEmbedding.forward-1269"><a href="#FSWEmbedding.forward-1269"><span class="linenos">1269</span></a>        <span class="k">del</span> <span class="n">W_pad</span>
</span><span id="FSWEmbedding.forward-1270"><a href="#FSWEmbedding.forward-1270"><span class="linenos">1270</span></a>
</span><span id="FSWEmbedding.forward-1271"><a href="#FSWEmbedding.forward-1271"><span class="linenos">1271</span></a>        <span class="c1"># Normalize W according to W_sum_padded</span>
</span><span id="FSWEmbedding.forward-1272"><a href="#FSWEmbedding.forward-1272"><span class="linenos">1272</span></a>        <span class="k">if</span> <span class="n">W</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1273"><a href="#FSWEmbedding.forward-1273"><span class="linenos">1273</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">ag</span><span class="o">.</span><span class="n">div_sparse_dense</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W_sum_padded</span><span class="p">,</span> <span class="n">slice_info_W</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1274"><a href="#FSWEmbedding.forward-1274"><span class="linenos">1274</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1275"><a href="#FSWEmbedding.forward-1275"><span class="linenos">1275</span></a>                                          <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1276"><a href="#FSWEmbedding.forward-1276"><span class="linenos">1276</span></a>            <span class="k">del</span> <span class="n">slice_info_W</span><span class="p">,</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding.forward-1277"><a href="#FSWEmbedding.forward-1277"><span class="linenos">1277</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1278"><a href="#FSWEmbedding.forward-1278"><span class="linenos">1278</span></a>            <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">/</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding.forward-1279"><a href="#FSWEmbedding.forward-1279"><span class="linenos">1279</span></a>            <span class="k">del</span> <span class="n">W_sum_padded</span>
</span><span id="FSWEmbedding.forward-1280"><a href="#FSWEmbedding.forward-1280"><span class="linenos">1280</span></a>
</span><span id="FSWEmbedding.forward-1281"><a href="#FSWEmbedding.forward-1281"><span class="linenos">1281</span></a>        <span class="c1"># For compatibility reasons, we support the case of zero-dimensional output tensor</span>
</span><span id="FSWEmbedding.forward-1282"><a href="#FSWEmbedding.forward-1282"><span class="linenos">1282</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_d_out</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1283"><a href="#FSWEmbedding.forward-1283"><span class="linenos">1283</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1284"><a href="#FSWEmbedding.forward-1284"><span class="linenos">1284</span></a>
</span><span id="FSWEmbedding.forward-1285"><a href="#FSWEmbedding.forward-1285"><span class="linenos">1285</span></a>        <span class="k">elif</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">):</span>
</span><span id="FSWEmbedding.forward-1286"><a href="#FSWEmbedding.forward-1286"><span class="linenos">1286</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1287"><a href="#FSWEmbedding.forward-1287"><span class="linenos">1287</span></a>                                                 <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1288"><a href="#FSWEmbedding.forward-1288"><span class="linenos">1288</span></a>                                                 <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1289"><a href="#FSWEmbedding.forward-1289"><span class="linenos">1289</span></a>
</span><span id="FSWEmbedding.forward-1290"><a href="#FSWEmbedding.forward-1290"><span class="linenos">1290</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1291"><a href="#FSWEmbedding.forward-1291"><span class="linenos">1291</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">max_parallel_slices</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="s1">&#39;max_parallel_slices must be None or a positive integer&#39;</span>
</span><span id="FSWEmbedding.forward-1292"><a href="#FSWEmbedding.forward-1292"><span class="linenos">1292</span></a>
</span><span id="FSWEmbedding.forward-1293"><a href="#FSWEmbedding.forward-1293"><span class="linenos">1293</span></a>            <span class="n">nIter</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">%</span> <span class="n">max_parallel_slices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span> <span class="o">//</span> <span class="n">max_parallel_slices</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1294"><a href="#FSWEmbedding.forward-1294"><span class="linenos">1294</span></a>
</span><span id="FSWEmbedding.forward-1295"><a href="#FSWEmbedding.forward-1295"><span class="linenos">1295</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">output_shape_before_collapse_and_totmass_augmentation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1296"><a href="#FSWEmbedding.forward-1296"><span class="linenos">1296</span></a>
</span><span id="FSWEmbedding.forward-1297"><a href="#FSWEmbedding.forward-1297"><span class="linenos">1297</span></a>            <span class="k">for</span> <span class="n">iIter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nIter</span><span class="p">):</span>
</span><span id="FSWEmbedding.forward-1298"><a href="#FSWEmbedding.forward-1298"><span class="linenos">1298</span></a>                <span class="n">inds_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">iIter</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_slices</span><span class="p">,</span> <span class="p">(</span><span class="n">iIter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_parallel_slices</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1299"><a href="#FSWEmbedding.forward-1299"><span class="linenos">1299</span></a>                <span class="n">slice_vecs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_vectors</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="FSWEmbedding.forward-1300"><a href="#FSWEmbedding.forward-1300"><span class="linenos">1300</span></a>                <span class="n">freqs_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequencies</span><span class="p">[</span><span class="n">inds_curr</span><span class="p">]</span>
</span><span id="FSWEmbedding.forward-1301"><a href="#FSWEmbedding.forward-1301"><span class="linenos">1301</span></a>
</span><span id="FSWEmbedding.forward-1302"><a href="#FSWEmbedding.forward-1302"><span class="linenos">1302</span></a>                <span class="n">out_curr</span> <span class="o">=</span> <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_forward_helper</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">slice_vecs_curr</span><span class="p">,</span> <span class="n">freqs_curr</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">X_edge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1303"><a href="#FSWEmbedding.forward-1303"><span class="linenos">1303</span></a>                                                        <span class="n">use_custom_cuda_extension_if_available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_custom_cuda_extension_if_available</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1304"><a href="#FSWEmbedding.forward-1304"><span class="linenos">1304</span></a>                                                        <span class="n">fail_if_cuda_extension_load_fails</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fail_if_cuda_extension_load_fails</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1305"><a href="#FSWEmbedding.forward-1305"><span class="linenos">1305</span></a>
</span><span id="FSWEmbedding.forward-1306"><a href="#FSWEmbedding.forward-1306"><span class="linenos">1306</span></a>                <span class="n">assign_at</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">out_curr</span><span class="p">,</span> <span class="n">output_slice_axis</span><span class="p">,</span> <span class="n">inds_curr</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1307"><a href="#FSWEmbedding.forward-1307"><span class="linenos">1307</span></a>
</span><span id="FSWEmbedding.forward-1308"><a href="#FSWEmbedding.forward-1308"><span class="linenos">1308</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cartesian_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_cartesian_axes</span> <span class="p">:</span>
</span><span id="FSWEmbedding.forward-1309"><a href="#FSWEmbedding.forward-1309"><span class="linenos">1309</span></a>            <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="n">element_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1310"><a href="#FSWEmbedding.forward-1310"><span class="linenos">1310</span></a>
</span><span id="FSWEmbedding.forward-1311"><a href="#FSWEmbedding.forward-1311"><span class="linenos">1311</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_total_mass</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1312"><a href="#FSWEmbedding.forward-1312"><span class="linenos">1312</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1313"><a href="#FSWEmbedding.forward-1313"><span class="linenos">1313</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">IDENTITY</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1314"><a href="#FSWEmbedding.forward-1314"><span class="linenos">1314</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding.forward-1315"><a href="#FSWEmbedding.forward-1315"><span class="linenos">1315</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">SQRT</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1316"><a href="#FSWEmbedding.forward-1316"><span class="linenos">1316</span></a>                    <span class="c1"># x/(sqrt(x+1)+1) is a numerically-safe formulation of sqrt(1+x)-1</span>
</span><span id="FSWEmbedding.forward-1317"><a href="#FSWEmbedding.forward-1317"><span class="linenos">1317</span></a>                    <span class="c1"># note that we don&#39;t use sqrt(1+x) since we need the function to vanish at x=0,</span>
</span><span id="FSWEmbedding.forward-1318"><a href="#FSWEmbedding.forward-1318"><span class="linenos">1318</span></a>                    <span class="c1"># and we don&#39;t use sqrt(x) since we need it to have a gradient at x=0.</span>
</span><span id="FSWEmbedding.forward-1319"><a href="#FSWEmbedding.forward-1319"><span class="linenos">1319</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span> <span class="n">W_sum</span> <span class="o">/</span> <span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">W_sum</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
</span><span id="FSWEmbedding.forward-1320"><a href="#FSWEmbedding.forward-1320"><span class="linenos">1320</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingTransformation</span><span class="o">.</span><span class="n">LOG</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1321"><a href="#FSWEmbedding.forward-1321"><span class="linenos">1321</span></a>                    <span class="n">encoded_total_mass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">W_sum</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1322"><a href="#FSWEmbedding.forward-1322"><span class="linenos">1322</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1323"><a href="#FSWEmbedding.forward-1323"><span class="linenos">1323</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding function: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_transformation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1324"><a href="#FSWEmbedding.forward-1324"><span class="linenos">1324</span></a>
</span><span id="FSWEmbedding.forward-1325"><a href="#FSWEmbedding.forward-1325"><span class="linenos">1325</span></a>            <span class="n">encoded_total_mass</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_scale</span>
</span><span id="FSWEmbedding.forward-1326"><a href="#FSWEmbedding.forward-1326"><span class="linenos">1326</span></a>
</span><span id="FSWEmbedding.forward-1327"><a href="#FSWEmbedding.forward-1327"><span class="linenos">1327</span></a>            <span class="k">del</span> <span class="n">W_sum</span>
</span><span id="FSWEmbedding.forward-1328"><a href="#FSWEmbedding.forward-1328"><span class="linenos">1328</span></a>
</span><span id="FSWEmbedding.forward-1329"><a href="#FSWEmbedding.forward-1329"><span class="linenos">1329</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="FSWEmbedding.forward-1330"><a href="#FSWEmbedding.forward-1330"><span class="linenos">1330</span></a>
</span><span id="FSWEmbedding.forward-1331"><a href="#FSWEmbedding.forward-1331"><span class="linenos">1331</span></a>            <span class="n">needs_emb_norm</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span> <span class="ow">in</span>
</span><span id="FSWEmbedding.forward-1332"><a href="#FSWEmbedding.forward-1332"><span class="linenos">1332</span></a>                              <span class="p">{</span><span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1333"><a href="#FSWEmbedding.forward-1333"><span class="linenos">1333</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1334"><a href="#FSWEmbedding.forward-1334"><span class="linenos">1334</span></a>                               <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">})</span>
</span><span id="FSWEmbedding.forward-1335"><a href="#FSWEmbedding.forward-1335"><span class="linenos">1335</span></a>
</span><span id="FSWEmbedding.forward-1336"><a href="#FSWEmbedding.forward-1336"><span class="linenos">1336</span></a>            <span class="k">if</span> <span class="n">needs_emb_norm</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1337"><a href="#FSWEmbedding.forward-1337"><span class="linenos">1337</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_emb</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1338"><a href="#FSWEmbedding.forward-1338"><span class="linenos">1338</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1339"><a href="#FSWEmbedding.forward-1339"><span class="linenos">1339</span></a>                <span class="n">X_emb_norm</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="FSWEmbedding.forward-1340"><a href="#FSWEmbedding.forward-1340"><span class="linenos">1340</span></a>
</span><span id="FSWEmbedding.forward-1341"><a href="#FSWEmbedding.forward-1341"><span class="linenos">1341</span></a>            <span class="k">match</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1342"><a href="#FSWEmbedding.forward-1342"><span class="linenos">1342</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">DECOUPLED</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1343"><a href="#FSWEmbedding.forward-1343"><span class="linenos">1343</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1344"><a href="#FSWEmbedding.forward-1344"><span class="linenos">1344</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">SCALED</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1345"><a href="#FSWEmbedding.forward-1345"><span class="linenos">1345</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1346"><a href="#FSWEmbedding.forward-1346"><span class="linenos">1346</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1347"><a href="#FSWEmbedding.forward-1347"><span class="linenos">1347</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">encoded_total_mass</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1348"><a href="#FSWEmbedding.forward-1348"><span class="linenos">1348</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_SCALED</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1349"><a href="#FSWEmbedding.forward-1349"><span class="linenos">1349</span></a>                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="c1"># to silence PyCharm</span>
</span><span id="FSWEmbedding.forward-1350"><a href="#FSWEmbedding.forward-1350"><span class="linenos">1350</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">(</span><span class="n">X_emb_norm</span><span class="p">,</span> <span class="n">encoded_total_mass</span><span class="o">*</span><span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1351"><a href="#FSWEmbedding.forward-1351"><span class="linenos">1351</span></a>                <span class="k">case</span> <span class="n">TotalMassEncodingMethod</span><span class="o">.</span><span class="n">HOMOGENEOUS_LEGACY</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1352"><a href="#FSWEmbedding.forward-1352"><span class="linenos">1352</span></a>                    <span class="n">X_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part1</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb_norm</span><span class="p">,</span>
</span><span id="FSWEmbedding.forward-1353"><a href="#FSWEmbedding.forward-1353"><span class="linenos">1353</span></a>                                       <span class="n">FSWEmbedding</span><span class="o">.</span><span class="n">_total_mass_homogeneous_legacy_encoding_part2</span><span class="p">(</span><span class="n">encoded_total_mass</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_emb</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1354"><a href="#FSWEmbedding.forward-1354"><span class="linenos">1354</span></a>                <span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>  <span class="c1"># fallback</span>
</span><span id="FSWEmbedding.forward-1355"><a href="#FSWEmbedding.forward-1355"><span class="linenos">1355</span></a>                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported encoding method: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_mass_encoding_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="FSWEmbedding.forward-1356"><a href="#FSWEmbedding.forward-1356"><span class="linenos">1356</span></a>
</span><span id="FSWEmbedding.forward-1357"><a href="#FSWEmbedding.forward-1357"><span class="linenos">1357</span></a>            <span class="k">del</span> <span class="n">X_emb_norm</span>
</span><span id="FSWEmbedding.forward-1358"><a href="#FSWEmbedding.forward-1358"><span class="linenos">1358</span></a>
</span><span id="FSWEmbedding.forward-1359"><a href="#FSWEmbedding.forward-1359"><span class="linenos">1359</span></a>        <span class="c1"># Add bias</span>
</span><span id="FSWEmbedding.forward-1360"><a href="#FSWEmbedding.forward-1360"><span class="linenos">1360</span></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_bias</span><span class="p">:</span>
</span><span id="FSWEmbedding.forward-1361"><a href="#FSWEmbedding.forward-1361"><span class="linenos">1361</span></a>            <span class="n">X_emb</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span><span id="FSWEmbedding.forward-1362"><a href="#FSWEmbedding.forward-1362"><span class="linenos">1362</span></a>
</span><span id="FSWEmbedding.forward-1363"><a href="#FSWEmbedding.forward-1363"><span class="linenos">1363</span></a>        <span class="k">return</span> <span class="n">X_emb</span>
</span></pre></div>


            <div class="docstring"><p>Compute the FSW embedding of an input multiset, measure, or graph.</p>

<p>This method maps input sets of vectors (optionally weighted) to vectors in ℝ^{d_out}
using the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and
graph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>X</strong> (torch.Tensor):
Input tensor of shape <code>(n, d_in)</code> or <code>(..., n, d_in)</code> for batched input.</li>
<li><strong>W</strong> (torch.Tensor or {'unit', 'uniform'}, default='unit'):
Weights tensor of shape <code>(n,)</code> or <code>(..., n)</code> corresponding to point importance.
If set to <code>'unit'</code> or <code>'uniform'</code>, uniform weights of <code>1/n</code> are assumed.</li>
<li><strong>X_edge</strong> (torch.Tensor, optional):
Optional edge feature tensor. Required if <code>d_edge &gt; 0</code> was set at initialization.</li>
<li><strong>graph_mode</strong> (bool, default=False):
If True, interprets <code>W</code> as an adjacency matrix and computes a neighbor-aggregated
embedding.</li>
<li><strong>max_parallel_slices</strong> (int, optional):
Limits the number of slices processed in parallel. Reduces memory usage by computing
the embedding in smaller blocks without changing the result.</li>
</ul>

<h6 id="returns">Returns</h6>

<ul>
<li><strong>torch.Tensor</strong>: The embedding tensor. Shape depends on the mode:
<ul>
<li><code>(d_out,)</code> or <code>(..., d_out)</code> in standard mode.</li>
<li><code>(..., num_slices, num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=False</code>.</li>
<li><code>(..., num_slices * num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=True</code>.</li>
</ul></li>
</ul>

<h6 id="notes">Notes</h6>

<p>Multisets and distributions:
    If <code>X</code> is <code>(n, d_in)</code> and <code>W</code> is <code>(n,)</code>, the pair represents a weighted point cloud.
    Weights must be non-negative with positive total mass.
    If <code>W</code> is <code>'unit'</code> or <code>'uniform'</code>, uniform weights are used internally.</p>

<p>Batching:
    Input tensors may include leading batch dimensions. For <code>X</code> of shape <code>(..., n, d_in)</code>
    and <code>W</code> of shape <code>(..., n)</code>, the output shape is <code>(..., d_out)</code>.</p>

<p>Graph mode:
    When <code>graph_mode=True</code>, <code>W</code> must be of shape <code>(..., n_recipients, n)</code> and <code>X</code> of
    shape <code>(..., n, d_in)</code> or broadcastable to that. The output will be
    <code>(..., n_recipients, d_out)</code>, where each vector represents a weighted embedding of
    neighboring nodes. This avoids expanding <code>X</code> across <code>n_recipients</code> explicitly.</p>

<p>Cartesian mode:
    If <code><a href="#FSWEmbedding.d_out">d_out</a></code> is not specified but <code><a href="#FSWEmbedding.num_slices">num_slices</a></code> and <code><a href="#FSWEmbedding.num_frequencies">num_frequencies</a></code> are, the embedding
    is computed over a Cartesian product. The output shape is:
        - <code>(..., num_slices, num_frequencies)</code> if <code>flatten_cartesian_axes=False</code>
        - <code>(..., num_slices * num_frequencies)</code> if <code>flatten_cartesian_axes=True</code></p>

<p>Slice serialization:
    If <code>max_parallel_slices=t</code> is set, the computation is performed in blocks of size <code>t</code>,
    reducing memory complexity by a factor of <code>num_slices / t</code>. The output remains unchanged.</p>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">FSWEmbedding.__init__</a></code>:  Constructor for model configuration options.  </p>
</div>


                            </div>
                </section>
                <section id="EnumWithResolve">
                            <input id="EnumWithResolve-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">EnumWithResolve</span><wbr>(<span class="base">enum.Enum</span>):

                <label class="view-source-button" for="EnumWithResolve-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EnumWithResolve"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EnumWithResolve-146"><a href="#EnumWithResolve-146"><span class="linenos">146</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EnumWithResolve</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
</span><span id="EnumWithResolve-147"><a href="#EnumWithResolve-147"><span class="linenos">147</span></a>    <span class="nd">@classmethod</span>
</span><span id="EnumWithResolve-148"><a href="#EnumWithResolve-148"><span class="linenos">148</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">resolve</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">_E</span><span class="p">],</span> <span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_E</span><span class="p">:</span>
</span><span id="EnumWithResolve-149"><a href="#EnumWithResolve-149"><span class="linenos">149</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
</span><span id="EnumWithResolve-150"><a href="#EnumWithResolve-150"><span class="linenos">150</span></a>            <span class="k">return</span> <span class="n">obj</span>
</span><span id="EnumWithResolve-151"><a href="#EnumWithResolve-151"><span class="linenos">151</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="EnumWithResolve-152"><a href="#EnumWithResolve-152"><span class="linenos">152</span></a>            <span class="k">try</span><span class="p">:</span>
</span><span id="EnumWithResolve-153"><a href="#EnumWithResolve-153"><span class="linenos">153</span></a>                <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
</span><span id="EnumWithResolve-154"><a href="#EnumWithResolve-154"><span class="linenos">154</span></a>            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
</span><span id="EnumWithResolve-155"><a href="#EnumWithResolve-155"><span class="linenos">155</span></a>                <span class="n">valid</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">cls</span><span class="p">]</span>
</span><span id="EnumWithResolve-156"><a href="#EnumWithResolve-156"><span class="linenos">156</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="EnumWithResolve-157"><a href="#EnumWithResolve-157"><span class="linenos">157</span></a>                    <span class="sa">f</span><span class="s2">&quot;Invalid string &#39;</span><span class="si">{</span><span class="n">obj</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">. Valid options: </span><span class="si">{</span><span class="n">valid</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="EnumWithResolve-158"><a href="#EnumWithResolve-158"><span class="linenos">158</span></a>                <span class="p">)</span>
</span><span id="EnumWithResolve-159"><a href="#EnumWithResolve-159"><span class="linenos">159</span></a>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
</span><span id="EnumWithResolve-160"><a href="#EnumWithResolve-160"><span class="linenos">160</span></a>            <span class="sa">f</span><span class="s2">&quot;Expected a string or </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instance, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="EnumWithResolve-161"><a href="#EnumWithResolve-161"><span class="linenos">161</span></a>        <span class="p">)</span>
</span><span id="EnumWithResolve-162"><a href="#EnumWithResolve-162"><span class="linenos">162</span></a>
</span><span id="EnumWithResolve-163"><a href="#EnumWithResolve-163"><span class="linenos">163</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="EnumWithResolve-164"><a href="#EnumWithResolve-164"><span class="linenos">164</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the string value of the enum member.&quot;&quot;&quot;</span>
</span><span id="EnumWithResolve-165"><a href="#EnumWithResolve-165"><span class="linenos">165</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span>
</span></pre></div>


    

                            <div id="EnumWithResolve.resolve" class="classattr">
                                        <input id="EnumWithResolve.resolve-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
                    <div class="decorator decorator-classmethod">@classmethod</div>

        <span class="def">def</span>
        <span class="name">resolve</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="o">~</span><span class="n">_E</span><span class="p">]</span>, </span><span class="param"><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span></span><span class="return-annotation">) -> <span class="o">~</span><span class="n">_E</span>:</span></span>

                <label class="view-source-button" for="EnumWithResolve.resolve-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EnumWithResolve.resolve"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EnumWithResolve.resolve-147"><a href="#EnumWithResolve.resolve-147"><span class="linenos">147</span></a>    <span class="nd">@classmethod</span>
</span><span id="EnumWithResolve.resolve-148"><a href="#EnumWithResolve.resolve-148"><span class="linenos">148</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">resolve</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">_E</span><span class="p">],</span> <span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_E</span><span class="p">:</span>
</span><span id="EnumWithResolve.resolve-149"><a href="#EnumWithResolve.resolve-149"><span class="linenos">149</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
</span><span id="EnumWithResolve.resolve-150"><a href="#EnumWithResolve.resolve-150"><span class="linenos">150</span></a>            <span class="k">return</span> <span class="n">obj</span>
</span><span id="EnumWithResolve.resolve-151"><a href="#EnumWithResolve.resolve-151"><span class="linenos">151</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="EnumWithResolve.resolve-152"><a href="#EnumWithResolve.resolve-152"><span class="linenos">152</span></a>            <span class="k">try</span><span class="p">:</span>
</span><span id="EnumWithResolve.resolve-153"><a href="#EnumWithResolve.resolve-153"><span class="linenos">153</span></a>                <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
</span><span id="EnumWithResolve.resolve-154"><a href="#EnumWithResolve.resolve-154"><span class="linenos">154</span></a>            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
</span><span id="EnumWithResolve.resolve-155"><a href="#EnumWithResolve.resolve-155"><span class="linenos">155</span></a>                <span class="n">valid</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">cls</span><span class="p">]</span>
</span><span id="EnumWithResolve.resolve-156"><a href="#EnumWithResolve.resolve-156"><span class="linenos">156</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="EnumWithResolve.resolve-157"><a href="#EnumWithResolve.resolve-157"><span class="linenos">157</span></a>                    <span class="sa">f</span><span class="s2">&quot;Invalid string &#39;</span><span class="si">{</span><span class="n">obj</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">. Valid options: </span><span class="si">{</span><span class="n">valid</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="EnumWithResolve.resolve-158"><a href="#EnumWithResolve.resolve-158"><span class="linenos">158</span></a>                <span class="p">)</span>
</span><span id="EnumWithResolve.resolve-159"><a href="#EnumWithResolve.resolve-159"><span class="linenos">159</span></a>        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
</span><span id="EnumWithResolve.resolve-160"><a href="#EnumWithResolve.resolve-160"><span class="linenos">160</span></a>            <span class="sa">f</span><span class="s2">&quot;Expected a string or </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instance, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
</span><span id="EnumWithResolve.resolve-161"><a href="#EnumWithResolve.resolve-161"><span class="linenos">161</span></a>        <span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="TotalMassEncodingTransformation">
                            <input id="TotalMassEncodingTransformation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">TotalMassEncodingTransformation</span><wbr>(<span class="base"><a href="#EnumWithResolve">EnumWithResolve</a></span>):

                <label class="view-source-button" for="TotalMassEncodingTransformation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TotalMassEncodingTransformation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TotalMassEncodingTransformation-168"><a href="#TotalMassEncodingTransformation-168"><span class="linenos">168</span></a><span class="k">class</span><span class="w"> </span><span class="nc">TotalMassEncodingTransformation</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="TotalMassEncodingTransformation-169"><a href="#TotalMassEncodingTransformation-169"><span class="linenos">169</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformation applied to the total mass before incorporating into the embedding.</span>
</span><span id="TotalMassEncodingTransformation-170"><a href="#TotalMassEncodingTransformation-170"><span class="linenos">170</span></a>
</span><span id="TotalMassEncodingTransformation-171"><a href="#TotalMassEncodingTransformation-171"><span class="linenos">171</span></a><span class="sd">    Each option defines a different transformation applied to the total mass of an input measure/multiset</span>
</span><span id="TotalMassEncodingTransformation-172"><a href="#TotalMassEncodingTransformation-172"><span class="linenos">172</span></a><span class="sd">    before it is incorporated into the embedding vector.</span>
</span><span id="TotalMassEncodingTransformation-173"><a href="#TotalMassEncodingTransformation-173"><span class="linenos">173</span></a>
</span><span id="TotalMassEncodingTransformation-174"><a href="#TotalMassEncodingTransformation-174"><span class="linenos">174</span></a><span class="sd">    Attributes</span>
</span><span id="TotalMassEncodingTransformation-175"><a href="#TotalMassEncodingTransformation-175"><span class="linenos">175</span></a><span class="sd">    ----------</span>
</span><span id="TotalMassEncodingTransformation-176"><a href="#TotalMassEncodingTransformation-176"><span class="linenos">176</span></a><span class="sd">    IDENTITY : str</span>
</span><span id="TotalMassEncodingTransformation-177"><a href="#TotalMassEncodingTransformation-177"><span class="linenos">177</span></a><span class="sd">        $f(x) = x$; no transformation.</span>
</span><span id="TotalMassEncodingTransformation-178"><a href="#TotalMassEncodingTransformation-178"><span class="linenos">178</span></a><span class="sd">    SQRT : str</span>
</span><span id="TotalMassEncodingTransformation-179"><a href="#TotalMassEncodingTransformation-179"><span class="linenos">179</span></a><span class="sd">        $f(x) = \\sqrt{1 + x} - 1$; mild nonlinearity.</span>
</span><span id="TotalMassEncodingTransformation-180"><a href="#TotalMassEncodingTransformation-180"><span class="linenos">180</span></a><span class="sd">    LOG : str</span>
</span><span id="TotalMassEncodingTransformation-181"><a href="#TotalMassEncodingTransformation-181"><span class="linenos">181</span></a><span class="sd">        $f(x) = \\log(1 + x)$; stronger compression of large values.</span>
</span><span id="TotalMassEncodingTransformation-182"><a href="#TotalMassEncodingTransformation-182"><span class="linenos">182</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="TotalMassEncodingTransformation-183"><a href="#TotalMassEncodingTransformation-183"><span class="linenos">183</span></a>    <span class="n">IDENTITY</span> <span class="o">=</span> <span class="s1">&#39;identity&#39;</span>
</span><span id="TotalMassEncodingTransformation-184"><a href="#TotalMassEncodingTransformation-184"><span class="linenos">184</span></a>    <span class="n">SQRT</span> <span class="o">=</span> <span class="s1">&#39;sqrt&#39;</span>
</span><span id="TotalMassEncodingTransformation-185"><a href="#TotalMassEncodingTransformation-185"><span class="linenos">185</span></a>    <span class="n">LOG</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span>
</span></pre></div>


            <div class="docstring"><p>Transformation applied to the total mass before incorporating into the embedding.</p>

<p>Each option defines a different transformation applied to the total mass of an input measure/multiset
before it is incorporated into the embedding vector.</p>

<h6 id="attributes">Attributes</h6>

<ul>
<li><strong>IDENTITY</strong> (str):
$f(x) = x$; no transformation.</li>
<li><strong>SQRT</strong> (str):
$f(x) = \sqrt{1 + x} - 1$; mild nonlinearity.</li>
<li><strong>LOG</strong> (str):
$f(x) = \log(1 + x)$; stronger compression of large values.</li>
</ul>
</div>


                            <div id="TotalMassEncodingTransformation.IDENTITY" class="classattr">
                                <div class="attr variable">
            <span class="name">IDENTITY</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingTransformation.IDENTITY">TotalMassEncodingTransformation.IDENTITY</a>: &#39;identity&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingTransformation.IDENTITY"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingTransformation.SQRT" class="classattr">
                                <div class="attr variable">
            <span class="name">SQRT</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingTransformation.SQRT">TotalMassEncodingTransformation.SQRT</a>: &#39;sqrt&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingTransformation.SQRT"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingTransformation.LOG" class="classattr">
                                <div class="attr variable">
            <span class="name">LOG</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingTransformation.LOG">TotalMassEncodingTransformation.LOG</a>: &#39;log&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingTransformation.LOG"></a>
    
    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#EnumWithResolve">EnumWithResolve</a></dt>
                                <dd id="TotalMassEncodingTransformation.resolve" class="function"><a href="#EnumWithResolve.resolve">resolve</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="TotalMassEncodingMethod">
                            <input id="TotalMassEncodingMethod-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">TotalMassEncodingMethod</span><wbr>(<span class="base"><a href="#EnumWithResolve">EnumWithResolve</a></span>):

                <label class="view-source-button" for="TotalMassEncodingMethod-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TotalMassEncodingMethod-189"><a href="#TotalMassEncodingMethod-189"><span class="linenos">189</span></a><span class="k">class</span><span class="w"> </span><span class="nc">TotalMassEncodingMethod</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="TotalMassEncodingMethod-190"><a href="#TotalMassEncodingMethod-190"><span class="linenos">190</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="TotalMassEncodingMethod-191"><a href="#TotalMassEncodingMethod-191"><span class="linenos">191</span></a><span class="sd">    Strategies for incorporating total mass into the embedding.</span>
</span><span id="TotalMassEncodingMethod-192"><a href="#TotalMassEncodingMethod-192"><span class="linenos">192</span></a>
</span><span id="TotalMassEncodingMethod-193"><a href="#TotalMassEncodingMethod-193"><span class="linenos">193</span></a><span class="sd">    Each method defines a different way of incorporating the total mass $\\mu\\left(\\Omega\\right) = \\sum_{i=1}^n w_i$ of an input measure</span>
</span><span id="TotalMassEncodingMethod-194"><a href="#TotalMassEncodingMethod-194"><span class="linenos">194</span></a><span class="sd">    $\\mu = \\sum_{i=1}^n w_i \\delta_{\\mathbf{x}^{(i)}}$ (i.e. the multiset size if $\\mu$ is a multiset) with the FSW embedding of the normalized input $\\mu_{\\rho}$ into a single output vector.</span>
</span><span id="TotalMassEncodingMethod-195"><a href="#TotalMassEncodingMethod-195"><span class="linenos">195</span></a><span class="sd">    For further discussion, see Appendix A.1 of the reference below.</span>
</span><span id="TotalMassEncodingMethod-196"><a href="#TotalMassEncodingMethod-196"><span class="linenos">196</span></a>
</span><span id="TotalMassEncodingMethod-197"><a href="#TotalMassEncodingMethod-197"><span class="linenos">197</span></a><span class="sd">    Attributes</span>
</span><span id="TotalMassEncodingMethod-198"><a href="#TotalMassEncodingMethod-198"><span class="linenos">198</span></a><span class="sd">    ----------</span>
</span><span id="TotalMassEncodingMethod-199"><a href="#TotalMassEncodingMethod-199"><span class="linenos">199</span></a><span class="sd">    DECOUPLED : str</span>
</span><span id="TotalMassEncodingMethod-200"><a href="#TotalMassEncodingMethod-200"><span class="linenos">200</span></a><span class="sd">        The total mass is appended as a separate component to the embedding vector,</span>
</span><span id="TotalMassEncodingMethod-201"><a href="#TotalMassEncodingMethod-201"><span class="linenos">201</span></a><span class="sd">        which is computed from the normalized input measure, as in Equation (18)</span>
</span><span id="TotalMassEncodingMethod-202"><a href="#TotalMassEncodingMethod-202"><span class="linenos">202</span></a><span class="sd">        in our paper:</span>
</span><span id="TotalMassEncodingMethod-203"><a href="#TotalMassEncodingMethod-203"><span class="linenos">203</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="TotalMassEncodingMethod-204"><a href="#TotalMassEncodingMethod-204"><span class="linenos">204</span></a>
</span><span id="TotalMassEncodingMethod-205"><a href="#TotalMassEncodingMethod-205"><span class="linenos">205</span></a><span class="sd">    SCALED : str</span>
</span><span id="TotalMassEncodingMethod-206"><a href="#TotalMassEncodingMethod-206"><span class="linenos">206</span></a><span class="sd">        Similar to `DECOUPLED`, but the embedding of the normalized input is scaled</span>
</span><span id="TotalMassEncodingMethod-207"><a href="#TotalMassEncodingMethod-207"><span class="linenos">207</span></a><span class="sd">        by the total mass. Using the notation of Equation (18), this yields:</span>
</span><span id="TotalMassEncodingMethod-208"><a href="#TotalMassEncodingMethod-208"><span class="linenos">208</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="TotalMassEncodingMethod-209"><a href="#TotalMassEncodingMethod-209"><span class="linenos">209</span></a>
</span><span id="TotalMassEncodingMethod-210"><a href="#TotalMassEncodingMethod-210"><span class="linenos">210</span></a><span class="sd">    HOMOGENEOUS : str</span>
</span><span id="TotalMassEncodingMethod-211"><a href="#TotalMassEncodingMethod-211"><span class="linenos">211</span></a><span class="sd">        A method that encodes the total mass while preserving homogeneity</span>
</span><span id="TotalMassEncodingMethod-212"><a href="#TotalMassEncodingMethod-212"><span class="linenos">212</span></a><span class="sd">        with respect to the elements of the input multiset. See Equation (19).</span>
</span><span id="TotalMassEncodingMethod-213"><a href="#TotalMassEncodingMethod-213"><span class="linenos">213</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert \\cdot \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="TotalMassEncodingMethod-214"><a href="#TotalMassEncodingMethod-214"><span class="linenos">214</span></a>
</span><span id="TotalMassEncodingMethod-215"><a href="#TotalMassEncodingMethod-215"><span class="linenos">215</span></a><span class="sd">    HOMOGENEOUS_SCALED : str</span>
</span><span id="TotalMassEncodingMethod-216"><a href="#TotalMassEncodingMethod-216"><span class="linenos">216</span></a><span class="sd">        Similar to `SCALED`, but preserves homogeneity.</span>
</span><span id="TotalMassEncodingMethod-217"><a href="#TotalMassEncodingMethod-217"><span class="linenos">217</span></a><span class="sd">        $$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert, \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</span>
</span><span id="TotalMassEncodingMethod-218"><a href="#TotalMassEncodingMethod-218"><span class="linenos">218</span></a>
</span><span id="TotalMassEncodingMethod-219"><a href="#TotalMassEncodingMethod-219"><span class="linenos">219</span></a><span class="sd">    HOMOGENEOUS_LEGACY : str</span>
</span><span id="TotalMassEncodingMethod-220"><a href="#TotalMassEncodingMethod-220"><span class="linenos">220</span></a><span class="sd">        An alternative, legacy version of the homogeneous method, retained</span>
</span><span id="TotalMassEncodingMethod-221"><a href="#TotalMassEncodingMethod-221"><span class="linenos">221</span></a><span class="sd">        for reference and compatibility.</span>
</span><span id="TotalMassEncodingMethod-222"><a href="#TotalMassEncodingMethod-222"><span class="linenos">222</span></a>
</span><span id="TotalMassEncodingMethod-223"><a href="#TotalMassEncodingMethod-223"><span class="linenos">223</span></a><span class="sd">    Notes</span>
</span><span id="TotalMassEncodingMethod-224"><a href="#TotalMassEncodingMethod-224"><span class="linenos">224</span></a><span class="sd">    -----</span>
</span><span id="TotalMassEncodingMethod-225"><a href="#TotalMassEncodingMethod-225"><span class="linenos">225</span></a><span class="sd">    In practice, $\\mu\\left(\\Omega\\right)$ in the above expressions is replaced by $\\alpha \\cdot f \\left( \\mu\\left(\\Omega\\right) \\right)$,</span>
</span><span id="TotalMassEncodingMethod-226"><a href="#TotalMassEncodingMethod-226"><span class="linenos">226</span></a><span class="sd">    where $f$ is the function defined by `TotalMassEncodingTransformation` and $\\alpha$ is a scale factor given in `total_mass_encoding_scale`.</span>
</span><span id="TotalMassEncodingMethod-227"><a href="#TotalMassEncodingMethod-227"><span class="linenos">227</span></a><span class="sd">    Additionally, $\\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert$ is multiplied by a normalizing factor $\\sqrt{m-1}^{-1}$.</span>
</span><span id="TotalMassEncodingMethod-228"><a href="#TotalMassEncodingMethod-228"><span class="linenos">228</span></a>
</span><span id="TotalMassEncodingMethod-229"><a href="#TotalMassEncodingMethod-229"><span class="linenos">229</span></a><span class="sd">    Reference</span>
</span><span id="TotalMassEncodingMethod-230"><a href="#TotalMassEncodingMethod-230"><span class="linenos">230</span></a><span class="sd">    ---------</span>
</span><span id="TotalMassEncodingMethod-231"><a href="#TotalMassEncodingMethod-231"><span class="linenos">231</span></a><span class="sd">    Tal Amir, Nadav Dym.</span>
</span><span id="TotalMassEncodingMethod-232"><a href="#TotalMassEncodingMethod-232"><span class="linenos">232</span></a><span class="sd">    &quot;Fourier Sliced-Wasserstein Embedding for Multisets and Measures.&quot;</span>
</span><span id="TotalMassEncodingMethod-233"><a href="#TotalMassEncodingMethod-233"><span class="linenos">233</span></a><span class="sd">    International Conference on Learning Representations (ICLR), 2025.</span>
</span><span id="TotalMassEncodingMethod-234"><a href="#TotalMassEncodingMethod-234"><span class="linenos">234</span></a><span class="sd">    https://iclr.cc/virtual/2025/poster/30562</span>
</span><span id="TotalMassEncodingMethod-235"><a href="#TotalMassEncodingMethod-235"><span class="linenos">235</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="TotalMassEncodingMethod-236"><a href="#TotalMassEncodingMethod-236"><span class="linenos">236</span></a>    <span class="n">DECOUPLED</span> <span class="o">=</span> <span class="s1">&#39;decoupled&#39;</span>
</span><span id="TotalMassEncodingMethod-237"><a href="#TotalMassEncodingMethod-237"><span class="linenos">237</span></a>    <span class="n">SCALED</span> <span class="o">=</span> <span class="s1">&#39;scaled&#39;</span>
</span><span id="TotalMassEncodingMethod-238"><a href="#TotalMassEncodingMethod-238"><span class="linenos">238</span></a>    <span class="n">HOMOGENEOUS</span> <span class="o">=</span> <span class="s1">&#39;homogeneous&#39;</span>
</span><span id="TotalMassEncodingMethod-239"><a href="#TotalMassEncodingMethod-239"><span class="linenos">239</span></a>    <span class="n">HOMOGENEOUS_SCALED</span> <span class="o">=</span> <span class="s1">&#39;homogeneous_scaled&#39;</span>
</span><span id="TotalMassEncodingMethod-240"><a href="#TotalMassEncodingMethod-240"><span class="linenos">240</span></a>    <span class="n">HOMOGENEOUS_LEGACY</span> <span class="o">=</span> <span class="s1">&#39;homogeneous_legacy&#39;</span>
</span></pre></div>


            <div class="docstring"><p>Strategies for incorporating total mass into the embedding.</p>

<p>Each method defines a different way of incorporating the total mass $\mu\left(\Omega\right) = \sum_{i=1}^n w_i$ of an input measure
$\mu = \sum_{i=1}^n w_i \delta_{\mathbf{x}^{(i)}}$ (i.e. the multiset size if $\mu$ is a multiset) with the FSW embedding of the normalized input $\mu_{\rho}$ into a single output vector.
For further discussion, see Appendix A.1 of the reference below.</p>

<h6 id="attributes">Attributes</h6>

<ul>
<li><strong>DECOUPLED</strong> (str):
The total mass is appended as a separate component to the embedding vector,
which is computed from the normalized input measure, as in Equation (18)
in our paper:
$$ \hat{E}^{\textup{FSW}}_{m}\left(\mu\right) = \left[ \mu\left(\Omega\right), \;  E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \right] $$</li>
<li><strong>SCALED</strong> (str):
Similar to <code><a href="#TotalMassEncodingMethod.DECOUPLED">DECOUPLED</a></code>, but the embedding of the normalized input is scaled
by the total mass. Using the notation of Equation (18), this yields:
$$ \hat{E}^{\textup{FSW}}_{m}\left(\mu\right) = \left[ \mu\left(\Omega\right), \;  \mu\left(\Omega\right) \cdot E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \right] $$</li>
<li><strong>HOMOGENEOUS</strong> (str):
A method that encodes the total mass while preserving homogeneity
with respect to the elements of the input multiset. See Equation (19).
$$ \hat{E}^{\textup{FSW}}_{m}\left(\mu\right) = \left[ \lVert E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \rVert \cdot \mu\left(\Omega\right), \;  E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \right] $$</li>
<li><strong>HOMOGENEOUS_SCALED</strong> (str):
Similar to <code><a href="#TotalMassEncodingMethod.SCALED">SCALED</a></code>, but preserves homogeneity.
$$ \hat{E}^{\textup{FSW}}_{m}\left(\mu\right) = \left[ \lVert E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \rVert, \;  \mu\left(\Omega\right) \cdot E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \right] $$</li>
<li><strong>HOMOGENEOUS_LEGACY</strong> (str):
An alternative, legacy version of the homogeneous method, retained
for reference and compatibility.</li>
</ul>

<h6 id="notes">Notes</h6>

<p>In practice, $\mu\left(\Omega\right)$ in the above expressions is replaced by $\alpha \cdot f \left( \mu\left(\Omega\right) \right)$,
where $f$ is the function defined by <code><a href="#TotalMassEncodingTransformation">TotalMassEncodingTransformation</a></code> and $\alpha$ is a scale factor given in <code>total_mass_encoding_scale</code>.
Additionally, $\lVert E^{\textup{FSW}}_{m-1}\left(\mu_{\rho}\right) \rVert$ is multiplied by a normalizing factor $\sqrt{m-1}^{-1}$.</p>

<h6 id="reference">Reference</h6>

<p>Tal Amir, Nadav Dym.
"Fourier Sliced-Wasserstein Embedding for Multisets and Measures."
International Conference on Learning Representations (ICLR), 2025.
<a href="https://iclr.cc/virtual/2025/poster/30562">https://iclr.cc/virtual/2025/poster/30562</a></p>
</div>


                            <div id="TotalMassEncodingMethod.DECOUPLED" class="classattr">
                                <div class="attr variable">
            <span class="name">DECOUPLED</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingMethod.DECOUPLED">TotalMassEncodingMethod.DECOUPLED</a>: &#39;decoupled&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod.DECOUPLED"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingMethod.SCALED" class="classattr">
                                <div class="attr variable">
            <span class="name">SCALED</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingMethod.SCALED">TotalMassEncodingMethod.SCALED</a>: &#39;scaled&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod.SCALED"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingMethod.HOMOGENEOUS" class="classattr">
                                <div class="attr variable">
            <span class="name">HOMOGENEOUS</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingMethod.HOMOGENEOUS">TotalMassEncodingMethod.HOMOGENEOUS</a>: &#39;homogeneous&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod.HOMOGENEOUS"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingMethod.HOMOGENEOUS_SCALED" class="classattr">
                                <div class="attr variable">
            <span class="name">HOMOGENEOUS_SCALED</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingMethod.HOMOGENEOUS_SCALED">TotalMassEncodingMethod.HOMOGENEOUS_SCALED</a>: &#39;homogeneous_scaled&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod.HOMOGENEOUS_SCALED"></a>
    
    

                            </div>
                            <div id="TotalMassEncodingMethod.HOMOGENEOUS_LEGACY" class="classattr">
                                <div class="attr variable">
            <span class="name">HOMOGENEOUS_LEGACY</span>        =
<span class="default_value">&lt;<a href="#TotalMassEncodingMethod.HOMOGENEOUS_LEGACY">TotalMassEncodingMethod.HOMOGENEOUS_LEGACY</a>: &#39;homogeneous_legacy&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#TotalMassEncodingMethod.HOMOGENEOUS_LEGACY"></a>
    
    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#EnumWithResolve">EnumWithResolve</a></dt>
                                <dd id="TotalMassEncodingMethod.resolve" class="function"><a href="#EnumWithResolve.resolve">resolve</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="FrequencyInitMethod">
                            <input id="FrequencyInitMethod-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FrequencyInitMethod</span><wbr>(<span class="base"><a href="#EnumWithResolve">EnumWithResolve</a></span>):

                <label class="view-source-button" for="FrequencyInitMethod-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FrequencyInitMethod"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FrequencyInitMethod-244"><a href="#FrequencyInitMethod-244"><span class="linenos">244</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FrequencyInitMethod</span><span class="p">(</span><span class="n">EnumWithResolve</span><span class="p">):</span>
</span><span id="FrequencyInitMethod-245"><a href="#FrequencyInitMethod-245"><span class="linenos">245</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="FrequencyInitMethod-246"><a href="#FrequencyInitMethod-246"><span class="linenos">246</span></a><span class="sd">    Method for initializing frequencies in the FSW embedding.</span>
</span><span id="FrequencyInitMethod-247"><a href="#FrequencyInitMethod-247"><span class="linenos">247</span></a>
</span><span id="FrequencyInitMethod-248"><a href="#FrequencyInitMethod-248"><span class="linenos">248</span></a><span class="sd">    This enumeration specifies how the frequencies in the FSW embedding are</span>
</span><span id="FrequencyInitMethod-249"><a href="#FrequencyInitMethod-249"><span class="linenos">249</span></a><span class="sd">    initialized.</span>
</span><span id="FrequencyInitMethod-250"><a href="#FrequencyInitMethod-250"><span class="linenos">250</span></a>
</span><span id="FrequencyInitMethod-251"><a href="#FrequencyInitMethod-251"><span class="linenos">251</span></a><span class="sd">    Attributes</span>
</span><span id="FrequencyInitMethod-252"><a href="#FrequencyInitMethod-252"><span class="linenos">252</span></a><span class="sd">    ----------</span>
</span><span id="FrequencyInitMethod-253"><a href="#FrequencyInitMethod-253"><span class="linenos">253</span></a><span class="sd">    RANDOM : str</span>
</span><span id="FrequencyInitMethod-254"><a href="#FrequencyInitMethod-254"><span class="linenos">254</span></a><span class="sd">        Frequencies are sampled independently at random from the distribution</span>
</span><span id="FrequencyInitMethod-255"><a href="#FrequencyInitMethod-255"><span class="linenos">255</span></a><span class="sd">        D_ξ, as defined in Section 3 of our paper.</span>
</span><span id="FrequencyInitMethod-256"><a href="#FrequencyInitMethod-256"><span class="linenos">256</span></a><span class="sd">    EVEN : str</span>
</span><span id="FrequencyInitMethod-257"><a href="#FrequencyInitMethod-257"><span class="linenos">257</span></a><span class="sd">        Frequencies are spaced deterministically for efficient coverage of the frequency domain,</span>
</span><span id="FrequencyInitMethod-258"><a href="#FrequencyInitMethod-258"><span class="linenos">258</span></a><span class="sd">        with spacing inversely proportional to the density function f_ξ.</span>
</span><span id="FrequencyInitMethod-259"><a href="#FrequencyInitMethod-259"><span class="linenos">259</span></a>
</span><span id="FrequencyInitMethod-260"><a href="#FrequencyInitMethod-260"><span class="linenos">260</span></a><span class="sd">    See Also</span>
</span><span id="FrequencyInitMethod-261"><a href="#FrequencyInitMethod-261"><span class="linenos">261</span></a><span class="sd">    --------</span>
</span><span id="FrequencyInitMethod-262"><a href="#FrequencyInitMethod-262"><span class="linenos">262</span></a><span class="sd">    FSWEmbedding.__init__ : Where this method is selected and used.</span>
</span><span id="FrequencyInitMethod-263"><a href="#FrequencyInitMethod-263"><span class="linenos">263</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="FrequencyInitMethod-264"><a href="#FrequencyInitMethod-264"><span class="linenos">264</span></a>
</span><span id="FrequencyInitMethod-265"><a href="#FrequencyInitMethod-265"><span class="linenos">265</span></a>    <span class="n">RANDOM</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span>
</span><span id="FrequencyInitMethod-266"><a href="#FrequencyInitMethod-266"><span class="linenos">266</span></a>    <span class="n">EVEN</span> <span class="o">=</span> <span class="s2">&quot;even&quot;</span>
</span></pre></div>


            <div class="docstring"><p>Method for initializing frequencies in the FSW embedding.</p>

<p>This enumeration specifies how the frequencies in the FSW embedding are
initialized.</p>

<h6 id="attributes">Attributes</h6>

<ul>
<li><strong>RANDOM</strong> (str):
Frequencies are sampled independently at random from the distribution
D_ξ, as defined in Section 3 of our paper.</li>
<li><strong>EVEN</strong> (str):
Frequencies are spaced deterministically for efficient coverage of the frequency domain,
with spacing inversely proportional to the density function f_ξ.</li>
</ul>

<h6 id="see-also">See Also</h6>

<p><code><a href="#FSWEmbedding.__init__">FSWEmbedding.__init__</a></code>:  Where this method is selected and used.  </p>
</div>


                            <div id="FrequencyInitMethod.RANDOM" class="classattr">
                                <div class="attr variable">
            <span class="name">RANDOM</span>        =
<span class="default_value">&lt;<a href="#FrequencyInitMethod.RANDOM">FrequencyInitMethod.RANDOM</a>: &#39;random&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#FrequencyInitMethod.RANDOM"></a>
    
    

                            </div>
                            <div id="FrequencyInitMethod.EVEN" class="classattr">
                                <div class="attr variable">
            <span class="name">EVEN</span>        =
<span class="default_value">&lt;<a href="#FrequencyInitMethod.EVEN">FrequencyInitMethod.EVEN</a>: &#39;even&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#FrequencyInitMethod.EVEN"></a>
    
    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt><a href="#EnumWithResolve">EnumWithResolve</a></dt>
                                <dd id="FrequencyInitMethod.resolve" class="function"><a href="#EnumWithResolve.resolve">resolve</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>