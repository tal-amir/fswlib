window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "fswlib.fsw_embedding.fsw_embedding", "modulename": "fswlib.fsw_embedding.fsw_embedding", "kind": "module", "doc": "<p>fsw_embedding.py</p>\n\n<p>Main Python module for computing the Fourier Sliced-Wasserstein (FSW) embedding.</p>\n\n<p>Authors:\n    Tal Amir, Nadav Dym\n    Technion \u2013 Israel Institute of Technology</p>\n\n<p>This code is based on our paper:\n    \"Fourier Sliced-Wasserstein Embedding for Multisets and Measures\"\n    Tal Amir, Nadav Dym\n    International Conference on Learning Representations (ICLR), 2025</p>\n\n<p>Paper URL:   <a href=\"https://iclr.cc/virtual/2025/poster/30562\">https://iclr.cc/virtual/2025/poster/30562</a>\nProject URL: <a href=\"https://github.com/tal-amir/fswlib\">https://github.com/tal-amir/fswlib</a></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.version", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "version", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;2.2&#x27;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.version_date", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "version_date", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;2025-05-30&#x27;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.fsw_embedding_debug_mode", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "fsw_embedding_debug_mode", "kind": "variable", "doc": "<p></p>\n", "default_value": "False"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.fsw_embedding_basic_safety_checks", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "fsw_embedding_basic_safety_checks", "kind": "variable", "doc": "<p></p>\n", "default_value": "True"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.fsw_embedding_high_precision", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "fsw_embedding_high_precision", "kind": "variable", "doc": "<p></p>\n", "default_value": "False"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.tal_global_timer", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "tal_global_timer", "kind": "variable", "doc": "<p></p>\n", "default_value": "0"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.tal_global_timer_start", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "tal_global_timer_start", "kind": "variable", "doc": "<p>Maps multisets in R^d_in to vectors in R^d_out using the Fourier Sliced-Wasserstrin Embedding.\nAlso supports weighted point-clouds in R^d_in, which are regarded as discrete distributions over R^d_in.</p>\n\n<p>The Euclidean distance between two embeddings approximates the Sliced-Wasserstein distance\ndistance the input distributions:\n            ||embed(X1,W1)-embed(X2,W2)||_2  =<approx>=  sqrt(d_out) * SW((X1,W1),(X2,W2))</p>\n\n<p>To guarantee that the embedding is injective with at most n input points, use\nd_out &gt;= 2*n*d_in+1 for multisets and d_out &gt;= 2n(d_in+1)+1 for distributions.</p>\n\n<p>The input point tensor X should be of size (<batch_dims>, n, d_in), where <batch_dims> can be\nany list of dimensions. The accompanying weights W should be of size (<batch_dims>, n).\nThe output is of size (<batch_dims>, d_out).</p>\n\n<p>If W is not provided, all weights are assumed to be uniform 1/n, with n being the number\nof points in X.</p>\n\n<p>The weights should be non-negative. They do not have to be normalized, as they are normalized\ninternally, but they should have a positive sum. </p>\n\n<h2 id=\"graph-mode\">Graph mode</h2>\n\n<p>If graph_mode=True, W is treated as a conjugacy matrix of a graph with vertex features given in X.\nIf X is of size (<batch_dims>, n, d_in), then W should be of size (<batch_dims>, nRecipients, n),\nwith W[<batch_indices>, i, j] containing the weight of the edge from vertex j to vertex i. \nThe output is then of size (<batch_dims>, nRecipients, d_out), with each vector output[<batch indices>, i, :]\nholding the embedding of all feature vectors of neighbors of vertex i, with the corresponding weights being\nthe weights of the edges leading from them to i.</p>\n\n<p>Note that W does not have to be square; hence the number of message recipients needs not be equal to the number\nof senders.</p>\n\n<h2 id=\"cartesian-mode\">Cartesian mode</h2>\n\n<p>If d_out=None and instead nSlices and nFreqs are provided, the embedding is computed with a Cartesian product of the\nslices and frequencies. The output shape is then (<batch_dims>, nSlices, nFreqs), or in graph mode\n(<batch_dims>, nRecipients, nSlices, nFreqs).</p>\n\n<p>If collapse_freqs=True, then the frequency axis is collaped to the slice axis, resulting in output size\n(<batch_dims>, nSlices x nFreqs), or in graph mode (<batch_dims>, nRecipients, nSlices x nFreqs).</p>\n\n<h2 id=\"sparse-w\">Sparse W</h2>\n\n<p>The input W can be sparse. In some use cases this could lead to a considerable reduction in running time and memory\ncomplexity. The most common use scenario is in graph mode, when W represents the adjacency matrix of a graph with\na large number of vertices and a relatively low number of edges.</p>\n", "default_value": "0"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding", "kind": "class", "doc": "<p>Embedding module implementing the Fourier Sliced-Wasserstein (FSW) embedding.</p>\n\n<p>This module computes a fixed-length representation of input multisets or measures\nby projecting them onto random (or learned) one-dimensional slices, applying\na variant of the Sliced Wasserstein transform in the Fourier domain.</p>\n\n<p>Designed for use with PyTorch, and optionally supports a custom CUDA extension\nfor faster computation on sparse inputs.</p>\n\n<p>See the ICLR 2025 paper \"Fourier Sliced-Wasserstein Embedding for Multisets and Measures\"\nby Tal Amir and Nadav Dym for details.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.__init__", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.__init__", "kind": "function", "doc": "<p>Initialize the FSWEmbedding module.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>d_in : int\n    Dimensionality of input multiset elements (ambient dimension).\nd_out : int or None, optional\n    Output embedding dimension. If None, defaults to <code>nSlices * nFreqs</code>.\nnSlices : int or None, optional\n    Number of projection directions. Required if <code>d_out</code> is not specified.\nnFreqs : int or None, optional\n    Number of Fourier frequencies per slice. Required if <code>d_out</code> is not specified.\ncollapse_freqs : bool, default=False\n    If True, aggregates over frequencies within each slice instead of flattening them.\nd_edge : int, default=0\n    Dimensionality of edge features (used only when <code>graph_mode=True</code> in forward).\nencode_total_mass : bool, default=False\n    If True, encodes the total mass (multiset size) into the embedding.\ntotal_mass_encoding_function : {'identity', 'sqrt', 'log'}, default='identity'\n    Function used to encode the total mass.\ntotal_mass_encoding_method : {'plain', 'homog', 'homog_alt'}, default='plain'\n    Method for incorporating total mass information into the embedding.\ntotal_mass_pad_thresh : float, default=1.0\n    Threshold below which mass is padded with a point mass at the origin.\nlearnable_slices : bool, default=False\n    If True, the projection directions are learned during training.\nlearnable_freqs : bool, default=False\n    If True, the Fourier frequencies are learned during training.\nlearnable_powers : bool, default=False\n    If True, learns power parameters applied to slice outputs.\nfreqs_init : float, int, str, or tuple, default='random'\n    Initialization scheme for frequencies. Can be a fixed value, range, or strategy.\nminimize_slice_coherence : bool, default=False\n    If True, encourages slice directions to be mutually incoherent.\nenable_bias : bool, default=True\n    If True, adds a bias term to the final embedding.\ndevice : torch.device, int, str, or None, optional\n    Device to place all module parameters and buffers on.\ndtype : torch.dtype or None, optional\n    Data type for computations (e.g., torch.float32 or torch.float64).\nuse_custom_cuda_extension_if_available : bool or None, optional\n    If True, uses the custom CUDA extension if available and supported.\nfail_if_cuda_extension_load_fails : bool, default=False\n    If True, raises a RuntimeError if the CUDA extension cannot be loaded.\nreport : bool, default=False\n    If True, prints internal details during initialization.\nuser_warnings : bool, default=True\n    If True, shows warnings to the user (e.g., fallback from CUDA extension).\nreport_on_coherence_minimization : bool, default=False\n    If True, logs progress during slice coherence minimization.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">d_in</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">d_out</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">nSlices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">nFreqs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">collapse_freqs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">d_edge</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">encode_total_mass</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_function</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;identity&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;plain&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_pad_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_slices</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_freqs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_powers</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">freqs_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;random&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">enable_bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">user_warnings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.user_warnings", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.user_warnings", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.d_in", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.d_in", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.d_edge", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.d_edge", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.encode_total_mass", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.encode_total_mass", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_dim", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_pad_thresh", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_pad_thresh", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_method", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_method", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_function", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_function", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.minimize_slice_coherence", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.minimize_slice_coherence", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.learnable_slices", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.learnable_slices", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.learnable_freqs", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.learnable_freqs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.learnable_powers", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.learnable_powers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.freqs_init", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.freqs_init", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.enable_bias", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.enable_bias", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.device_new", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.device_new", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.dtype_new", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.dtype_new", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.use_custom_cuda_extension_if_available", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.use_custom_cuda_extension_if_available", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.fail_if_cuda_extension_load_fails", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.fail_if_cuda_extension_load_fails", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.report", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.report", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.report_on_coherence_minimization", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.report_on_coherence_minimization", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.projVecs", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.projVecs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.freqs", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.freqs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.bias", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.bias", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.reset_parameters", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.reset_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">freqs_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.to", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.to", "kind": "function", "doc": "<p>Move and/or cast the parameters and buffers.</p>\n\n<p>This can be called as</p>\n\n<p>.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:</p>\n\n<p>.. function:: to(dtype, non_blocking=False)\n   :noindex:</p>\n\n<p>.. function:: to(tensor, non_blocking=False)\n   :noindex:</p>\n\n<p>.. function:: to(memory_format=torch.channels_last)\n   :noindex:</p>\n\n<p>Its signature is similar to <code>torch.Tensor.to()</code>, but only accepts\nfloating point or complex <code>dtype</code>\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to <code>dtype</code>\n(if given). The integral parameters and buffers will be moved\n<code>device</code>, if that is given, but with dtypes unchanged. When\n<code>non_blocking</code> is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.</p>\n\n<p>See below for examples.</p>\n\n<div class=\"alert note\">\n\n<p>This method modifies the module in-place.</p>\n\n</div>\n\n<p>Args:\n    device (<code>torch.device</code>): the desired device of the parameters\n        and buffers in this module\n    dtype (<code>torch.dtype</code>): the desired floating point or complex dtype of\n        the parameters and buffers in this module\n    tensor (torch.Tensor): Tensor whose dtype and device are the desired\n        dtype and device for all parameters and buffers in this module\n    memory_format (<code>torch.memory_format</code>): the desired memory\n        format for 4D parameters and buffers in this module (keyword\n        only argument)</p>\n\n<p>Returns:\n    Module: self</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.get_device", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.get_device", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.get_dtype", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.get_dtype", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.generate_embedding_parameters", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.generate_embedding_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">d_in</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">nSlices</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">nFreqs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">cartesian_mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">collapse_freqs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">freqs_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.spread_freqs_at_interval", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.spread_freqs_at_interval", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">center</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">radius</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.forward", "kind": "function", "doc": "<p>Compute the Fourier Sliced-Wasserstein embedding of input multisets, measures, or graphs.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>X : torch.Tensor\n    Input tensor of shape <code>(n, d_in)</code> representing a multiset of <code>n</code> points in <code>\u211d^{d_in}</code>, or\n    of shape <code>(&lt;batch_dims&gt;, n, d_in)</code> for a batch of input multisets.\n    In <code>graph_mode=True</code>, may also be of shape <code>(&lt;batch_dims[:-1]&gt;, d_in)</code> to share input points across the last batch dimension.</p>\n\n<p>W : torch.Tensor or str, default='unit'\n    Weights tensor of shape <code>(n,)</code> or <code>(&lt;batch_dims&gt;, n)</code>, assigning non-negative weights to input points.\n    If set to <code>'unit'</code>, the input is treated as a uniform distribution over its elements.\n    Must contain at least one non-zero weight per multiset.</p>\n\n<p>X_edge : torch.Tensor or None, optional\n    Optional tensor of edge features. Currently unused.</p>\n\n<p>graph_mode : bool, default=False\n    If True, interprets the input as a graph structure.\n    Each distribution is computed using a shared set of points <code>X</code>, with weights drawn from corresponding rows of <code>W</code>.\n    Allows efficient application to graphs, e.g., computing an embedding of the neighborhood of each node.</p>\n\n<p>serialize_num_slices : int or None, optional\n    If set to an integer <code>t</code>, serializes the computation over projection slices in batches of size <code>t</code>.\n    This reduces memory usage by a factor of <code>&lt;num_slices&gt; / t</code> without changing the result.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>X_emb : torch.Tensor\n    Output tensor of shape:\n      - <code>(&lt;batch_dims&gt;, d_out)</code> if <code>collapse_freqs=True</code>, or\n      - <code>(&lt;batch_dims&gt;, nSlices, nFreqs)</code> if <code>collapse_freqs=False</code>.</p>\n\n<pre><code>If `graph_mode=True`, the output shape becomes `(n, d_out)` or `(&lt;batch_dims&gt;, n, d_out)`, depending on input shape.\n</code></pre>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Input points <code>X</code> and weights <code>W</code> can be batched across any number of leading dimensions.</li>\n<li>In <code>graph_mode</code>, <code>X</code> is shared across the last batch dimension, allowing efficient graph neighborhood embeddings.</li>\n<li>If <code>X_edge</code> is used, it must be compatible with the weight structure of <code>W</code> (not yet implemented).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">W</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;unit&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">X_edge</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">graph_mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">serialize_num_slices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.forward_helper", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.forward_helper", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">X</span>,</span><span class=\"param\">\t<span class=\"n\">W</span>,</span><span class=\"param\">\t<span class=\"n\">projVecs</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">graph_mode</span>,</span><span class=\"param\">\t<span class=\"n\">X_edge</span>,</span><span class=\"param\">\t<span class=\"n\">cartesian_mode</span>,</span><span class=\"param\">\t<span class=\"n\">batch_dims</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.get_mutual_coherence", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.get_mutual_coherence", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_homog_alt_encoding_part1", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_homog_alt_encoding_part1", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">totmass</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_homog_alt_encoding_part2", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_homog_alt_encoding_part2", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">totmass</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.timer_start", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "timer_start", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.timer_stop", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "timer_stop", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.assert_coalesced", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "assert_coalesced", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.diff_zeropad", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "diff_zeropad", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">X</span>, </span><span class=\"param\"><span class=\"n\">dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.replace_in_tuple", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "replace_in_tuple", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">T</span>, </span><span class=\"param\"><span class=\"n\">index</span>, </span><span class=\"param\"><span class=\"n\">value</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.qprint", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "qprint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">q</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>, </span><span class=\"param\"><span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.qprintln", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "qprintln", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">q</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>, </span><span class=\"param\"><span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.assign_at", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "assign_at", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">source</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">inds</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ifnone", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ifnone", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">a</span>, </span><span class=\"param\"><span class=\"n\">b</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.coalesce_grad", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.coalesce_grad", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.coalesce_grad.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.coalesce_grad.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.coalesce_grad.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.coalesce_grad.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">perms</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">broadcast_perms_dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse_vals", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse_vals", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse_vals.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse_vals.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">sparse_tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">perm</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.permute_sparse_vals.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.permute_sparse_vals.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sum_sparseToDense", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sum_sparseToDense", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sum_sparseToDense.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sum_sparseToDense.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sum_sparseToDense.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sum_sparseToDense.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_same_pattern", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_same_pattern", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_same_pattern.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_same_pattern.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_same_pattern.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_same_pattern.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_same_pattern", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_same_pattern", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_same_pattern.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_same_pattern.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">fac</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_same_pattern.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_same_pattern.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_sparse_dense", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_sparse_dense", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_sparse_dense.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_sparse_dense.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.mul_sparse_dense.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.mul_sparse_dense.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.div_sparse_dense", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.div_sparse_dense", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.div_sparse_dense.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.div_sparse_dense.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">A</span>,</span><span class=\"param\">\t<span class=\"n\">B</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.div_sparse_dense.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.div_sparse_dense.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_sparse_dense", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_sparse_dense", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_sparse_dense.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_sparse_dense.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.add_sparse_dense.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.add_sparse_dense.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.custom_lowclamp", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.custom_lowclamp", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.custom_lowclamp.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.custom_lowclamp.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.custom_lowclamp.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.custom_lowclamp.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sinc_cos_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sinc_cos_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sinc_cos_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sinc_cos_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sinc_cos_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sinc_cos_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.concat_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.concat_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.concat_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.concat_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.concat_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.concat_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_dense_dim", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_dense_dim", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_dense_dim.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_dense_dim.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.unsqueeze_dense_dim.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.unsqueeze_dense_dim.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.flatten_dense_dim", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.flatten_dense_dim", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.flatten_dense_dim.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.flatten_dense_dim.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.flatten_dense_dim.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.flatten_dense_dim.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">descending</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">aaa</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">descending</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.sort_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.sort_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">aaa</span><span class=\"p\">:</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.cumsum_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.cumsum_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.cumsum_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.cumsum_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.cumsum_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.cumsum_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.repmat_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.repmat_sparse", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.repmat_sparse.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.repmat_sparse.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">n</span>, </span><span class=\"param\"><span class=\"n\">dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.ag.repmat_sparse.backward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "ag.repmat_sparse.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.sparse_coo_tensor_coalesced", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.sparse_coo_tensor_coalesced", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">values</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.coalesce_unique", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.coalesce_unique", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.coalesce_repeated", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.coalesce_repeated", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">window_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.verify_coalescence", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.verify_coalescence", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">dense_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.to_sparse_full", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.to_sparse_full", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.same_shape_prod", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.same_shape_prod", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.same_shape_prod_", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.same_shape_prod_", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.ravel_index", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.ravel_index", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.sort_inds_vals", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.sort_inds_vals", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">values</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.div_sparse_dense", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.div_sparse_dense", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span>, </span><span class=\"param\"><span class=\"n\">B</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.sum_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.sum_sparse", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">A</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.cumsum_sparse", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.cumsum_sparse", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">A</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">slice_info</span>,</span><span class=\"param\">\t<span class=\"n\">reverse</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.sparse_flip", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.sparse_flip", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span>, </span><span class=\"param\"><span class=\"n\">dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.get_slice_info", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.get_slice_info", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">A</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span>,</span><span class=\"param\">\t<span class=\"n\">calc_nnz_per_slice</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.verify_slice_info", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.verify_slice_info", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">slice_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.dim_to_list", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.dim_to_list", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.get_broadcast_dims_B_to_A", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.get_broadcast_dims_B_to_A", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span>, </span><span class=\"param\"><span class=\"n\">B</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.permute", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.permute", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">A</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">perms</span>, </span><span class=\"param\"><span class=\"n\">broadcast_perms_dim</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">backward_mode</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.sp.dsinc", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "sp.dsinc", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">return_sinc</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWCustomCudaExtensionLoadWarning", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWCustomCudaExtensionLoadWarning", "kind": "class", "doc": "<p>Raised when the custom CUDA extension could not be loaded and the fallback torch code is used.</p>\n", "bases": "builtins.UserWarning"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWCustomCudaExtensionLoadError", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWCustomCudaExtensionLoadError", "kind": "class", "doc": "<p>Raised when the custom CUDA extension could not be loaded and fallback behavior is disabled.</p>\n", "bases": "builtins.RuntimeError"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.load_custom_cuda_extension", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "load_custom_cuda_extension", "kind": "function", "doc": "<p>Attempts to load the custom CUDA extension (libfsw_embedding.so).\nEmits a warning if loading fails, or raises an error depending on config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">fail_if_cuda_extension_load_fails</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>, </span><span class=\"param\"><span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.segcumsum", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "segcumsum", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">values</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">segment_ids</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">max_seg_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">in_place</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">thorough_verify_input</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.segcumsum_torch", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "segcumsum_torch", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">values</span>, </span><span class=\"param\"><span class=\"n\">segment_ids</span>, </span><span class=\"param\"><span class=\"n\">max_seg_size</span>, </span><span class=\"param\"><span class=\"n\">in_place</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.segcumsum_torch_main", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "segcumsum_torch_main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">values</span>, </span><span class=\"param\"><span class=\"n\">segment_ids</span>, </span><span class=\"param\"><span class=\"n\">max_seg_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.segcumsum_cuda", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "segcumsum_cuda", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">values</span>, </span><span class=\"param\"><span class=\"n\">segment_ids</span>, </span><span class=\"param\"><span class=\"n\">max_seg_size</span>, </span><span class=\"param\"><span class=\"n\">in_place</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.segcumsum_slow", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "segcumsum_slow", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">segment_ids</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.get_max_threads_per_block", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "get_max_threads_per_block", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">device_index</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.minimize_mutual_coherence", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "minimize_mutual_coherence", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">X_init</span>, </span><span class=\"param\"><span class=\"n\">report</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.minimize_mutual_coherence_p", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "minimize_mutual_coherence_p", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">X_init</span>, </span><span class=\"param\"><span class=\"n\">p</span>, </span><span class=\"param\"><span class=\"n\">step_size_init</span>, </span><span class=\"param\"><span class=\"n\">improvement_thresh</span>, </span><span class=\"param\"><span class=\"n\">nIter_max</span>, </span><span class=\"param\"><span class=\"n\">report</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.calc_G", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "calc_G", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">X</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.calc_mu_from_G", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "calc_mu_from_G", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">G</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.eval_G", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "eval_G", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">G</span>, </span><span class=\"param\"><span class=\"n\">p</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();