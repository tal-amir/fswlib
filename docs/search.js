window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "fswlib", "modulename": "fswlib", "kind": "module", "doc": "<h1 id=\"fswlib-a-pytorch-library-for-the-fourier-sliced-wasserstein-fsw-embedding\">fswlib: A PyTorch Library for the Fourier Sliced-Wasserstein (FSW) Embedding</h1>\n\n<p>This package provides an implementation of the <strong>Fourier Sliced-Wasserstein (FSW) embedding</strong>, introduced in our <a href=\"https://iclr.cc/virtual/2025/poster/30562\">ICLR 2025 paper</a></p>\n\n<blockquote>\n  <p><strong>Fourier Sliced-Wasserstein Embedding for Multisets and Measures</strong><br />\n  Tal Amir &amp; Nadav Dym<br />\n  <em>International Conference on Learning Representations (ICLR)</em>, 2025</p>\n</blockquote>\n\n<hr />\n\n<h2 id=\"requirements\">\ud83d\udce6 Requirements</h2>\n\n<ul>\n<li><strong>Python</strong> \u2265 3.10.3 (released March 2022)  </li>\n<li><strong>PyTorch</strong> \u2265 2.1.0 (released October 2023)  </li>\n<li><strong>NumPy</strong> \u2265 1.24.4 (released June 2023)  </li>\n</ul>\n\n<p>The core package has been tested on <strong>Linux</strong> and <strong>Windows</strong>.<br />\nIt may also run on <strong>macOS</strong>, though this has not been verified.  </p>\n\n<hr />\n\n<h2 id=\"installation\">\ud83d\udd27 Installation</h2>\n\n<p>To install the package:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>fswlib\n</code></pre>\n</div>\n\n<p>The core package runs on both <strong>CPU</strong> and <strong>CUDA-enabled GPUs</strong>, using PyTorch's standard CUDA backend.  </p>\n\n<p>In addition, it includes an optional <strong>custom CUDA extension</strong> that can provide up to 2\u00d7 speedup for sparse weight matrices (e.g., sparse graphs). This extension is currently supported only on <strong>Linux</strong>.</p>\n\n<p>To compile the optional extension, run:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>fswlib-build\n</code></pre>\n</div>\n\n<hr />\n\n<h2 id=\"usage-example\">\ud83d\udcd8 Usage Example</h2>\n\n<p>Below is a basic usage example of the <code>FSWEmbedding</code> class.  </p>\n\n<p>For more examples, see the <code>examples/</code> <a href=\"https://github.com/tal-amir/fswlib/tree/main/examples\">directory</a> of the GitHub repository.<br />\nFull API documentation is available at <a href=\"https://tal-amir.github.io/fswlib\">https://tal-amir.github.io/fswlib</a>.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">fswlib</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FSWEmbedding</span>\n\n<span class=\"c1\"># Configuration</span>\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cuda&#39;</span> <span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span> <span class=\"k\">else</span> <span class=\"s1\">&#39;cpu&#39;</span>\n<span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>\n<span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"mi\">15</span>     <span class=\"c1\"># Dimension of multiset elements</span>\n<span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>     <span class=\"c1\"># Multiset size</span>\n<span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"mi\">123</span>    <span class=\"c1\"># Embedding output dimension</span>\n\n<span class=\"c1\"># Create FSW embedding module for multisets/measures over \u211d^d</span>\n<span class=\"n\">embed</span> <span class=\"o\">=</span> <span class=\"n\">FSWEmbedding</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"o\">=</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># --- Single input multiset ---</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>  <span class=\"c1\"># Optional weights</span>\n\n<span class=\"n\">X_emb</span> <span class=\"o\">=</span> <span class=\"n\">embed</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">)</span>  <span class=\"c1\"># Embeds a weighted multiset</span>\n<span class=\"n\">X_emb</span> <span class=\"o\">=</span> <span class=\"n\">embed</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>     <span class=\"c1\"># Embeds X assuming uniform weights</span>\n\n<span class=\"c1\"># --- A batch of input multisets ---</span>\n<span class=\"n\">batch_dims</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">7</span><span class=\"p\">,</span><span class=\"mi\">9</span><span class=\"p\">)</span>\n<span class=\"n\">Xb</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"n\">batch_dims</span><span class=\"o\">+</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"n\">d</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">Wb</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">batch_dims</span><span class=\"o\">+</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">Xb_emb</span> <span class=\"o\">=</span> <span class=\"n\">embed</span><span class=\"p\">(</span><span class=\"n\">Xb</span><span class=\"p\">,</span> <span class=\"n\">Wb</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Dimension of multiset elements: </span><span class=\"si\">{</span><span class=\"n\">d</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Embedding dimension: </span><span class=\"si\">{</span><span class=\"n\">m</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">One multiset X of size </span><span class=\"si\">{</span><span class=\"n\">n</span><span class=\"si\">}</span><span class=\"s2\">:&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;X shape:&quot;</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;embed(X) shape:&quot;</span><span class=\"p\">,</span> <span class=\"n\">X_emb</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n\n<span class=\"n\">batch_dim_str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;\u00d7&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">batch_dims</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">Batch of </span><span class=\"si\">{</span><span class=\"n\">batch_dim_str</span><span class=\"si\">}</span><span class=\"s2\"> multisets, each of size </span><span class=\"si\">{</span><span class=\"n\">n</span><span class=\"si\">}</span><span class=\"s2\">:&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Xb shape:&quot;</span><span class=\"p\">,</span> <span class=\"n\">Xb</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;embed(Xb) shape:&quot;</span><span class=\"p\">,</span> <span class=\"n\">Xb_emb</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Output:</p>\n\n<pre><code>Dimension of multiset elements: 15\nEmbedding dimension: 123\n\nOne multiset X of size 50:\nX shape: torch.Size([50, 15])\nembed(X) shape: torch.Size([123])\n\nBatch of 5\u00d73\u00d77\u00d79 multisets, each of size 50:\nXb shape: torch.Size([5, 3, 7, 9, 50, 15])\nembed(Xb) shape: torch.Size([5, 3, 7, 9, 123])\n</code></pre>\n\n<p>The example below illustrates the difference between the core embedding, which is invariant to the input multiset size, and an embedding that explicitly encodes it.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># --- Encoding multiset size (total mass) ---</span>\n<span class=\"c1\"># By default, the embedding is invariant to the input multiset size, since it</span>\n<span class=\"c1\"># treats inputs as *probability measures*.</span>\n<span class=\"c1\"># Set `encode_total_mass = True` to make the embedding encode the size of the</span>\n<span class=\"c1\"># input multisets, or, more generally, the total mass (i.e. sum of weights).</span>\n<span class=\"n\">embed_total_mass_invariant</span> <span class=\"o\">=</span> <span class=\"n\">FSWEmbedding</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"o\">=</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">embed_total_mass_aware</span> <span class=\"o\">=</span>     <span class=\"n\">FSWEmbedding</span><span class=\"p\">(</span><span class=\"n\">d_in</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">d_out</span><span class=\"o\">=</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">encode_total_mass</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Two multisets of different size but identical element proportions</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">v2</span><span class=\"p\">,</span> <span class=\"n\">v3</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n\n<span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">v2</span><span class=\"p\">,</span> <span class=\"n\">v3</span><span class=\"p\">])</span>\n<span class=\"n\">X2</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">v2</span><span class=\"p\">,</span> <span class=\"n\">v2</span><span class=\"p\">,</span> <span class=\"n\">v3</span><span class=\"p\">,</span> <span class=\"n\">v3</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Embedding *without* total mass encoding</span>\n<span class=\"n\">X1_emb</span> <span class=\"o\">=</span> <span class=\"n\">embed_total_mass_invariant</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">)</span>\n<span class=\"n\">X2_emb</span> <span class=\"o\">=</span> <span class=\"n\">embed_total_mass_invariant</span><span class=\"p\">(</span><span class=\"n\">X2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Embedding *with* total mass encoding</span>\n<span class=\"n\">X1_emb_aware</span> <span class=\"o\">=</span> <span class=\"n\">embed_total_mass_aware</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">)</span>\n<span class=\"n\">X2_emb_aware</span> <span class=\"o\">=</span> <span class=\"n\">embed_total_mass_aware</span><span class=\"p\">(</span><span class=\"n\">X2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Measure the differences</span>\n<span class=\"n\">diff_invariant</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">X1_emb</span> <span class=\"o\">-</span> <span class=\"n\">X2_emb</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n<span class=\"n\">diff_aware</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">X1_emb_aware</span> <span class=\"o\">-</span> <span class=\"n\">X2_emb_aware</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Two different-size multisets with identical element proportions:&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;X\u2081 = {v1, v2, v3},   X\u2082 = {v1, v1, v2, v2, v3, v3}&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Embedding difference: \u2016Embed(X\u2081) \u2212 Embed(X\u2082)\u2016\u2082&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;With total mass encoding:     </span><span class=\"si\">{</span><span class=\"n\">diff_aware</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Without total mass encoding:  </span><span class=\"si\">{</span><span class=\"n\">diff_invariant</span><span class=\"si\">:</span><span class=\"s2\">.2e</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<p>Output:</p>\n\n<pre><code>Two different-size multisets with identical element proportions:\nX\u2081 = {v1, v2, v3},   X\u2082 = {v1, v1, v2, v2, v3, v3}\nEmbedding difference: \u2016Embed(X\u2081) \u2212 Embed(X\u2082)\u2016\u2082\nWith total mass encoding:     3.0\nWithout total mass encoding:  5.09e-07\n</code></pre>\n\n<hr />\n\n<h2 id=\"citation\">\ud83d\udcc4 Citation</h2>\n\n<p>If you use this library in your research, please cite our paper:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">amir2025fsw</span><span class=\"p\">,</span>\n<span class=\"w\">  </span><span class=\"na\">title</span><span class=\"p\">=</span><span class=\"s\">{Fourier Sliced-{W}asserstein Embedding for Multisets and Measures}</span><span class=\"p\">,</span>\n<span class=\"w\">  </span><span class=\"na\">author</span><span class=\"p\">=</span><span class=\"s\">{Tal Amir and Nadav Dym}</span><span class=\"p\">,</span>\n<span class=\"w\">  </span><span class=\"na\">booktitle</span><span class=\"p\">=</span><span class=\"s\">{International Conference on Learning Representations (ICLR)}</span><span class=\"p\">,</span>\n<span class=\"w\">  </span><span class=\"na\">year</span><span class=\"p\">=</span><span class=\"s\">{2025}</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<hr />\n\n<h2 id=\"links\">\ud83d\udd17 Links</h2>\n\n<ul>\n<li><strong>Paper</strong>: <a href=\"https://iclr.cc/virtual/2025/poster/30562\">ICLR 2025</a>  </li>\n<li><strong>Code</strong>: <a href=\"https://github.com/tal-amir/fswlib\">GitHub repository</a></li>\n</ul>\n\n<hr />\n\n<h2 id=\"maintainer\">\ud83d\udc68\ud83c\udffb\u200d\ud83d\udd27 Maintainer</h2>\n\n<p>This library is maintained by <strong>Tal Amir</strong><br />\nHomepage: <a href=\"https://tal-amir.github.io\">https://tal-amir.github.io</a><br />\nEMail: <a href=\"mailto:talamir@technion.ac.il\">talamir@technion.ac.il</a></p>\n"}, {"fullname": "fswlib.FSWEmbedding", "modulename": "fswlib", "qualname": "FSWEmbedding", "kind": "class", "doc": "<p>Fourier Sliced-Wasserstein (FSW) embedding module.</p>\n\n<p>Maps input multisets (or, more generally, discrete measures) in\n$\\mathbb{R}^{d_\\text{in}}$ to fixed-length vectors in\n$\\mathbb{R}^{d_\\text{out}}$ via the Fourier Sliced-Wasserstein\nembedding as described in [Amir &amp; Dym, ICLR 2025].</p>\n\n<h6 id=\"features\">Features</h6>\n\n<p>\u2022 <strong>Batched inputs</strong>: eupports arbitrary number of batch dimensions.\n\u2022 <strong>Graph mode</strong>: efficient message-aggregation, including sparse adjacency support.\n\u2022 <strong>Differentiability</strong>: Full autograd/gradient support.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Constructor parameters.<br />\n<code>FSWEmbedding.forward</code>:  Input/output tensor shapes and options.  </p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "fswlib.FSWEmbedding.__init__", "modulename": "fswlib", "qualname": "FSWEmbedding.__init__", "kind": "function", "doc": "<p>Initialize an FSWEmbedding module.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>d_in</strong> (int):\nThe dimension of input multiset elements or, more generally, measure support points.<br />\nCoresponds to $d$ in $\\mathcal{S}_{\\leq N}\\left(\\mathbb{R}^d\\right)$, $\\mathcal{P}_{\\leq N}\\left(\\mathbb{R}^d\\right)$, or $\\mathcal{M}_{\\leq N}\\left(\\mathbb{R}^d\\right)$ in our paper.</li>\n<li><strong>d_out</strong> (int; optional):\nDesired embedding dimension.<br />\nIf not set, both <code>num_slices</code> and <code>num_frequencies</code> must be explicitly provided.</li>\n<li><strong>num_slices</strong> (int; optional):\nNumber of slices.<br />\nWhen provided, activates <code>cartesian_mode</code>, and <code>d_out</code> should be left None.<br />\nSee also: <code>flatten_cartesian_axes</code></li>\n<li><strong>num_frequencies</strong> (int; optional):\nNumber of frequencies per slice.<br />\nWhen provided, activates <code>cartesian_mode</code>, and <code>d_out</code> should be left None.<br />\nSee also: <code>flatten_cartesian_axes</code></li>\n<li><strong>flatten_cartesian_axes</strong> (bool; default=False):\nIf True, flattens the slice and frequency dimensions into a single output axis.<br />\nOnly relevant if <code>num_slices</code> and <code>num_frequencies</code> are provided.</li>\n<li><strong>d_edge</strong> (int; default=0):\nDimension of edge feature vectors. Used only for graph inputs.<br />\nSee the <code>graph_mode</code> argument of <code>FSWEmbedding.forward</code> for details.</li>\n<li><strong>encode_total_mass</strong> (bool; default=False):\nWhether to incorporate the input multiset size (or, more generally, the <em>total mass</em> of the input measure)\ninto the embedding output.</li>\n<li><strong>total_mass_encoding_transformation</strong> ({'identity', 'sqrt', 'log'} or TotalMassEncodingFunction; default='identity'):\nTransformation applied to the total mass <em>before</em> embedding.<br />\nSee also: <code>TotalMassEncodingFunction</code></li>\n<li><strong>total_mass_encoding_method</strong> ({'decoupled', 'scaled', 'homogeneous', 'homogeneous_scaled', 'homogeneous_legacy'} or TotalMassEncodingMethod; default='decoupled'):\nStrategy for combining the total mass with the core embedding.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>total_mass_encoding_scale</strong> (float; default=1.0):\nThe encoded total mass is multiplied by this scaling factor.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>total_mass_padding_thresh</strong> (float or int; default=1.0):\nInputs with total mass below this threshold are padded with the zero vector to reach it; see\nin [Amir and Dym, ICLR 2025], Appendix A.1.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>learnable_slices</strong> (bool; default=False):\nIf True, slice vectors are learnable parameters.</li>\n<li><strong>learnable_frequencies</strong> (bool; default=False):\nIf True, frequency values are learnable parameters.</li>\n<li><strong>frequency_init</strong> (float, str, tuple of float, or FrequencyInitMethod; default='random'):\nInitialization scheme for frequencies:\n<ul>\n<li>A float: sets all frequencies to the same value.</li>\n<li>A tuple <code>(low, high)</code> of floats: sets evenly spaced values in that interval.</li>\n<li>'random': frequencies are drawn independently from the distribution $\\mathcal{D_{\\xi}}$, defined in\n[Amir and Dym, ICLR 2025], Section 3.</li>\n<li>'even': frequencies are spaced evenly according to their distribution $\\mathcal{D_{\\xi}}$, with spaces\ninversely proportional to the density.<br />\nSee also: <code>FrequencyInitMethod</code></li>\n</ul></li>\n<li><strong>minimize_slice_coherence</strong> (bool; default=False):\nIf True, minimizes the <em>mutual coherence</em> between slices for a more uniform spread on the unit sphere.<br />\nIf False, slice vectors are drawn uniformly at random from the unit sphere.</li>\n<li><strong>enable_bias</strong> (bool; default=True):\nIf True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized\nto zero.</li>\n<li><strong>device</strong> (torch.device, int, str, or None, optional):\nThe torch device on which to allocate tensors (e.g., 'cpu', 'cuda', or an index).<br />\nIf not provided, the default device defined in Torch is used.</li>\n<li><strong>dtype</strong> (torch.dtype, optional):\nData type of input and output tensors (e.g., torch.float32).\nIf not provided, the default dtype defined in Torch is used.</li>\n<li><strong>use_custom_cuda_extension_if_available</strong> (bool or None, optional):\nWhether to use the custom CUDA kernel if present.\nDefault: Linux: True, all other systems: False</li>\n<li><strong>fail_if_cuda_extension_load_fails</strong> (bool; default=False):\nWhether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</li>\n<li><strong>report</strong> (bool; default=False):\nIf True, prints a report with diagnostic information during initialization and forward computation.</li>\n<li><strong>report_on_coherence_minimization</strong> (bool; default=False):\nIf True, prints special diagnostics during slice coherence minimization.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>If Cartesian mode is activated and <code>encode_total_mass</code> is True, <code>flatten_cartesian_axes</code> must be True.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FrequencyInitMethod</code>: \nEnum for selecting frequency initialization strategies.<br />\n<code>TotalMassEncodingTransformation</code>: \nEnum for total mass transformations.<br />\n<code>TotalMassEncodingMethod</code>: \nEnum for strategies to incorporate total mass into the embedding.  </p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">d_in</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">d_out</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_slices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_frequencies</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">flatten_cartesian_axes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">d_edge</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">encode_total_mass</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_transformation</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">TotalMassEncodingTransformation</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;identity&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">TotalMassEncodingMethod</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;decoupled&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_scale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_padding_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_slices</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_frequencies</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">frequency_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FrequencyInitMethod</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;random&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">enable_bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "fswlib.FSWEmbedding.slice_vectors", "modulename": "fswlib", "qualname": "FSWEmbedding.slice_vectors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.FSWEmbedding.frequencies", "modulename": "fswlib", "qualname": "FSWEmbedding.frequencies", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.FSWEmbedding.bias", "modulename": "fswlib", "qualname": "FSWEmbedding.bias", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.FSWEmbedding.from_config", "modulename": "fswlib", "qualname": "FSWEmbedding.from_config", "kind": "function", "doc": "<p>Construct an FSWEmbedding instance from a configuration dictionary.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong> (dict):\nDictionary of keyword arguments matching the <code>__init__</code> parameters.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>FSWEmbedding</strong>: A new instance initialized with the provided configuration.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>TypeError</strong>: If any keys in the dictionary are not valid constructor arguments.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FSWEmbedding</span>:</span></span>", "funcdef": "def"}, {"fullname": "fswlib.FSWEmbedding.reset_parameters", "modulename": "fswlib", "qualname": "FSWEmbedding.reset_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">frequency_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FrequencyInitMethod</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.FSWEmbedding.to", "modulename": "fswlib", "qualname": "FSWEmbedding.to", "kind": "function", "doc": "<p>Moves the module to the specified device or dtype.</p>\n\n<p>Example:</p>\n\n<pre><code>module.to(torch.float32)\nmodule.to(device='cuda')\n</code></pre>\n\n<p>See also: torch.nn.Module.to()</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.FSWEmbedding.num_slices", "modulename": "fswlib", "qualname": "FSWEmbedding.num_slices", "kind": "variable", "doc": "<p>Number of slices used in the embedding.</p>\n", "annotation": ": int"}, {"fullname": "fswlib.FSWEmbedding.num_frequencies", "modulename": "fswlib", "qualname": "FSWEmbedding.num_frequencies", "kind": "variable", "doc": "<p>Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.</p>\n", "annotation": ": int"}, {"fullname": "fswlib.FSWEmbedding.cartesian_mode", "modulename": "fswlib", "qualname": "FSWEmbedding.cartesian_mode", "kind": "variable", "doc": "<p>If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices\nand frequencies.\nIn Cartesian mode, the embeding dimension is <code>d_out</code> = <code>num_slices \u00d7 num_frequencies</code>.\nCartesian mode is activated by providing <code>num_slices</code> and <code>num_frequencies</code> to <code>FSWEmbedding.__init__</code>bool\nSee also: <code>flatten_cartesian_axes</code></p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.flatten_cartesian_axes", "modulename": "fswlib", "qualname": "FSWEmbedding.flatten_cartesian_axes", "kind": "variable", "doc": "<p>In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.\nIf True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (<code>num_slices</code>, <code>num_frequencies</code>).\nIf False, the otput is shaped <code>num_slices</code> \u00d7 <code>num_frequencies</code>.\nThis setting is only relevant if <code>cartesian_mode</code> is True.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.learnable_slices", "modulename": "fswlib", "qualname": "FSWEmbedding.learnable_slices", "kind": "variable", "doc": "<p>Whether slice directions are learnable parameters.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.learnable_frequencies", "modulename": "fswlib", "qualname": "FSWEmbedding.learnable_frequencies", "kind": "variable", "doc": "<p>Whether frequency values are learnable parameters.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.enable_bias", "modulename": "fswlib", "qualname": "FSWEmbedding.enable_bias", "kind": "variable", "doc": "<p>Whether a learnable bias vector is added to the output embedding.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.encode_total_mass", "modulename": "fswlib", "qualname": "FSWEmbedding.encode_total_mass", "kind": "variable", "doc": "<p>Whether the total mass of the input measure is encoded into the embedding.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.FSWEmbedding.total_mass_encoding_transformation", "modulename": "fswlib", "qualname": "FSWEmbedding.total_mass_encoding_transformation", "kind": "variable", "doc": "<p>Function applied to the total mass before it is stored.</p>\n", "annotation": ": fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation"}, {"fullname": "fswlib.FSWEmbedding.total_mass_encoding_method", "modulename": "fswlib", "qualname": "FSWEmbedding.total_mass_encoding_method", "kind": "variable", "doc": "<p>Strategy used to incorporate total mass into the final embedding vector.</p>\n", "annotation": ": fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod"}, {"fullname": "fswlib.FSWEmbedding.total_mass_encoding_scale", "modulename": "fswlib", "qualname": "FSWEmbedding.total_mass_encoding_scale", "kind": "variable", "doc": "<p>The encoded total mass is scaled by this factor.</p>\n", "annotation": ": float"}, {"fullname": "fswlib.FSWEmbedding.total_mass_padding_thresh", "modulename": "fswlib", "qualname": "FSWEmbedding.total_mass_padding_thresh", "kind": "variable", "doc": "<p>Minimum total mass threshold; inputs below this value are padded to reach it.</p>\n", "annotation": ": float"}, {"fullname": "fswlib.FSWEmbedding.d_in", "modulename": "fswlib", "qualname": "FSWEmbedding.d_in", "kind": "variable", "doc": "<p>int: Ambient dimension of the input elements.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This value is set at initialization and determines the expected feature dimension of input points.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>d_in</code> argument specifies this value at initialization.  </p>\n", "annotation": ": int"}, {"fullname": "fswlib.FSWEmbedding.d_out", "modulename": "fswlib", "qualname": "FSWEmbedding.d_out", "kind": "variable", "doc": "<p>int: Dimensionality of the embedding output.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The dimension of the vector produced by the embedding for each multiset or distribution.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This value is set at initialization and governs the size of the embedding output.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>d_out</code> argument specifies this value at initialization.  </p>\n", "annotation": ": int"}, {"fullname": "fswlib.FSWEmbedding.device", "modulename": "fswlib", "qualname": "FSWEmbedding.device", "kind": "variable", "doc": "<p>torch.device: The device on which the module's parameters and buffers are stored.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.device</strong>: The PyTorch device (<code>'cpu'</code>, <code>'cuda'</code>, etc.) where the embedding computations will take place.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This behaves like the <code>device</code> property in standard PyTorch modules.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>device</code> can be specified at initialization.  </p>\n"}, {"fullname": "fswlib.FSWEmbedding.dtype", "modulename": "fswlib", "qualname": "FSWEmbedding.dtype", "kind": "variable", "doc": "<p>torch.dtype: The default data type used by the module.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.dtype</strong>: The data type (<code>torch.float32</code>, <code>torch.float64</code>, etc.) of the module\u2019s parameters and buffers.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This behaves like the <code>dtype</code> property in standard PyTorch modules.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>dtype</code> can be specified at initialization.  </p>\n"}, {"fullname": "fswlib.FSWEmbedding.forward", "modulename": "fswlib", "qualname": "FSWEmbedding.forward", "kind": "function", "doc": "<p>Compute the FSW embedding of an input multiset, measure, or graph.</p>\n\n<p>This method maps input sets of vectors (optionally weighted) to vectors in \u211d^{d_out}\nusing the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and\ngraph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>X</strong> (torch.Tensor):\nInput tensor of shape <code>(n, d_in)</code> or <code>(..., n, d_in)</code> for batched input.</li>\n<li><strong>W</strong> (torch.Tensor or {'unit', 'uniform'}, default='unit'):\nWeights tensor of shape <code>(n,)</code> or <code>(..., n)</code> corresponding to point importance.\nIf set to <code>'unit'</code> or <code>'uniform'</code>, uniform weights of <code>1/n</code> are assumed.</li>\n<li><strong>X_edge</strong> (torch.Tensor, optional):\nOptional edge feature tensor. Required if <code>d_edge &gt; 0</code> was set at initialization.</li>\n<li><strong>graph_mode</strong> (bool, default=False):\nIf True, interprets <code>W</code> as an adjacency matrix and computes a neighbor-aggregated\nembedding.</li>\n<li><strong>max_parallel_slices</strong> (int, optional):\nLimits the number of slices processed in parallel. Reduces memory usage by computing\nthe embedding in smaller blocks without changing the result.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.Tensor</strong>: The embedding tensor. Shape depends on the mode:\n<ul>\n<li><code>(d_out,)</code> or <code>(..., d_out)</code> in standard mode.</li>\n<li><code>(..., num_slices, num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=False</code>.</li>\n<li><code>(..., num_slices * num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=True</code>.</li>\n</ul></li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>Multisets and distributions:\n    If <code>X</code> is <code>(n, d_in)</code> and <code>W</code> is <code>(n,)</code>, the pair represents a weighted point cloud.\n    Weights must be non-negative with positive total mass.\n    If <code>W</code> is <code>'unit'</code> or <code>'uniform'</code>, uniform weights are used internally.</p>\n\n<p>Batching:\n    Input tensors may include leading batch dimensions. For <code>X</code> of shape <code>(..., n, d_in)</code>\n    and <code>W</code> of shape <code>(..., n)</code>, the output shape is <code>(..., d_out)</code>.</p>\n\n<p>Graph mode:\n    When <code>graph_mode=True</code>, <code>W</code> must be of shape <code>(..., n_recipients, n)</code> and <code>X</code> of\n    shape <code>(..., n, d_in)</code> or broadcastable to that. The output will be\n    <code>(..., n_recipients, d_out)</code>, where each vector represents a weighted embedding of\n    neighboring nodes. This avoids expanding <code>X</code> across <code>n_recipients</code> explicitly.</p>\n\n<p>Cartesian mode:\n    If <code>d_out</code> is not specified but <code>num_slices</code> and <code>num_frequencies</code> are, the embedding\n    is computed over a Cartesian product. The output shape is:\n        - <code>(..., num_slices, num_frequencies)</code> if <code>flatten_cartesian_axes=False</code>\n        - <code>(..., num_slices * num_frequencies)</code> if <code>flatten_cartesian_axes=True</code></p>\n\n<p>Slice serialization:\n    If <code>max_parallel_slices=t</code> is set, the computation is performed in blocks of size <code>t</code>,\n    reducing memory complexity by a factor of <code>num_slices / t</code>. The output remains unchanged.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Constructor for model configuration options.  </p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">W</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;unit&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;uniform&#39;</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;unit&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">X_edge</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">graph_mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">max_parallel_slices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.FSWCustomCudaExtensionLoadWarning", "modulename": "fswlib", "qualname": "FSWCustomCudaExtensionLoadWarning", "kind": "class", "doc": "<p>Raised when the custom CUDA extension could not be loaded and the fallback torch code is used.</p>\n", "bases": "builtins.UserWarning"}, {"fullname": "fswlib.FSWCustomCudaExtensionLoadError", "modulename": "fswlib", "qualname": "FSWCustomCudaExtensionLoadError", "kind": "class", "doc": "<p>Raised when the custom CUDA extension could not be loaded and fallback behavior is disabled.</p>\n", "bases": "builtins.RuntimeError"}, {"fullname": "fswlib.EnumWithResolve", "modulename": "fswlib", "qualname": "EnumWithResolve", "kind": "class", "doc": "<p></p>\n", "bases": "enum.Enum"}, {"fullname": "fswlib.EnumWithResolve.resolve", "modulename": "fswlib", "qualname": "EnumWithResolve.resolve", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">_E</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">obj</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"o\">~</span><span class=\"n\">_E</span>:</span></span>", "funcdef": "def"}, {"fullname": "fswlib.TotalMassEncodingTransformation", "modulename": "fswlib", "qualname": "TotalMassEncodingTransformation", "kind": "class", "doc": "<p>Transformation applied to the total mass before incorporating into the embedding.</p>\n\n<p>Each option defines a different transformation applied to the total mass of an input measure/multiset\nbefore it is incorporated into the embedding vector.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>IDENTITY</strong> (str):\n$f(x) = x$; no transformation.</li>\n<li><strong>SQRT</strong> (str):\n$f(x) = \\sqrt{1 + x} - 1$; mild nonlinearity.</li>\n<li><strong>LOG</strong> (str):\n$f(x) = \\log(1 + x)$; stronger compression of large values.</li>\n</ul>\n", "bases": "fswlib.fsw_embedding.fsw_embedding.EnumWithResolve"}, {"fullname": "fswlib.TotalMassEncodingTransformation.IDENTITY", "modulename": "fswlib", "qualname": "TotalMassEncodingTransformation.IDENTITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.IDENTITY: &#x27;identity&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingTransformation.SQRT", "modulename": "fswlib", "qualname": "TotalMassEncodingTransformation.SQRT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.SQRT: &#x27;sqrt&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingTransformation.LOG", "modulename": "fswlib", "qualname": "TotalMassEncodingTransformation.LOG", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.LOG: &#x27;log&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingMethod", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod", "kind": "class", "doc": "<p>Strategies for incorporating total mass into the embedding.</p>\n\n<p>Each method defines a different way of incorporating the total mass $\\mu\\left(\\Omega\\right) = \\sum_{i=1}^n w_i$ of an input measure\n$\\mu = \\sum_{i=1}^n w_i \\delta_{\\mathbf{x}^{(i)}}$ (i.e. the multiset size if $\\mu$ is a multiset) with the FSW embedding of the normalized input $\\mu_{\\rho}$ into a single output vector.\nFor further discussion, see Appendix A.1 of the reference below.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>DECOUPLED</strong> (str):\nThe total mass is appended as a separate component to the embedding vector,\nwhich is computed from the normalized input measure, as in Equation (18)\nin our paper:\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>SCALED</strong> (str):\nSimilar to <code>DECOUPLED</code>, but the embedding of the normalized input is scaled\nby the total mass. Using the notation of Equation (18), this yields:\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS</strong> (str):\nA method that encodes the total mass while preserving homogeneity\nwith respect to the elements of the input multiset. See Equation (19).\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert \\cdot \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS_SCALED</strong> (str):\nSimilar to <code>SCALED</code>, but preserves homogeneity.\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert, \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS_LEGACY</strong> (str):\nAn alternative, legacy version of the homogeneous method, retained\nfor reference and compatibility.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>In practice, $\\mu\\left(\\Omega\\right)$ in the above expressions is replaced by $\\alpha \\cdot f \\left( \\mu\\left(\\Omega\\right) \\right)$,\nwhere $f$ is the function defined by <code>TotalMassEncodingTransformation</code> and $\\alpha$ is a scale factor given in <code>total_mass_encoding_scale</code>.\nAdditionally, $\\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert$ is multiplied by a normalizing factor $\\sqrt{m-1}^{-1}$.</p>\n\n<h6 id=\"reference\">Reference</h6>\n\n<p>Tal Amir, Nadav Dym.\n\"Fourier Sliced-Wasserstein Embedding for Multisets and Measures.\"\nInternational Conference on Learning Representations (ICLR), 2025.\n<a href=\"https://iclr.cc/virtual/2025/poster/30562\">https://iclr.cc/virtual/2025/poster/30562</a></p>\n", "bases": "fswlib.fsw_embedding.fsw_embedding.EnumWithResolve"}, {"fullname": "fswlib.TotalMassEncodingMethod.DECOUPLED", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod.DECOUPLED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.DECOUPLED: &#x27;decoupled&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingMethod.SCALED", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod.SCALED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.SCALED: &#x27;scaled&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingMethod.HOMOGENEOUS", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS: &#x27;homogeneous&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingMethod.HOMOGENEOUS_SCALED", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS_SCALED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS_SCALED: &#x27;homogeneous_scaled&#x27;&gt;"}, {"fullname": "fswlib.TotalMassEncodingMethod.HOMOGENEOUS_LEGACY", "modulename": "fswlib", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS_LEGACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS_LEGACY: &#x27;homogeneous_legacy&#x27;&gt;"}, {"fullname": "fswlib.FrequencyInitMethod", "modulename": "fswlib", "qualname": "FrequencyInitMethod", "kind": "class", "doc": "<p>Method for initializing frequencies in the FSW embedding.</p>\n\n<p>This enumeration specifies how the frequencies in the FSW embedding are\ninitialized.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>RANDOM</strong> (str):\nFrequencies are sampled independently at random from the distribution\nD_\u03be, as defined in Section 3 of our paper.</li>\n<li><strong>EVEN</strong> (str):\nFrequencies are spaced deterministically for efficient coverage of the frequency domain,\nwith spacing inversely proportional to the density function f_\u03be.</li>\n</ul>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Where this method is selected and used.  </p>\n", "bases": "fswlib.fsw_embedding.fsw_embedding.EnumWithResolve"}, {"fullname": "fswlib.FrequencyInitMethod.RANDOM", "modulename": "fswlib", "qualname": "FrequencyInitMethod.RANDOM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FrequencyInitMethod.RANDOM: &#x27;random&#x27;&gt;"}, {"fullname": "fswlib.FrequencyInitMethod.EVEN", "modulename": "fswlib", "qualname": "FrequencyInitMethod.EVEN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FrequencyInitMethod.EVEN: &#x27;even&#x27;&gt;"}, {"fullname": "fswlib.test_fsw_embedding", "modulename": "fswlib", "qualname": "test_fsw_embedding", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding", "modulename": "fswlib.fsw_embedding.fsw_embedding", "kind": "module", "doc": "<p>fsw_embedding.py</p>\n\n<p>Main module for computing the Fourier Sliced-Wasserstein (FSW) embedding.\nPart of the <code>fswlib</code> package: <a href=\"https://pypi.org/project/fswlib/\">https://pypi.org/project/fswlib/</a></p>\n\n<p>Authors:\n    Tal Amir, Nadav Dym\n    Technion \u2013 Israel Institute of Technology</p>\n\n<p>This code is based on the paper:\n    \"Fourier Sliced-Wasserstein Embedding for Multisets and Measures\"\n    Tal Amir, Nadav Dym\n    International Conference on Learning Representations (ICLR), 2025</p>\n\n<p>Paper URL:\n    <a href=\"https://iclr.cc/virtual/2025/poster/30562\">https://iclr.cc/virtual/2025/poster/30562</a></p>\n\n<p>Project repository:\n    <a href=\"https://github.com/tal-amir/fswlib\">https://github.com/tal-amir/fswlib</a></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding", "kind": "class", "doc": "<p>Fourier Sliced-Wasserstein (FSW) embedding module.</p>\n\n<p>Maps input multisets (or, more generally, discrete measures) in\n$\\mathbb{R}^{d_\\text{in}}$ to fixed-length vectors in\n$\\mathbb{R}^{d_\\text{out}}$ via the Fourier Sliced-Wasserstein\nembedding as described in [Amir &amp; Dym, ICLR 2025].</p>\n\n<h6 id=\"features\">Features</h6>\n\n<p>\u2022 <strong>Batched inputs</strong>: eupports arbitrary number of batch dimensions.\n\u2022 <strong>Graph mode</strong>: efficient message-aggregation, including sparse adjacency support.\n\u2022 <strong>Differentiability</strong>: Full autograd/gradient support.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Constructor parameters.<br />\n<code>FSWEmbedding.forward</code>:  Input/output tensor shapes and options.  </p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.__init__", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.__init__", "kind": "function", "doc": "<p>Initialize an FSWEmbedding module.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>d_in</strong> (int):\nThe dimension of input multiset elements or, more generally, measure support points.<br />\nCoresponds to $d$ in $\\mathcal{S}_{\\leq N}\\left(\\mathbb{R}^d\\right)$, $\\mathcal{P}_{\\leq N}\\left(\\mathbb{R}^d\\right)$, or $\\mathcal{M}_{\\leq N}\\left(\\mathbb{R}^d\\right)$ in our paper.</li>\n<li><strong>d_out</strong> (int; optional):\nDesired embedding dimension.<br />\nIf not set, both <code>num_slices</code> and <code>num_frequencies</code> must be explicitly provided.</li>\n<li><strong>num_slices</strong> (int; optional):\nNumber of slices.<br />\nWhen provided, activates <code>cartesian_mode</code>, and <code>d_out</code> should be left None.<br />\nSee also: <code>flatten_cartesian_axes</code></li>\n<li><strong>num_frequencies</strong> (int; optional):\nNumber of frequencies per slice.<br />\nWhen provided, activates <code>cartesian_mode</code>, and <code>d_out</code> should be left None.<br />\nSee also: <code>flatten_cartesian_axes</code></li>\n<li><strong>flatten_cartesian_axes</strong> (bool; default=False):\nIf True, flattens the slice and frequency dimensions into a single output axis.<br />\nOnly relevant if <code>num_slices</code> and <code>num_frequencies</code> are provided.</li>\n<li><strong>d_edge</strong> (int; default=0):\nDimension of edge feature vectors. Used only for graph inputs.<br />\nSee the <code>graph_mode</code> argument of <code>FSWEmbedding.forward</code> for details.</li>\n<li><strong>encode_total_mass</strong> (bool; default=False):\nWhether to incorporate the input multiset size (or, more generally, the <em>total mass</em> of the input measure)\ninto the embedding output.</li>\n<li><strong>total_mass_encoding_transformation</strong> ({'identity', 'sqrt', 'log'} or TotalMassEncodingFunction; default='identity'):\nTransformation applied to the total mass <em>before</em> embedding.<br />\nSee also: <code>TotalMassEncodingFunction</code></li>\n<li><strong>total_mass_encoding_method</strong> ({'decoupled', 'scaled', 'homogeneous', 'homogeneous_scaled', 'homogeneous_legacy'} or TotalMassEncodingMethod; default='decoupled'):\nStrategy for combining the total mass with the core embedding.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>total_mass_encoding_scale</strong> (float; default=1.0):\nThe encoded total mass is multiplied by this scaling factor.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>total_mass_padding_thresh</strong> (float or int; default=1.0):\nInputs with total mass below this threshold are padded with the zero vector to reach it; see\nin [Amir and Dym, ICLR 2025], Appendix A.1.<br />\nSee also: <code>TotalMassEncodingMethod</code></li>\n<li><strong>learnable_slices</strong> (bool; default=False):\nIf True, slice vectors are learnable parameters.</li>\n<li><strong>learnable_frequencies</strong> (bool; default=False):\nIf True, frequency values are learnable parameters.</li>\n<li><strong>frequency_init</strong> (float, str, tuple of float, or FrequencyInitMethod; default='random'):\nInitialization scheme for frequencies:\n<ul>\n<li>A float: sets all frequencies to the same value.</li>\n<li>A tuple <code>(low, high)</code> of floats: sets evenly spaced values in that interval.</li>\n<li>'random': frequencies are drawn independently from the distribution $\\mathcal{D_{\\xi}}$, defined in\n[Amir and Dym, ICLR 2025], Section 3.</li>\n<li>'even': frequencies are spaced evenly according to their distribution $\\mathcal{D_{\\xi}}$, with spaces\ninversely proportional to the density.<br />\nSee also: <code>FrequencyInitMethod</code></li>\n</ul></li>\n<li><strong>minimize_slice_coherence</strong> (bool; default=False):\nIf True, minimizes the <em>mutual coherence</em> between slices for a more uniform spread on the unit sphere.<br />\nIf False, slice vectors are drawn uniformly at random from the unit sphere.</li>\n<li><strong>enable_bias</strong> (bool; default=True):\nIf True, adds a learnable bias vector to the output embedding. When enabled, the bias is initialized\nto zero.</li>\n<li><strong>device</strong> (torch.device, int, str, or None, optional):\nThe torch device on which to allocate tensors (e.g., 'cpu', 'cuda', or an index).<br />\nIf not provided, the default device defined in Torch is used.</li>\n<li><strong>dtype</strong> (torch.dtype, optional):\nData type of input and output tensors (e.g., torch.float32).\nIf not provided, the default dtype defined in Torch is used.</li>\n<li><strong>use_custom_cuda_extension_if_available</strong> (bool or None, optional):\nWhether to use the custom CUDA kernel if present.\nDefault: Linux: True, all other systems: False</li>\n<li><strong>fail_if_cuda_extension_load_fails</strong> (bool; default=False):\nWhether to raise a runtime error (rather than a warning) if the CUDA extension failes to load.</li>\n<li><strong>report</strong> (bool; default=False):\nIf True, prints a report with diagnostic information during initialization and forward computation.</li>\n<li><strong>report_on_coherence_minimization</strong> (bool; default=False):\nIf True, prints special diagnostics during slice coherence minimization.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>If Cartesian mode is activated and <code>encode_total_mass</code> is True, <code>flatten_cartesian_axes</code> must be True.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FrequencyInitMethod</code>: \nEnum for selecting frequency initialization strategies.<br />\n<code>TotalMassEncodingTransformation</code>: \nEnum for total mass transformations.<br />\n<code>TotalMassEncodingMethod</code>: \nEnum for strategies to incorporate total mass into the embedding.  </p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">d_in</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">d_out</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_slices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_frequencies</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">flatten_cartesian_axes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">d_edge</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">encode_total_mass</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_transformation</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">TotalMassEncodingTransformation</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;identity&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">TotalMassEncodingMethod</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;decoupled&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_encoding_scale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">total_mass_padding_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_slices</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">learnable_frequencies</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">frequency_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FrequencyInitMethod</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;random&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">enable_bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">dtype</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_custom_cuda_extension_if_available</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">fail_if_cuda_extension_load_fails</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.slice_vectors", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.slice_vectors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.frequencies", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.frequencies", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.bias", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.bias", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.from_config", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.from_config", "kind": "function", "doc": "<p>Construct an FSWEmbedding instance from a configuration dictionary.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong> (dict):\nDictionary of keyword arguments matching the <code>__init__</code> parameters.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>FSWEmbedding</strong>: A new instance initialized with the provided configuration.</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li><strong>TypeError</strong>: If any keys in the dictionary are not valid constructor arguments.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FSWEmbedding</span>:</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.reset_parameters", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.reset_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">frequency_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">fswlib</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">fsw_embedding</span><span class=\"o\">.</span><span class=\"n\">FrequencyInitMethod</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">minimize_slice_coherence</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">report_on_coherence_minimization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.to", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.to", "kind": "function", "doc": "<p>Moves the module to the specified device or dtype.</p>\n\n<p>Example:</p>\n\n<pre><code>module.to(torch.float32)\nmodule.to(device='cuda')\n</code></pre>\n\n<p>See also: torch.nn.Module.to()</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.num_slices", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.num_slices", "kind": "variable", "doc": "<p>Number of slices used in the embedding.</p>\n", "annotation": ": int"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.num_frequencies", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.num_frequencies", "kind": "variable", "doc": "<p>Number of frequencies used in the embedding. In Cartesian mode, this is the number of frequencies per slice.</p>\n", "annotation": ": int"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.cartesian_mode", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.cartesian_mode", "kind": "variable", "doc": "<p>If True, the embedding is computed for each (slice, frequency) pair in the Cartesian product of slices\nand frequencies.\nIn Cartesian mode, the embeding dimension is <code>d_out</code> = <code>num_slices \u00d7 num_frequencies</code>.\nCartesian mode is activated by providing <code>num_slices</code> and <code>num_frequencies</code> to <code>FSWEmbedding.__init__</code>bool\nSee also: <code>flatten_cartesian_axes</code></p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.flatten_cartesian_axes", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.flatten_cartesian_axes", "kind": "variable", "doc": "<p>In Cartesian mode, tells Whether the slice and frequency axes are flattened into a single dimension.\nIf True, each input multiset/distribution corresponds to a two-dimensional output, with the shape (<code>num_slices</code>, <code>num_frequencies</code>).\nIf False, the otput is shaped <code>num_slices</code> \u00d7 <code>num_frequencies</code>.\nThis setting is only relevant if <code>cartesian_mode</code> is True.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.learnable_slices", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.learnable_slices", "kind": "variable", "doc": "<p>Whether slice directions are learnable parameters.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.learnable_frequencies", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.learnable_frequencies", "kind": "variable", "doc": "<p>Whether frequency values are learnable parameters.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.enable_bias", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.enable_bias", "kind": "variable", "doc": "<p>Whether a learnable bias vector is added to the output embedding.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.encode_total_mass", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.encode_total_mass", "kind": "variable", "doc": "<p>Whether the total mass of the input measure is encoded into the embedding.</p>\n", "annotation": ": bool"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_transformation", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_transformation", "kind": "variable", "doc": "<p>Function applied to the total mass before it is stored.</p>\n", "annotation": ": fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_method", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_method", "kind": "variable", "doc": "<p>Strategy used to incorporate total mass into the final embedding vector.</p>\n", "annotation": ": fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_encoding_scale", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_encoding_scale", "kind": "variable", "doc": "<p>The encoded total mass is scaled by this factor.</p>\n", "annotation": ": float"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.total_mass_padding_thresh", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.total_mass_padding_thresh", "kind": "variable", "doc": "<p>Minimum total mass threshold; inputs below this value are padded to reach it.</p>\n", "annotation": ": float"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.d_in", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.d_in", "kind": "variable", "doc": "<p>int: Ambient dimension of the input elements.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The input dimensionality of the multiset elements (i.e., the last dimension of the input tensors).</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This value is set at initialization and determines the expected feature dimension of input points.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>d_in</code> argument specifies this value at initialization.  </p>\n", "annotation": ": int"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.d_out", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.d_out", "kind": "variable", "doc": "<p>int: Dimensionality of the embedding output.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>int</strong>: The dimension of the vector produced by the embedding for each multiset or distribution.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This value is set at initialization and governs the size of the embedding output.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>d_out</code> argument specifies this value at initialization.  </p>\n", "annotation": ": int"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.device", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.device", "kind": "variable", "doc": "<p>torch.device: The device on which the module's parameters and buffers are stored.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.device</strong>: The PyTorch device (<code>'cpu'</code>, <code>'cuda'</code>, etc.) where the embedding computations will take place.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This behaves like the <code>device</code> property in standard PyTorch modules.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>device</code> can be specified at initialization.  </p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.dtype", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.dtype", "kind": "variable", "doc": "<p>torch.dtype: The default data type used by the module.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.dtype</strong>: The data type (<code>torch.float32</code>, <code>torch.float64</code>, etc.) of the module\u2019s parameters and buffers.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>This behaves like the <code>dtype</code> property in standard PyTorch modules.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>__init__</code>:  The <code>dtype</code> can be specified at initialization.  </p>\n"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FSWEmbedding.forward", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FSWEmbedding.forward", "kind": "function", "doc": "<p>Compute the FSW embedding of an input multiset, measure, or graph.</p>\n\n<p>This method maps input sets of vectors (optionally weighted) to vectors in \u211d^{d_out}\nusing the Fourier Sliced-Wasserstein (FSW) embedding. It supports batched inputs and\ngraph-based neighbor aggregation, with possibly sparse weight/adjacency matrices.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>X</strong> (torch.Tensor):\nInput tensor of shape <code>(n, d_in)</code> or <code>(..., n, d_in)</code> for batched input.</li>\n<li><strong>W</strong> (torch.Tensor or {'unit', 'uniform'}, default='unit'):\nWeights tensor of shape <code>(n,)</code> or <code>(..., n)</code> corresponding to point importance.\nIf set to <code>'unit'</code> or <code>'uniform'</code>, uniform weights of <code>1/n</code> are assumed.</li>\n<li><strong>X_edge</strong> (torch.Tensor, optional):\nOptional edge feature tensor. Required if <code>d_edge &gt; 0</code> was set at initialization.</li>\n<li><strong>graph_mode</strong> (bool, default=False):\nIf True, interprets <code>W</code> as an adjacency matrix and computes a neighbor-aggregated\nembedding.</li>\n<li><strong>max_parallel_slices</strong> (int, optional):\nLimits the number of slices processed in parallel. Reduces memory usage by computing\nthe embedding in smaller blocks without changing the result.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<ul>\n<li><strong>torch.Tensor</strong>: The embedding tensor. Shape depends on the mode:\n<ul>\n<li><code>(d_out,)</code> or <code>(..., d_out)</code> in standard mode.</li>\n<li><code>(..., num_slices, num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=False</code>.</li>\n<li><code>(..., num_slices * num_frequencies)</code> in Cartesian mode if <code>flatten_cartesian_axes=True</code>.</li>\n</ul></li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>Multisets and distributions:\n    If <code>X</code> is <code>(n, d_in)</code> and <code>W</code> is <code>(n,)</code>, the pair represents a weighted point cloud.\n    Weights must be non-negative with positive total mass.\n    If <code>W</code> is <code>'unit'</code> or <code>'uniform'</code>, uniform weights are used internally.</p>\n\n<p>Batching:\n    Input tensors may include leading batch dimensions. For <code>X</code> of shape <code>(..., n, d_in)</code>\n    and <code>W</code> of shape <code>(..., n)</code>, the output shape is <code>(..., d_out)</code>.</p>\n\n<p>Graph mode:\n    When <code>graph_mode=True</code>, <code>W</code> must be of shape <code>(..., n_recipients, n)</code> and <code>X</code> of\n    shape <code>(..., n, d_in)</code> or broadcastable to that. The output will be\n    <code>(..., n_recipients, d_out)</code>, where each vector represents a weighted embedding of\n    neighboring nodes. This avoids expanding <code>X</code> across <code>n_recipients</code> explicitly.</p>\n\n<p>Cartesian mode:\n    If <code>d_out</code> is not specified but <code>num_slices</code> and <code>num_frequencies</code> are, the embedding\n    is computed over a Cartesian product. The output shape is:\n        - <code>(..., num_slices, num_frequencies)</code> if <code>flatten_cartesian_axes=False</code>\n        - <code>(..., num_slices * num_frequencies)</code> if <code>flatten_cartesian_axes=True</code></p>\n\n<p>Slice serialization:\n    If <code>max_parallel_slices=t</code> is set, the computation is performed in blocks of size <code>t</code>,\n    reducing memory complexity by a factor of <code>num_slices / t</code>. The output remains unchanged.</p>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Constructor for model configuration options.  </p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">W</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;unit&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;uniform&#39;</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;unit&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">X_edge</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">graph_mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">max_parallel_slices</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.EnumWithResolve", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "EnumWithResolve", "kind": "class", "doc": "<p></p>\n", "bases": "enum.Enum"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.EnumWithResolve.resolve", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "EnumWithResolve.resolve", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span><span class=\"p\">:</span> <span class=\"n\">Type</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">_E</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">obj</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"o\">~</span><span class=\"n\">_E</span>:</span></span>", "funcdef": "def"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingTransformation", "kind": "class", "doc": "<p>Transformation applied to the total mass before incorporating into the embedding.</p>\n\n<p>Each option defines a different transformation applied to the total mass of an input measure/multiset\nbefore it is incorporated into the embedding vector.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>IDENTITY</strong> (str):\n$f(x) = x$; no transformation.</li>\n<li><strong>SQRT</strong> (str):\n$f(x) = \\sqrt{1 + x} - 1$; mild nonlinearity.</li>\n<li><strong>LOG</strong> (str):\n$f(x) = \\log(1 + x)$; stronger compression of large values.</li>\n</ul>\n", "bases": "EnumWithResolve"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation.IDENTITY", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingTransformation.IDENTITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.IDENTITY: &#x27;identity&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation.SQRT", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingTransformation.SQRT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.SQRT: &#x27;sqrt&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingTransformation.LOG", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingTransformation.LOG", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingTransformation.LOG: &#x27;log&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod", "kind": "class", "doc": "<p>Strategies for incorporating total mass into the embedding.</p>\n\n<p>Each method defines a different way of incorporating the total mass $\\mu\\left(\\Omega\\right) = \\sum_{i=1}^n w_i$ of an input measure\n$\\mu = \\sum_{i=1}^n w_i \\delta_{\\mathbf{x}^{(i)}}$ (i.e. the multiset size if $\\mu$ is a multiset) with the FSW embedding of the normalized input $\\mu_{\\rho}$ into a single output vector.\nFor further discussion, see Appendix A.1 of the reference below.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>DECOUPLED</strong> (str):\nThe total mass is appended as a separate component to the embedding vector,\nwhich is computed from the normalized input measure, as in Equation (18)\nin our paper:\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>SCALED</strong> (str):\nSimilar to <code>DECOUPLED</code>, but the embedding of the normalized input is scaled\nby the total mass. Using the notation of Equation (18), this yields:\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\mu\\left(\\Omega\\right), \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS</strong> (str):\nA method that encodes the total mass while preserving homogeneity\nwith respect to the elements of the input multiset. See Equation (19).\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert \\cdot \\mu\\left(\\Omega\\right), \\;  E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS_SCALED</strong> (str):\nSimilar to <code>SCALED</code>, but preserves homogeneity.\n$$ \\hat{E}^{\\textup{FSW}}_{m}\\left(\\mu\\right) = \\left[ \\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert, \\;  \\mu\\left(\\Omega\\right) \\cdot E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\right] $$</li>\n<li><strong>HOMOGENEOUS_LEGACY</strong> (str):\nAn alternative, legacy version of the homogeneous method, retained\nfor reference and compatibility.</li>\n</ul>\n\n<h6 id=\"notes\">Notes</h6>\n\n<p>In practice, $\\mu\\left(\\Omega\\right)$ in the above expressions is replaced by $\\alpha \\cdot f \\left( \\mu\\left(\\Omega\\right) \\right)$,\nwhere $f$ is the function defined by <code>TotalMassEncodingTransformation</code> and $\\alpha$ is a scale factor given in <code>total_mass_encoding_scale</code>.\nAdditionally, $\\lVert E^{\\textup{FSW}}_{m-1}\\left(\\mu_{\\rho}\\right) \\rVert$ is multiplied by a normalizing factor $\\sqrt{m-1}^{-1}$.</p>\n\n<h6 id=\"reference\">Reference</h6>\n\n<p>Tal Amir, Nadav Dym.\n\"Fourier Sliced-Wasserstein Embedding for Multisets and Measures.\"\nInternational Conference on Learning Representations (ICLR), 2025.\n<a href=\"https://iclr.cc/virtual/2025/poster/30562\">https://iclr.cc/virtual/2025/poster/30562</a></p>\n", "bases": "EnumWithResolve"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod.DECOUPLED", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod.DECOUPLED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.DECOUPLED: &#x27;decoupled&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod.SCALED", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod.SCALED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.SCALED: &#x27;scaled&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod.HOMOGENEOUS", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS: &#x27;homogeneous&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod.HOMOGENEOUS_SCALED", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS_SCALED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS_SCALED: &#x27;homogeneous_scaled&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.TotalMassEncodingMethod.HOMOGENEOUS_LEGACY", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "TotalMassEncodingMethod.HOMOGENEOUS_LEGACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;TotalMassEncodingMethod.HOMOGENEOUS_LEGACY: &#x27;homogeneous_legacy&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FrequencyInitMethod", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FrequencyInitMethod", "kind": "class", "doc": "<p>Method for initializing frequencies in the FSW embedding.</p>\n\n<p>This enumeration specifies how the frequencies in the FSW embedding are\ninitialized.</p>\n\n<h6 id=\"attributes\">Attributes</h6>\n\n<ul>\n<li><strong>RANDOM</strong> (str):\nFrequencies are sampled independently at random from the distribution\nD_\u03be, as defined in Section 3 of our paper.</li>\n<li><strong>EVEN</strong> (str):\nFrequencies are spaced deterministically for efficient coverage of the frequency domain,\nwith spacing inversely proportional to the density function f_\u03be.</li>\n</ul>\n\n<h6 id=\"see-also\">See Also</h6>\n\n<p><code>FSWEmbedding.__init__</code>:  Where this method is selected and used.  </p>\n", "bases": "EnumWithResolve"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FrequencyInitMethod.RANDOM", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FrequencyInitMethod.RANDOM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FrequencyInitMethod.RANDOM: &#x27;random&#x27;&gt;"}, {"fullname": "fswlib.fsw_embedding.fsw_embedding.FrequencyInitMethod.EVEN", "modulename": "fswlib.fsw_embedding.fsw_embedding", "qualname": "FrequencyInitMethod.EVEN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;FrequencyInitMethod.EVEN: &#x27;even&#x27;&gt;"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();